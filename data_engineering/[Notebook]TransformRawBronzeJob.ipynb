{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/itong1900/opt/anaconda3/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/itong1900/opt/anaconda3/lib/python3.8/site-packages (from pyspark) (0.10.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import ArrayType, FloatType, LongType, IntegerType\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 ms, sys: 18.3 ms, total: 33.2 ms\n",
      "Wall time: 5.63 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder.appName(\"ottoDB\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# path = \"~/otto_kaggle/otto_obj_recommendation_sys/data_engineering/\"\n",
    "# os.getcwd()\n",
    "# #os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.76 ms, sys: 2.86 ms, total: 8.62 ms\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainDf = spark.read.json(\"../../allData/rawFull/train.jsonl\",lineSep='\\n') # spark.read.json(\"../../allData/validationData/out_7day_test/train_sessions.jsonl\",lineSep='\\n')\n",
    "testDf =  spark.read.json(\"../../allData/rawFull/test.jsonl\",lineSep='\\n')# spark.read.json(\"../../allData/validationData/out_7day_test/test_sessions.jsonl\",lineSep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- events: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- aid: long (nullable = true)\n",
      " |    |    |-- ts: long (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |-- session: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Display the original schema for reference\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some utils\n",
    "def convert_type_num(arr):\n",
    "    return [0 if i == \"clicks\" else (1 if i == \"carts\" else 2) for i in arr]\n",
    "convert_type_num_udf = udf(lambda row: convert_type_num(row), ArrayType(IntegerType()))\n",
    "\n",
    "def convert_ts_second_num(arr):\n",
    "    return [int(i/1000) for i in arr]\n",
    "convert_ts_seconds_udf = udf(lambda row:  convert_ts_second_num(row), ArrayType(LongType()))\n",
    "\n",
    "def data_preprocessing(df_spark):\n",
    "    result_df = df_spark.withColumn(\"total_action\", size(col(\"events\")))\\\n",
    "                        .withColumn(\"aids\", col(\"events.aid\"))\\\n",
    "                        .withColumn(\"ts_seconds\", convert_ts_seconds_udf(col(\"events.ts\")) )\\\n",
    "                        .withColumn(\"action_types\", convert_type_num_udf(col(\"events.type\")))\\\n",
    "                        .withColumn(\"session_start_time\", col(\"ts_seconds\").getItem(0))\\\n",
    "                        .withColumn(\"session_end_time\",element_at(col('ts_seconds'), -1))\\\n",
    "                        .select(\"session\", \"total_action\", \"session_start_time\", \"session_end_time\", \"aids\", \"ts_seconds\", \"action_types\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def save_csv_meta_info(df_spark, filename, path=\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/\"):\n",
    "    df_spark.select(\"session\", \"total_action\", \"session_start_time\", \"session_end_time\").write.parquet(f\"{path}/temp_file.parquet\")\n",
    "    df_core_pd = pd.read_parquet(f'{path}/temp_file.parquet', engine='pyarrow')\n",
    "    df_core_pd.to_csv(f'{path}/{filename}', index = False)\n",
    "    ## delete the temp file from disc\n",
    "    if os.path.exists(f'{path}/temp_file.parquet'):\n",
    "        shutil.rmtree(f'{path}/temp_file.parquet')\n",
    "    else:\n",
    "        print(\"Warnings: temp file removal error.\")\n",
    "        \n",
    "\n",
    "def save_npz_core_info(df_spark, filename, path=\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/\"):\n",
    "    ## save the core info, aids, ts, ops as .npz file\n",
    "    ## Step I: convert to exploded pyspark dfs\n",
    "    df_parquet = df_spark.select(explode(arrays_zip(\"aids\", \"ts_seconds\", \"action_types\"))) \\\n",
    "                            .select(\"col.aids\", \"col.ts_seconds\", \"col.action_types\")\n",
    "    ## Step II: save temp parquet file to disc\n",
    "    df_parquet.write.parquet(f\"{path}/temp_file.parquet\")\n",
    "    ## step III: read the temp parquet file from disc\n",
    "    df_core_pd = pd.read_parquet(f'{path}/temp_file.parquet', engine='pyarrow')\n",
    "    np_aids = np.array(df_core_pd[\"aids\"])\n",
    "    np_ts = np.array(df_core_pd[\"ts_seconds\"])\n",
    "    np_ops = np.array(df_core_pd[\"action_types\"])\n",
    "    ## step IV: save the np arrays as .npz file\n",
    "    np.savez(f\"{path}/{filename}\", aids=np_aids, ts=np_ts, ops=np_ops)\n",
    "    ## delete the temp file from disc\n",
    "    if os.path.exists(f'{path}/temp_file.parquet'):\n",
    "        shutil.rmtree(f'{path}/temp_file.parquet')\n",
    "    else:\n",
    "        print(\"Warnings: temp file removal error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess trainDf and testDf \n",
    "trainDf_transformed = data_preprocessing(trainDf)\n",
    "testDf_transformed = data_preprocessing(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 1.35 s, total: 21.2 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## save the csv meta info portion\n",
    "save_csv_meta_info(trainDf_transformed, \"train_meta_data.csv\")\n",
    "save_csv_meta_info(testDf_transformed, \"test_meta_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 13.6 s, total: 25.8 s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## save the .npz core info portion\n",
    "save_npz_core_info(trainDf_transformed, \"train_core_data.npz\")\n",
    "save_npz_core_info(testDf_transformed, \"test_core_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|session|total_action|session_start_time|session_end_time|                aids|          ts_seconds|        action_types|\n",
      "+-------+------------+------------------+----------------+--------------------+--------------------+--------------------+\n",
      "|      0|         147|        1659304800|      1661103727|[1517085, 1563459...|[1659304800, 1659...|[0, 0, 0, 0, 0, 0...|\n",
      "|      1|          27|        1659304800|      1660857067|[424964, 1492293,...|[1659304800, 1659...|[1, 0, 1, 0, 1, 0...|\n",
      "|      2|          13|        1659304800|      1660577379|[763743, 137492, ...|[1659304800, 1659...|[0, 0, 0, 0, 0, 0...|\n",
      "|      3|         226|        1659304800|      1661109666|[1425967, 1425967...|[1659304800, 1659...|[1, 0, 0, 0, 1, 0...|\n",
      "|      4|           3|        1659304800|      1659304900|[613619, 298827, ...|[1659304800, 1659...|           [0, 0, 2]|\n",
      "+-------+------------+------------------+----------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
