{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import ArrayType, FloatType, LongType, IntegerType\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 ms, sys: 16.3 ms, total: 29.8 ms\n",
      "Wall time: 4.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = SparkSession.builder.appName(\"reranking_model\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: long (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- gt_type: integer (nullable = false)\n",
      " |-- gt: integer (nullable = false)\n",
      "\n",
      "CPU times: user 11.2 ms, sys: 4.39 ms, total: 15.6 ms\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clicks_labels = spark.read.json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\",lineSep='\\n')\\\n",
    "                                .withColumn(\"aid\", col(\"labels.clicks\"))\\\n",
    "                                .withColumn(\"gt_type\", lit(0))\\\n",
    "                                .withColumn(\"gt\", lit(1))\\\n",
    "                                .select(col(\"session\"), col(\"aid\"), col(\"gt_type\"), col(\"gt\"))\n",
    "\n",
    "carts_labels = spark.read.json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\",lineSep='\\n')\\\n",
    "                                .withColumn(\"aids\", col(\"labels.carts\"))\\\n",
    "                                .withColumn(\"gt_type\", lit(1))\\\n",
    "                                .withColumn(\"gt\", lit(1))\\\n",
    "                                .select(col(\"session\"), explode(col(\"aids\")).alias(\"aid\"), col(\"gt_type\"), col(\"gt\"))\n",
    "\n",
    "orders_labels = spark.read.json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\",lineSep='\\n')\\\n",
    "                                .withColumn(\"aids\", col(\"labels.orders\"))\\\n",
    "                                .withColumn(\"gt_type\", lit(2))\\\n",
    "                                .withColumn(\"gt\", lit(1))\\\n",
    "                                .select(col(\"session\"), explode(col(\"aids\")).alias(\"aid\"), col(\"gt_type\"), col(\"gt\"))\n",
    "\n",
    "gt_labels = clicks_labels.union(carts_labels.union(orders_labels))\n",
    "\n",
    "gt_labels.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: string (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- pred_type: integer (nullable = false)\n",
      "\n",
      "CPU times: user 14.6 ms, sys: 5.11 ms, total: 19.7 ms\n",
      "Wall time: 3.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clicks_pred = spark.read.csv(\"../../allData/validationData/phaseII_80_items_preranking.csv\", header=True)\\\n",
    "                        .withColumn(\"session\", split(col(\"session_type\"), '_').getItem(0))\\\n",
    "                        .withColumn(\"action\", split(col(\"session_type\"), \"_\").getItem(1))\\\n",
    "                        .drop(\"session_type\")\\\n",
    "                        .filter(col(\"action\") == \"clicks\")\\\n",
    "                        .drop(\"action\")\\\n",
    "                        .withColumnRenamed(\"labels\", \"clicks_predict\")\\\n",
    "                        .withColumn(\"clicks_predict\", split(col(\"clicks_predict\"), ' ').cast(\"array<long>\"))\\\n",
    "                        .select(col(\"session\"), explode(col(\"clicks_predict\")).alias(\"aid\"))\\\n",
    "                        .withColumn(\"pred_type\", lit(0))\n",
    "\n",
    "carts_pred = spark.read.csv(\"../../allData/validationData/phaseII_80_items_preranking.csv\", header=True)\\\n",
    "                        .withColumn(\"session\", split(col(\"session_type\"), '_').getItem(0))\\\n",
    "                        .withColumn(\"action\", split(col(\"session_type\"), \"_\").getItem(1))\\\n",
    "                        .drop(\"session_type\")\\\n",
    "                        .filter(col(\"action\") == \"carts\")\\\n",
    "                        .drop(\"action\")\\\n",
    "                        .withColumnRenamed(\"labels\", \"carts_predict\")\\\n",
    "                        .withColumn(\"carts_predict\", split(col(\"carts_predict\"), ' ').cast(\"array<long>\"))\\\n",
    "                        .select(col(\"session\"), explode(col(\"carts_predict\")).alias(\"aid\"))\\\n",
    "                        .withColumn(\"pred_type\", lit(1))\n",
    "                    \n",
    "orders_pred = spark.read.csv(\"../../allData/validationData/phaseII_80_items_preranking.csv\", header=True)\\\n",
    "                        .withColumn(\"session\", split(col(\"session_type\"), '_').getItem(0))\\\n",
    "                        .withColumn(\"action\", split(col(\"session_type\"), \"_\").getItem(1))\\\n",
    "                        .drop(\"session_type\")\\\n",
    "                        .filter(col(\"action\") == \"orders\")\\\n",
    "                        .drop(\"action\")\\\n",
    "                        .withColumnRenamed(\"labels\", \"orders_predict\")\\\n",
    "                        .withColumn(\"orders_predict\", split(col(\"orders_predict\"), ' ').cast(\"array<long>\"))\\\n",
    "                        .select(col(\"session\"), explode(col(\"orders_predict\")).alias(\"aid\"))\\\n",
    "                        .withColumn(\"pred_type\", lit(2))\n",
    "\n",
    "pred_labels = clicks_pred.union(carts_pred.union(orders_pred))\n",
    "\n",
    "pred_labels.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: string (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- pred_type: integer (nullable = false)\n",
      " |-- gt_type: integer (nullable = true)\n",
      " |-- gt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf = pred_labels.join(gt_labels, [\"session\", \"aid\"], \"left\").na.fill(value=0, subset=[\"gt\"])\n",
    "fullDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+-------+---+\n",
      "| session|    aid|pred_type|gt_type| gt|\n",
      "+--------+-------+---------+-------+---+\n",
      "|11098528| 258814|        0|   null|  0|\n",
      "|11098528| 258814|        1|   null|  0|\n",
      "|11098528| 258814|        2|   null|  0|\n",
      "|11098528| 735729|        0|   null|  0|\n",
      "|11098528| 735729|        1|   null|  0|\n",
      "|11098528| 735729|        2|   null|  0|\n",
      "|11098528| 756588|        0|   null|  0|\n",
      "|11098528| 756588|        1|   null|  0|\n",
      "|11098528| 756588|        2|   null|  0|\n",
      "|11114626|1628790|        1|   null|  0|\n",
      "|11114626|1811058|        1|   null|  0|\n",
      "|11133705| 479970|        2|   null|  0|\n",
      "|11133705|1169176|        2|   null|  0|\n",
      "|11133705|1527102|        2|   null|  0|\n",
      "|11323773| 684656|        0|   null|  0|\n",
      "|11323773|1173032|        0|   null|  0|\n",
      "|11340321|1535361|        1|   null|  0|\n",
      "|11359041| 987258|        2|   null|  0|\n",
      "|11359041|1723722|        2|   null|  0|\n",
      "|11555514| 300193|        0|   null|  0|\n",
      "+--------+-------+---------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: string (nullable = true)\n",
      " |-- total_action: string (nullable = true)\n",
      " |-- session_start_time: string (nullable = true)\n",
      " |-- session_end_time: string (nullable = true)\n",
      " |-- session_time_lapse: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaInfo = spark.read.csv(\"../../allData/validationData/test_meta_data.csv\", header=True)\\\n",
    "                     .withColumn(\"session_time_lapse\", col(\"session_end_time\") - col(\"session_start_time\"))\n",
    "metaInfo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------------+----------------+\n",
      "| session|total_action|session_start_time|session_end_time|\n",
      "+--------+------------+------------------+----------------+\n",
      "|11098528|           1|        1661119200|      1661119200|\n",
      "|11098529|           1|        1661119200|      1661119200|\n",
      "|11098530|           6|        1661119200|      1661120532|\n",
      "|11098531|          24|        1661119200|      1661119746|\n",
      "|11098532|           2|        1661119201|      1661119996|\n",
      "|11098533|          17|        1661119201|      1661159615|\n",
      "|11098534|           7|        1661119202|      1661120868|\n",
      "|11098535|          10|        1661119202|      1661173474|\n",
      "|11098536|           7|        1661119202|      1661119932|\n",
      "|11098537|          23|        1661119202|      1661122991|\n",
      "+--------+------------+------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metaInfo.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: long (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- action_times: array (nullable = false)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- action_types: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- seq_orders: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- total_prev_interacts: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(col(\"session\")).orderBy(col(\"ts\").asc())  ## using row_number(), the first action in the session will have seq_order = 1\n",
    "\n",
    "inputDataDf = spark.read.json(\"../../allData/validationData/out_7day_test/test_sessions.jsonl\", lineSep='\\n')\\\n",
    "                        .select(col(\"session\"), explode(col(\"events\")).alias(\"events\"))\\\n",
    "                        .withColumn(\"aid\", col(\"events.aid\"))\\\n",
    "                        .withColumn(\"ts\", col(\"events.ts\"))\\\n",
    "                        .withColumn(\"type\", when(col(\"events.type\")=='clicks', 0).when(col(\"events.type\")=='carts', 1).otherwise(2))\\\n",
    "                        .drop(\"events\")\\\n",
    "                        .withColumn(\"seq_order\", row_number().over(window_spec))\\\n",
    "                        .groupBy(\"session\", \"aid\")\\\n",
    "                        .agg(collect_list(\"ts\").alias(\"action_times\"), collect_list(\"type\").alias(\"action_types\"), collect_list(\"seq_order\").alias(\"seq_orders\"))\\\n",
    "                        .withColumn(\"total_prev_interacts\", size(col(\"action_types\")))\n",
    "\n",
    "inputDataDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+------------+----------+\n",
      "| session|    aid|        action_times|action_types|seq_orders|\n",
      "+--------+-------+--------------------+------------+----------+\n",
      "|11098534| 223062|[1661119496657, 1...|   [0, 0, 0]| [2, 5, 6]|\n",
      "|11098534| 908024|     [1661120868194]|         [0]|       [7]|\n",
      "|11098534|1342293|     [1661119561408]|         [0]|       [4]|\n",
      "|11098534|1449202|     [1661119202009]|         [0]|       [1]|\n",
      "|11098534|1607945|     [1661119527004]|         [0]|       [3]|\n",
      "|11098538|  90427|     [1661362146282]|         [0]|      [18]|\n",
      "|11098538| 218349|     [1661361943900]|         [0]|      [16]|\n",
      "|11098538| 388376|     [1661119202628]|         [0]|       [1]|\n",
      "|11098538| 523982|     [1661362852681]|         [0]|      [20]|\n",
      "|11098538| 649126|     [1661363042725]|         [0]|      [22]|\n",
      "+--------+-------+--------------------+------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputDataDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: string (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- pred_type: integer (nullable = false)\n",
      " |-- gt_type: integer (nullable = true)\n",
      " |-- gt: integer (nullable = true)\n",
      " |-- total_action: string (nullable = true)\n",
      " |-- session_start_time: string (nullable = true)\n",
      " |-- session_end_time: string (nullable = true)\n",
      " |-- action_times: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- action_types: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- seq_orders: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf = fullDf.join(metaInfo, [\"session\"], \"left\")\\\n",
    "               .join(inputDataDf, [\"session\", \"aid\"], \"left\")\n",
    "\n",
    "fullDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---------+-------+---+------------+------------------+----------------+---------------+------------+----------+\n",
      "| session|    aid|pred_type|gt_type| gt|total_action|session_start_time|session_end_time|   action_times|action_types|seq_orders|\n",
      "+--------+-------+---------+-------+---+------------+------------------+----------------+---------------+------------+----------+\n",
      "|11098528| 258814|        0|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 258814|        1|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 258814|        2|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 735729|        0|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 735729|        1|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 735729|        2|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 756588|        0|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 756588|        1|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 756588|        2|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11133705| 479970|        2|   null|  0|           1|        1661151714|      1661151714|           null|        null|      null|\n",
      "|11133705|1169176|        2|   null|  0|           1|        1661151714|      1661151714|           null|        null|      null|\n",
      "|11133705|1527102|        2|   null|  0|           1|        1661151714|      1661151714|           null|        null|      null|\n",
      "|11323773| 684656|        0|   null|  0|           1|        1661193259|      1661193259|           null|        null|      null|\n",
      "|11323773|1173032|        0|   null|  0|           1|        1661193259|      1661193259|           null|        null|      null|\n",
      "|11098528|  11830|        0|      2|  1|           1|        1661119200|      1661119200|[1661119200060]|         [0]|       [1]|\n",
      "|11098528|  11830|        1|      2|  1|           1|        1661119200|      1661119200|[1661119200060]|         [0]|       [1]|\n",
      "|11098528|  11830|        2|      2|  1|           1|        1661119200|      1661119200|[1661119200060]|         [0]|       [1]|\n",
      "|11098528| 571762|        0|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 571762|        1|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "|11098528| 571762|        2|   null|  0|           1|        1661119200|      1661119200|           null|        null|      null|\n",
      "+--------+-------+---------+-------+---+------------+------------------+----------------+---------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fullDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: long (nullable = true)\n",
      " |-- aid: long (nullable = true)\n",
      " |-- action_times: array (nullable = false)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- action_types: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- seq_orders: array (nullable = false)\n",
      " |    |-- element: integer (containsNull = false)\n",
      " |-- total_prev_interacts: integer (nullable = false)\n",
      " |-- total_action: string (nullable = true)\n",
      " |-- session_start_time: string (nullable = true)\n",
      " |-- session_end_time: string (nullable = true)\n",
      " |-- session_time_lapse: double (nullable = true)\n",
      "\n",
      "+--------+-------+--------------------+------------+----------+--------------------+------------+------------------+----------------+------------------+\n",
      "| session|    aid|        action_times|action_types|seq_orders|total_prev_interacts|total_action|session_start_time|session_end_time|session_time_lapse|\n",
      "+--------+-------+--------------------+------------+----------+--------------------+------------+------------------+----------------+------------------+\n",
      "|11098534| 223062|[1661119496657, 1...|   [0, 0, 0]| [2, 5, 6]|                   3|           7|        1661119202|      1661120868|            1666.0|\n",
      "|11098534| 908024|     [1661120868194]|         [0]|       [7]|                   1|           7|        1661119202|      1661120868|            1666.0|\n",
      "|11098534|1342293|     [1661119561408]|         [0]|       [4]|                   1|           7|        1661119202|      1661120868|            1666.0|\n",
      "|11098534|1449202|     [1661119202009]|         [0]|       [1]|                   1|           7|        1661119202|      1661120868|            1666.0|\n",
      "|11098534|1607945|     [1661119527004]|         [0]|       [3]|                   1|           7|        1661119202|      1661120868|            1666.0|\n",
      "+--------+-------+--------------------+------------+----------+--------------------+------------+------------------+----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "CPU times: user 3.04 ms, sys: 1.76 ms, total: 4.8 ms\n",
      "Wall time: 8.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp = inputDataDf.join(metaInfo, \"session\", \"left\")\n",
    "temp.printSchema()\n",
    "temp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o221.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job 18 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o221.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job 18 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 52122)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/itong1900/opt/anaconda3/lib/python3.8/site-packages/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "temp.write.parquet(\"../../allData/reranking/dummy_4.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.96 ms, sys: 1.62 ms, total: 4.58 ms\n",
      "Wall time: 23.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "inputDataDf.write.parquet(\"../../allData/reranking/dummy_3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.8 ms, sys: 11 ms, total: 36.8 ms\n",
      "Wall time: 5.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "groundTruthLabelsDf = spark.read.json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\",lineSep='\\n')\\\n",
    "                         .withColumn(\"clicks_answer\", col(\"labels.clicks\"))\\\n",
    "                         .withColumn(\"carts_answer\", col(\"labels.carts\"))\\\n",
    "                         .withColumn(\"orders_answer\", col(\"labels.orders\"))\\\n",
    "                         .drop(\"labels\")\n",
    "\n",
    "## =========================================\n",
    "## =========================================\n",
    "\n",
    "rawPredictionsDf = spark.read.csv(\"../../allData/validationData/phaseII_80_items_preranking.csv\", header=True)\\\n",
    "                        .withColumn(\"session\", split(col(\"session_type\"), '_').getItem(0))\\\n",
    "                        .withColumn(\"action\", split(col(\"session_type\"), \"_\").getItem(1))\\\n",
    "                        .drop(\"session_type\")\n",
    "\n",
    "clicksPredictionDf = rawPredictionsDf.filter(col(\"action\") == \"clicks\")\\\n",
    "                                     .drop(\"action\")\\\n",
    "                                     .withColumnRenamed(\"labels\", \"clicks_predict\")\\\n",
    "                                     .withColumn(\"clicks_predict\", split(col(\"clicks_predict\"), ' ').cast(\"array<long>\"))\\\n",
    "                                    \n",
    "cartsPredictionDf = rawPredictionsDf.filter(col(\"action\") == \"carts\")\\\n",
    "                                    .drop(\"action\")\\\n",
    "                                    .withColumnRenamed(\"labels\", \"carts_predict\")\\\n",
    "                                    .withColumn(\"carts_predict\", split(col(\"carts_predict\"), ' ').cast(\"array<long>\"))\n",
    "\n",
    "ordersPredictionDf = rawPredictionsDf.filter(col(\"action\") == \"orders\")\\\n",
    "                                     .drop(\"action\")\\\n",
    "                                     .withColumnRenamed(\"labels\", \"orders_predict\")\\\n",
    "                                     .withColumn(\"orders_predict\", split(col(\"orders_predict\"), ' ').cast(\"array<long>\"))\n",
    "\n",
    "combinePredictionDf = clicksPredictionDf.join(cartsPredictionDf, \"session\")\\\n",
    "                                        .join(ordersPredictionDf, \"session\")\\\n",
    "                                        .withColumn(\"clicks_predict\", slice(col(\"clicks_predict\"), 0, 40))\n",
    "                                        \n",
    "                           \n",
    "\n",
    "\n",
    "## ========================================\n",
    "## ========================================\n",
    "inputDataDf = spark.read.json(\"../../allData/validationData/out_7day_test/test_sessions.jsonl\", lineSep='\\n')\\\n",
    "                        .select(\"session\", explode(\"events\").alias(\"events\"))\n",
    "\n",
    "clicksInputDf = inputDataDf.filter(col(\"events.type\") == \"clicks\")\\\n",
    "                           .withColumn(\"aid\", col(\"events.aid\"))\\\n",
    "                           .groupBy(\"session\")\\\n",
    "                           .agg(collect_list(\"aid\").alias(\"clicks_input\"))\n",
    "\n",
    "cartsInputDf = inputDataDf.filter(col(\"events.type\") == \"carts\")\\\n",
    "                           .withColumn(\"aid\", col(\"events.aid\"))\\\n",
    "                           .groupBy(\"session\")\\\n",
    "                           .agg(collect_list(\"aid\").alias(\"carts_input\"))\n",
    "\n",
    "ordersInputDf = inputDataDf.filter(col(\"events.type\") == \"orders\")\\\n",
    "                           .withColumn(\"aid\", col(\"events.aid\"))\\\n",
    "                           .groupBy(\"session\")\\\n",
    "                           .agg(collect_list(\"aid\").alias(\"orders_input\"))\n",
    "\n",
    "inputCombinedDf = clicksInputDf.join(cartsInputDf, \"session\", \"outer\")\\\n",
    "                               .join(ordersInputDf, \"session\", \"outer\")\n",
    "\n",
    "## ============================================\n",
    "## ===========================================\n",
    "validationDf = groundTruthLabelsDf.join(combinePredictionDf, \"session\", \"left\")\\\n",
    "                                  .join(inputCombinedDf, \"session\", \"left\")\\\n",
    "                                  .withColumn(\"input_len\", size(col(\"clicks_input\")) + size(col(\"carts_input\")) + size(col(\"orders_input\")))\\\n",
    "                                     .withColumn(\"correct_click_pred\", array_contains(col(\"clicks_predict\"), col(\"clicks_answer\")))\\\n",
    "                                     .withColumn(\"correct_cart_pred\", least(size(array_intersect(col(\"carts_predict\"), col(\"carts_answer\"))), lit(20)))\\\n",
    "                                     .withColumn(\"cart_recall_score\", col(\"correct_cart_pred\")/least(size(col(\"carts_answer\")), lit(20)))\\\n",
    "                                     .withColumn(\"correct_order_pred\", least(size(array_intersect(col(\"orders_predict\"), col(\"orders_answer\"))), lit(20)))\\\n",
    "                                     .withColumn(\"order_recall_score\", col(\"correct_order_pred\")/least(size(col(\"orders_answer\")), lit(20)))\\\n",
    "                                     .select(col(\"session\"),\n",
    "                                             col(\"clicks_input\"),\n",
    "                                             col(\"clicks_predict\"),\n",
    "                                             col(\"clicks_answer\"),\n",
    "                                             col(\"correct_click_pred\"),\n",
    "                                             col(\"carts_input\"),\n",
    "                                             col(\"carts_predict\"),\n",
    "                                             col(\"carts_answer\"),\n",
    "                                             col(\"correct_cart_pred\"),\n",
    "                                             col(\"cart_recall_score\"),\n",
    "                                             col(\"orders_input\"),\n",
    "                                             col(\"orders_predict\"),\n",
    "                                             col(\"orders_answer\"),\n",
    "                                             col(\"correct_order_pred\"),\n",
    "                                             col(\"order_recall_score\"),\n",
    "                                             col(\"input_len\"))                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------------------+\n",
      "|numerator|denominator|carts_recall_sanity_check|\n",
      "+---------+-----------+-------------------------+\n",
      "|   279937|     566105|       0.4944966039868929|\n",
      "+---------+-----------+-------------------------+\n",
      "\n",
      "CPU times: user 12.4 ms, sys: 6 ms, total: 18.4 ms\n",
      "Wall time: 46.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Ideal case: Ceiling of optimizing the pre-ranking result\n",
    "## carts\n",
    "temp = validationDf.filter(~col(\"carts_answer\").isNull())\\\n",
    "                     .select(col(\"session\"), col(\"correct_cart_pred\") ,col(\"carts_answer\"), col(\"carts_predict\"))\\\n",
    "                     .withColumn(\"denominator_factor\", least(lit(20), size(col(\"carts_answer\"))))\\\n",
    "                     .agg(sum(\"correct_cart_pred\").alias(\"numerator\"),\n",
    "                          sum(\"denominator_factor\").alias(\"denominator\"))\\\n",
    "                     .withColumn(\"carts_recall_sanity_check\", col(\"numerator\")/col(\"denominator\"))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------------+\n",
      "|numerator|denominator|orders_recall_sanity_check|\n",
      "+---------+-----------+--------------------------+\n",
      "|   217149|     310905|        0.6984416461620109|\n",
      "+---------+-----------+--------------------------+\n",
      "\n",
      "CPU times: user 10.5 ms, sys: 7.02 ms, total: 17.5 ms\n",
      "Wall time: 34.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## orders\n",
    "temp = validationDf.filter(~col(\"orders_answer\").isNull())\\\n",
    "                     .select(col(\"session\"), col(\"correct_order_pred\") ,col(\"orders_answer\"), col(\"orders_predict\"))\\\n",
    "                     .withColumn(\"denominator_factor\", least(lit(20), size(col(\"orders_answer\"))))\\\n",
    "                     .agg(sum(\"correct_order_pred\").alias(\"numerator\"),\n",
    "                          sum(\"denominator_factor\").alias(\"denominator\"))\\\n",
    "                     .withColumn(\"orders_recall_sanity_check\", col(\"numerator\")/col(\"denominator\"))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------------------+\n",
      "|numerator|denominator|clicks_recall_sanity_check|\n",
      "+---------+-----------+--------------------------+\n",
      "|  1102728|    1737968|        0.6344926949172827|\n",
      "+---------+-----------+--------------------------+\n",
      "\n",
      "CPU times: user 9.7 ms, sys: 4.93 ms, total: 14.6 ms\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## clicks\n",
    "temp = validationDf.filter(~col(\"clicks_answer\").isNull())\\\n",
    "                     .select(col(\"session\"), col(\"correct_click_pred\") ,col(\"clicks_answer\"), col(\"clicks_predict\"))\\\n",
    "                     .withColumn(\"denominator_factor\", lit(1))\\\n",
    "                     .agg(sum(col(\"correct_click_pred\").cast(IntegerType())).alias(\"numerator\"),\n",
    "                          sum(\"denominator_factor\").alias(\"denominator\"))\\\n",
    "                     .withColumn(\"clicks_recall_sanity_check\", col(\"numerator\")/col(\"denominator\"))\n",
    "temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "We want the training / validation / test data in the following schema:\n",
    "3 models will be trained(1 for clicks predictions, 1 for carts predictions, 1 for orders predictions)\n",
    "1. labels: labels will be binary, either if they actually appear or not. The goal is to reach ceiling of recall rates described above.\n",
    "2. identifier: aid_session_id, uniquely defines if an aid should be added to the final 20 in a session.\n",
    "3. Features:  \n",
    "    a. Has this aid appear in this session before?  \n",
    "    b. Previous interaction type counts, clicks #, carts #, orders #  \n",
    "    c. Associated action(interacted earlier -> neareast interactions, similar item been interacted -> ) ts:   \n",
    "    d. seq  \n",
    "4. Session Features:  \n",
    "    a. session_len  \n",
    "    b. log_recency_score  \n",
    "    c. type_weighted_log_recency_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: long (nullable = true)\n",
      " |-- clicks_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- clicks_predict: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- clicks_answer: long (nullable = true)\n",
      " |-- correct_click_pred: boolean (nullable = true)\n",
      " |-- carts_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- carts_predict: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- carts_answer: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- correct_cart_pred: integer (nullable = false)\n",
      " |-- cart_recall_score: double (nullable = true)\n",
      " |-- orders_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- orders_predict: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- orders_answer: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- correct_order_pred: integer (nullable = false)\n",
      " |-- order_recall_score: double (nullable = true)\n",
      " |-- input_len: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "validationDf.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clicks Reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session: long (nullable = true)\n",
      " |-- clicks_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- carts_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- orders_input: array (nullable = true)\n",
      " |    |-- element: long (containsNull = false)\n",
      " |-- clicks_answer: long (nullable = true)\n",
      " |-- action_aid: long (nullable = true)\n",
      " |-- ground_truth: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def count_occ(arr, value):\n",
    "#     res = 0\n",
    "#     for ele in array:\n",
    "#         if ele == value:\n",
    "#             res += 1\n",
    "#     return res\n",
    "\n",
    "# count_occ_udf = udf(lambda arr, value: count_occ(arr, value), IntegerType())\n",
    "\n",
    "clicksFullDf = validationDf.select(col(\"session\"), col(\"clicks_input\"), col(\"carts_input\"), col(\"orders_input\"), col(\"clicks_answer\"), explode(\"clicks_predict\").alias(\"action_aid\"))\\\n",
    "                           .withColumn(\"input_len\", size(col(\"clicks_input\")) + size(col(\"carts_input\")) + size(col(\"orders_input\")))\\\n",
    "                           .withColumn(\"has_clicked_before\", array_contains(col(\"clicks_input\"), col(\"action_aid\")))\\\n",
    "                           .withColumn(\"ground_truth\", col(\"clicks_answer\") == col(\"action_aid\"))\\\n",
    "                           .select(col(\"session\"), col(\"clicks_input\"), col(\"carts_input\"), col(\"orders_input\"), col(\"clicks_answer\"), col(\"action_aid\"), col(\"ground_truth\"))\n",
    "\n",
    "clicksFullDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o589.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 51.0 failed 1 times, most recent failure: Lost task 5.0 in stage 51.0 (TID 276) (yitongs-macbook.attlocal.net executor driver): java.lang.RuntimeException: Unexpected value for start in function slice: SQL array indices start at 1.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala:1260)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: Unexpected value for start in function slice: SQL array indices start at 1.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala:1260)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-487c4c0cae90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclicksFullDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    613\u001b[0m                 )\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o589.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 51.0 failed 1 times, most recent failure: Lost task 5.0 in stage 51.0 (TID 276) (yitongs-macbook.attlocal.net executor driver): java.lang.RuntimeException: Unexpected value for start in function slice: SQL array indices start at 1.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala:1260)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\nCaused by: java.lang.RuntimeException: Unexpected value for start in function slice: SQL array indices start at 1.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala:1260)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.unexpectedValueForStartInFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "clicksFullDf.show(1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o336.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job 37 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c9e35a035494>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclicksFullDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../../allData/reranking/clicksFullDf.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m     def text(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o336.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\nCaused by: org.apache.spark.SparkException: Job 37 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1188)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1186)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1186)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2887)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2784)\n\tat org.apache.spark.SparkContext.$anonfun$stop$11(SparkContext.scala:2095)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1484)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2095)\n\tat org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:660)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2066)\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:245)\n\t... 42 more\n"
     ]
    }
   ],
   "source": [
    "clicksFullDf.write.parquet(\"../../allData/reranking/clicksFullDf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
