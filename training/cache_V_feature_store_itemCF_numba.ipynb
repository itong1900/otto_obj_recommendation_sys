{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the baseline itemCF_numba, and add feature store function to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.6 s, sys: 2.23 s, total: 10.8 s\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/validationData/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/validationData/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/validationData/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/validationData/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661103727</td>\n",
       "      <td>0</td>\n",
       "      <td>1661103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660857067</td>\n",
       "      <td>147</td>\n",
       "      <td>1660857067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660577379</td>\n",
       "      <td>174</td>\n",
       "      <td>1660577379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661109666</td>\n",
       "      <td>187</td>\n",
       "      <td>1661109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1659304900</td>\n",
       "      <td>413</td>\n",
       "      <td>1659304900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  total_action  session_start_time  session_end_time  start_idx  \\\n",
       "0        0           147          1659304800        1661103727          0   \n",
       "1        1            27          1659304800        1660857067        147   \n",
       "2        2            13          1659304800        1660577379        174   \n",
       "3        3           226          1659304800        1661109666        187   \n",
       "4        4             3          1659304800        1659304900        413   \n",
       "\n",
       "     end_time  \n",
       "0  1661103727  \n",
       "1  1660857067  \n",
       "2  1660577379  \n",
       "3  1661109666  \n",
       "4  1659304900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 200   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "            elif mode == \"buy2buy\":\n",
    "                if (ops[i] == 0) or (ops[j] == 0):\n",
    "                    continue\n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 s, sys: 473 ms, total: 21.1 s\n",
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [01:47<26:06,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1999/12079 [04:36<31:34:09, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2999/12079 [06:56<16:56:17,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3998/12079 [08:27<14:44,  9.14it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 4998/12079 [11:45<11:27, 10.30it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 6000/12079 [13:58<9:39:18,  5.72s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6997/12079 [16:04<05:58, 14.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 7999/12079 [18:04<18:54:06, 16.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 8999/12079 [20:03<14:51:11, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 9999/12079 [21:58<4:56:28,  8.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10998/12079 [22:36<00:29, 37.00it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11999/12079 [24:54<05:29,  4.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12079/12079 [24:56<00:00,  8.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45min 8s, sys: 19min 51s, total: 1h 4min 59s\n",
      "Wall time: 26min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\"] #, \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 150)\n",
    "            gc.collect()\n",
    "            # break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 150)   ## TODO: make this num small enough to reduce time for normalization, consider keeping 100, give more option for selection\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[int64,float64]<iv=None>({331941: 1.0, 371417: 0.5888439497177552, 32249: 0.42943762435324906, 303302: 0.2151606169331991, 461689: 0.21463847720057422, 1371202: 0.18500509000648885, 1775482: 0.09952497694341997, 1765072: 0.07861434634553799, 1853268: 0.008780961279884873, 1231891: 0.003965427852916738, 989590: 0.002005855472564233, 320601: 0.0011189360665819362, 1190046: 0.0001025840881962515, 1236142: 1.6958427526431777e-06})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A sanity check\n",
    "simMatrices[\"buy2buy\"][1517085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of iuf sim matrix 1818001\n",
      "len of iuf sim matrix 999742\n"
     ]
    }
   ],
   "source": [
    "print(\"len of iuf sim matrix\" ,len(simMatrices[\"iuf\"]))\n",
    "print(\"len of iuf sim matrix\" ,len(simMatrices[\"buy2buy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4097"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference -- Make prediction using the matrices derived from above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section E: Main Logic in Making Inferences (DO NOT RUN in this notebook)\n",
    "1. clicks_inferences: time_decay sim matrix + regular action weights <1, 6, 3>.\n",
    "2. carts_inferencs: iuf sim matrix + weights <4, 2, 5> (as clicks actions tend to lead to cart action next).\n",
    "3. orders_inferences: iuf sim matrix + regular action weights <1, 6, 3>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length\n",
    "    end_time = ts[ending_idx]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += np.inf #seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 20))\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def run_inference_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# result_iuf = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_iuf_2 = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "#     start_row = row_idx\n",
    "#     end_row = min(row_idx + PARALLEL, len(df))\n",
    "#     rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_iuf, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_iuf_2, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))   ## considebly add the weights for click action in the real time.\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_time_decay, simMatrices[\"time_decay\"], np.array([3.0, 6.0, 3.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions - Convert results to csv, get validation set result - DO NOT RUN in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# subs = []\n",
    "# op_names = [\"clicks\", \"carts\", \"orders\"]\n",
    "\n",
    "# for result, op in zip([result_time_decay, result_iuf_2, result_iuf], op_names):\n",
    "#     sub = pd.DataFrame({\"session_type\": result.keys(), \"labels\": result.values()})\n",
    "#     sub.session_type = sub.session_type.astype(str) + f\"_{op}\"\n",
    "#     sub.labels = sub.labels.apply(lambda x: \" \".join(x.astype(str)))\n",
    "#     subs.append(sub)\n",
    "    \n",
    "# submission = pd.concat(subs).reset_index(drop=True)\n",
    "# #sub.sort_values(by=[\"session_type\"])  ## optional\n",
    "# #submission.to_csv('submission.csv', index = False)\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# submission.to_csv('../../allData/validationData/p_v_579_80_items.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Saving Feature, Run E or F, depends on mode, this will not only generate result, but the featues associated.\n",
    "**Feature engineering are made here**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nb.jit(nopython=True)\n",
    "def getLocalTsInfo(utc_ts, timezone):\n",
    "    local_times_info = datetime.fromtimestamp(utc_ts, pytz.timezone(timezone))\n",
    "    if (local_times_info.hour >= 18) or (local_times_info.hour <= 2):\n",
    "        day_noon_night = 2\n",
    "    elif (local_times_info.hour >= 3) and (local_times_info.hour <= 11):\n",
    "        day_noon_night = 1\n",
    "    else: ## (local_times_info.hour >= 12) and (local_times_info.hour <= 15)\n",
    "        day_noon_night = 0\n",
    "    return local_times_info.weekday(), local_times_info.hour, day_noon_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LA (0, 8, 1)\n",
      "Berlin: (0, 17, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"LA\", getLocalTsInfo(1659367439, 'America/Los_Angeles'))\n",
    "print(\"Berlin:\", getLocalTsInfo(1659367439, 'Europe/Berlin'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def update_feature_vec(aid, features_tuple_arr, features_idx_map, new_feat_tuple):\n",
    "    ## append features\n",
    "    if aid not in features_idx_map:\n",
    "        features_tuple_arr.append(new_feat_tuple)\n",
    "        new_pos = len(features_tuple_arr)-1\n",
    "        ## save the position in the tuple arr\n",
    "        features_idx_map[aid] = new_pos\n",
    "    else: # <is_prev_int, seq_w, time_w, total_ops_w, session_len, # uniuqe aids, CF_score, aid's itemTotalLike >\n",
    "        # ================== 8 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8]\n",
    "        else:\n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8]\n",
    "        # ================== 9 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot9_max_sim = 1\n",
    "        else:\n",
    "            slot9_max_sim = max(new_feat_tuple[9], features_tuple_arr[features_idx_map[aid]][9])\n",
    "        # ================== 10 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot10_mean_sim = 1\n",
    "        else:\n",
    "            slot10_mean_sim = ((features_tuple_arr[features_idx_map[aid]][10] * (features_tuple_arr[features_idx_map[aid]][8]-1) ) + new_feat_tuple[10] ) / features_tuple_arr[features_idx_map[aid]][8]\n",
    "        # ================== 14 seq_w ==\n",
    "        slot14_max_seq_w = max(new_feat_tuple[14], features_tuple_arr[features_idx_map[aid]][14])\n",
    "        # ================== 15 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================== 16 ==\n",
    "        slot16_min_seq_w = min(new_feat_tuple[16], features_tuple_arr[features_idx_map[aid]][16])\n",
    "        # ================== 17 time_w == \n",
    "        slot17_max_time_w = max(new_feat_tuple[17], features_tuple_arr[features_idx_map[aid]][17])\n",
    "        # ================== 18 == \n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 19 ==\n",
    "        slot19_min_time_w = min(new_feat_tuple[19], features_tuple_arr[features_idx_map[aid]][19])\n",
    "        # ================= 20 ops_w ==\n",
    "        slot20_max_ops_w = max(new_feat_tuple[20], features_tuple_arr[features_idx_map[aid]][20])\n",
    "        # ================= 21 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 22 ==\n",
    "        slot22_min_ops_w = min(new_feat_tuple[22], features_tuple_arr[features_idx_map[aid]][22]) #new_feat_tuple[22] if new_feat_tuple[22] < features_tuple_arr[features_idx_map[aid]][22] else features_tuple_arr[features_idx_map[aid]][22]\n",
    "        # ================= 28 cf_incre ==\n",
    "        slot28_max_cf_incre = max(new_feat_tuple[28], features_tuple_arr[features_idx_map[aid]][28])\n",
    "        # ================= 29 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 30 ==\n",
    "        slot30_min_cf_incre = min(new_feat_tuple[30], features_tuple_arr[features_idx_map[aid]][30])\n",
    "\n",
    "        \n",
    "        features_tuple_arr[features_idx_map[aid]] = (new_feat_tuple[0], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3],\n",
    "                                                     new_feat_tuple[4],\n",
    "                                                     new_feat_tuple[5],\n",
    "                                                     new_feat_tuple[6],\n",
    "                                                     new_feat_tuple[7],\n",
    "                                                     slot8_ref_time,\n",
    "                                                     slot9_max_sim,\n",
    "                                                     slot10_mean_sim,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11],\n",
    "                                                     new_feat_tuple[12],\n",
    "                                                     new_feat_tuple[13],\n",
    "                                                     slot14_max_seq_w,\n",
    "                                                     slot15_mean_seq_w,\n",
    "                                                     slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w,\n",
    "                                                     slot18_mean_time_w,\n",
    "                                                     slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w,\n",
    "                                                     slot21_mean_ops_w,\n",
    "                                                     slot22_min_ops_w,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][23] + new_feat_tuple[23], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][24] + new_feat_tuple[24],\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][25] + new_feat_tuple[25],\n",
    "                                                     new_feat_tuple[26],\n",
    "                                                     new_feat_tuple[27],\n",
    "                                                     slot28_max_cf_incre,\n",
    "                                                     slot29_mean_cf_incre,\n",
    "                                                     slot30_min_cf_incre,\n",
    "                                                     slot14_max_seq_w - slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w - slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w - slot22_min_ops_w,\n",
    "                                                     slot28_max_cf_incre - slot30_min_cf_incre\n",
    "                                                     )\n",
    "\n",
    "# <\n",
    "# slot_0: is_prev_interacted, \n",
    "# slot_1: seq_w_total, \n",
    "# slot_2: time_w_total, \n",
    "# slot_3: ops_w_total: for visited item, ops weight total in this session; for unvisited item, ops weight total of the item referencing this item. \n",
    "# slot_4: session_len, \n",
    "# slot_5: num uniuqe aids, \n",
    "# slot_6: CF_score, \n",
    "# slot_7: aid's itemTotalLike: total like score use for normalization.  \n",
    "# slot_8: reference time by similar matrix(if aid visited, default 100; if aid not visited, 1-19(when all aid only interact once, could grow to very large if a lot of actions on one aid), depending how many aid reference this item)\n",
    "# slot_9: max_sim_score:  (1 if it's a visited item)\n",
    "# slot_10: mean_sim_score: (1 if it's a visited item)\n",
    "# slot_11: num_interact, (0 for unvisited item; count of interaction for visited item)\n",
    "# slot_12: time_span of the session \n",
    "# slot_13: action_recency: time to last action(end time), for unvisited items -> the time to of reference_aid to the last action\n",
    "# slot_14: seq_w_max: \n",
    "# slot_15: seq_w_mean: for visited item -> seq_w_total / num_interact; for unvisited item -> seq_w_total / reference_time\n",
    "## ========================= round 2 ================\n",
    "# slot_16: seq_w_min: \n",
    "# slot_17: time_w_max:\n",
    "# slot_18: time_w_mean: similar to slot_15\n",
    "# slot_19: time_w_min\n",
    "# slot_20: ops_w_max:\n",
    "# slot_21: ops_w_mean:\n",
    "# slot_22: ops_w_min: \n",
    "# slot_23: num_clicks: visited item, direct num; unvisited item, take the reference item's num\n",
    "# slot_24: num_carts:\n",
    "# slot_25: num_orders:\n",
    "# slot_26: last_action_type: 0 -> clicks, 1-> carts, 2 -> orders\n",
    "# slot_27: time_to_now: latest interaction time to now \n",
    "# slot_28: cf_increment_max: \n",
    "# slot_29: cf_increment_mean:\n",
    "# slot_30: cf_increment_min:\n",
    "##  ======================== Derivable ============================\n",
    "# slot_31: seq_w max_min_gap: slot_14 - slot_16 \n",
    "# slot_32: time_w_max_min_gap: slot_17 - slot_19\n",
    "# slot_33: ops_w_max_min_gap: slot_20 - slot_22\n",
    "# slot_34: cf_incre_max_min_gap: slot_28 - slot_30\n",
    "## ====================== Round 3 features ========================\n",
    "# slot_35: last3Inter_cf_incre_max: maximum cf_incre in the last 3 action, if <= 3 actions, same as slot_28\n",
    "# slot_36: last3Inter_cf_incre_mean: \n",
    "# slot_37: last3Inter_cf_incre_min:\n",
    "# slot_38: last3Inter_time_w_max:  maximum time_w in the last 3 action, if <= 3 actions, same as slot_17\n",
    "# slot_39: last3Inter_time_w_mean:\n",
    "# slot_40: last3Inter_time_w_min: \n",
    "# slot_41: last3Inter_seq_w_max:  maximum time_w in the last 3 action, if <= 3 actions, same as slot_14\n",
    "# slot_42: last3Inter_seq_w_mean:\n",
    "# slot_43: last3Inter_seq_w_min:\n",
    "# slot_44: raw_seq_order: last action's seq_order, no depreciation\n",
    "# slot_45: raw_seq_order_max: \n",
    "# slot_46: raw_seq_order_mean:\n",
    "# slot_47: raw_seq_order_min\n",
    "\n",
    "# append ts lastly to unblock feature below\n",
    "#  \n",
    "\n",
    "## ================= numba restriction, add in notebook =====================\n",
    "# slot_35: last_interact_local_day_of_week, Mon -> 0; Tues -> 1; ..... ; Sun -> 6\n",
    "# slot_36: last_interact_local_hour: \n",
    "# slot_37: last_interact_day_night, local ts, if 18:00:00 ~ 2:59:59 -> night/2; 3:00:00 ~ 11:59:59 -> morning/1; 12:00:00 ~ 17:59:59 -> afternoon/0\n",
    "# >\n",
    "FEATURE_TUPLE_TEMPLATE = (bool(0), np.float64(0.0), np.float64(0.0), np.int64(0), np.int64(0), np.int64(0), \\\n",
    "    np.float64(0.0), np.float64(0.0), np.int32(0), np.float32(0.0), np.float64(0.0), np.int32(0), np.float64(0.0), np.float64(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "        np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "            np.int32(0), np.int32(0), np.int32(0), np.int32(0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), \\\n",
    "                np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0))\n",
    "\n",
    "FEATURE_NAMES = [\"prev_int\", \"seq_w_total\", \"time_w_total\", \"action_w_total\", \"session_len\", \"num_uniuqe_aids\", \"CF_score\", \"itemTotalLike\", \"ref_time\", \"max_sim_score\",\\\n",
    "    \"mean_sim_score\", \"num_interact\", \"time_span\", \"action_recency\", \"seq_w_max\", \"seq_w_mean\", \"seq_w_min\", \"time_w_max\", \"time_w_mean\", \"time_w_min\", \\\n",
    "        \"ops_w_max\", \"ops_w_mean\", \"ops_w_min\", \"num_clicks\", \"num_carts\", \"num_orders\", \"last_action_type\", \"time_to_now\", \"cf_incre_max\", \"cf_incre_mean\", \\\n",
    "            \"cf_incre_min\", \"seqW_std\", \"timeW_std\", \"actionW_std\", \"cf_incre_std\", \"last3_cfIncre_max\", \"last3_cfIncre_mean\", \"last3_cfIncre_min\", \\\n",
    "                \"last3_timeW_max\", \"last3_timeW_mean\", \"last3_timeW_min\", \"last3_seqW_max\", \"last3_seqW_mean\", \"last3_seqW_min\", \\\n",
    "                    \"last1_seq_order_raw\", \"raw_seq_order_max\", \"raw_seq_order_mean\", \"raw_seq_order_min\", \"last_op_ts\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main feature save logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, item_total_likes, test_ops_weights):\n",
    "    NOW_TIME = ts[-1] ## ts of latest avaiable action\n",
    "    PREV_INTERACT_BONUS = 20\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    time_span = end_time - start_time\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    ## feature vector template: [aid: <is_prev_int, seq_w, time_w, associated_action, session_len,.. >]\n",
    "    features_tuple_arr = nb.typed.List()\n",
    "    features_tuple_arr.append(FEATURE_TUPLE_TEMPLATE)\n",
    "    features_idx_map = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.int64)\n",
    "\n",
    "    helper_idx = starting_idx\n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## caculate scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op]\n",
    "            potential_to_recommend[aid] += cf_incre #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME-ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## get the scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            potential_to_recommend[aid] += cf_incre\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME - ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                \n",
    "                cf_incre = seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]\n",
    "                potential_to_recommend[similar_item] += cf_incre  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                update_feature_vec(similar_item, features_tuple_arr, features_idx_map, \\\n",
    "                    (0, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[similar_item], \\\n",
    "                        item_total_likes[similar_item], 1, full_sim_matrix[aid][similar_item], full_sim_matrix[aid][similar_item], 0, \\\n",
    "                            time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op,\\\n",
    "                                NOW_TIME-ts[helper_idx], cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 100))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "#         features_save[(session, aid)] = features_tuple_arr[features_idx_map[aid]]\n",
    "        feature_tuples_this_session.append(features_tuple_arr[features_idx_map[aid]])\n",
    "    \n",
    "    return feature_tuples_this_session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for batch processing the features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt_tables(type):\n",
    "    \"\"\" type -> carts / orders \"\"\"\n",
    "    gt_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "    gt_labels['aids'] = gt_labels[\"labels\"].apply(lambda x: x.get(type))\n",
    "    gt_labels = gt_labels[gt_labels.aids.notnull()]\n",
    "    gt_labels = gt_labels.drop(\"labels\", axis = 1)\n",
    "    ## ========= special df to identify the unique session id to look at ================\n",
    "    valid_gt_sessions = gt_labels.drop(\"aids\", axis = 1) \n",
    "    ## ========================================================================\n",
    "    ## keep go on for gt labels processing\n",
    "    gt_labels = gt_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "    gt_labels[\"gt\"] = 1\n",
    "    return valid_gt_sessions, gt_labels\n",
    "\n",
    "def process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels):\n",
    "    \"\"\" rawDf -> Df with session, aids(100), feature_tuple \"\"\"\n",
    "    ## join valid_gt_session with rawDf, now only gt_features in valid sessions(have at least 1 aid to predict) are kept\n",
    "    gt_features_valid_session = pd.merge(rawDf, valid_gt_sessions, on=\"session\")\n",
    "\n",
    "    ## Now explode the whole valid_gt_session aids, these session - aid are served as the train/val/test data for the reranker model, \n",
    "    ## for orders, \n",
    "    ## for carts, a total of 569697 correct guesses(not 100% included in the recall)\n",
    "    gt_features_valid_session = gt_features_valid_session.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "\n",
    "    ## finally, attach the gt_lables 1/null to the df to return\n",
    "    final_df = pd.merge(gt_features_valid_session, gt_labels, on=[\"session\", \"aids\"], how='left')\n",
    "\n",
    "    # ## open up the feature tuple \n",
    "    # for slot_id, f_name in enumerate(FEATURE_NAMES):\n",
    "    #     final_df[f_name] = final_df[\"feature_tuple\"].apply(lambda x: x[slot_id])\n",
    "\n",
    "    # new crazy fast method\n",
    "    features = np.vstack(final_df[\"feature_tuple\"].values)\n",
    "    temp_df = pd.DataFrame(features)\n",
    "    del features\n",
    "    temp_df.columns = [f'{feat_name}' for feat_name in FEATURE_NAMES]\n",
    "    final_df[temp_df.columns] = temp_df\n",
    "    del temp_df\n",
    "\n",
    "    final_df = final_df.drop(\"feature_tuple\", axis = 1)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the gt datas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-6045114fd1fb>:58: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1munsafe cast from int64 to bool. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\n",
      "  visit_flag[aid] = 1\n",
      "  8%|▊         | 146/1742 [11:57<30:29:05, 68.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 291/1742 [22:52<10:30:12, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 436/1742 [33:50<10:36:55, 29.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_2 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 581/1742 [44:19<8:22:09, 25.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_3 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 726/1742 [55:10<8:40:06, 30.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_4 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 871/1742 [1:05:35<5:58:08, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_5 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1016/1742 [1:15:48<5:05:43, 25.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_6 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1161/1742 [1:26:31<4:44:40, 29.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_7 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1306/1742 [1:37:05<3:10:22, 26.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_8 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1451/1742 [1:47:48<2:18:23, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_9 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1596/1742 [1:59:01<1:26:46, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_10 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1741/1742 [2:09:25<00:28, 28.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_11 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [2:09:28<00:00,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_12 completes saving.\n",
      "CPU times: user 1h 24min 30s, sys: 14min 3s, total: 1h 38min 33s\n",
      "Wall time: 2h 9min 36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_orders = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1783737 sessions in total, we separate them into K batches\n",
    "K = 12\n",
    "batch_size = 1783737 // K  ##  -> 148644 dealing with around 150k sessions per batch\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (1024 * 145) * i for i in range(1, K+3)]   ## batch process every 1024 * 145 rows\n",
    "# 145 -> 148644 // 1024  -> batch_size // PARALLEL\n",
    "\n",
    "feature_batch_id = 0\n",
    "## load important tables\n",
    "valid_gt_sessions, gt_labels = load_gt_tables(\"orders\")\n",
    "print(\"finish loading the gt datas\")\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session_by_caching(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], item_total_likes, np.array([2.0, 6.0, 6.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    # break\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels)\n",
    "        batch_result.to_parquet(f\"../../allData/features/order_features_V6/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_orders\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_orders = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        \n",
    "    #break\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "## answer to compare\n",
    "# features_all_sessions[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the feature storage pipeline for carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the gt datas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-f3c07b149987>:51: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, float64, float64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, int64, int64, int64, int32, int64, float64, float64, float64, float64, float64, float64, float64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
      "<ipython-input-11-f3c07b149987>:51: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, int64, int64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, bool, bool, bool, int32, int64, float64, float64, float64, int64, int64, int64, int64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
      "<ipython-input-11-f3c07b149987>:83: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, float64, float64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, bool, bool, bool, int32, int64, float64, float64, float64, int64, int64, int64, int64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(similar_item, features_tuple_arr, features_idx_map, \\\n",
      "  8%|▊         | 146/1742 [05:33<32:44:28, 73.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 291/1742 [09:45<18:32:45, 46.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 436/1742 [15:32<27:41:57, 76.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_2 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 581/1742 [19:05<11:45:59, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_3 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 726/1742 [23:39<15:54:44, 56.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_4 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 871/1742 [26:53<7:38:28, 31.58s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_5 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1016/1742 [29:30<4:13:49, 20.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_6 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1161/1742 [32:45<5:15:20, 32.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_7 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1306/1742 [35:58<3:42:00, 30.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_8 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1451/1742 [39:12<2:16:28, 28.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_9 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1596/1742 [43:10<1:42:22, 42.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_10 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1741/1742 [46:00<00:23, 23.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_11 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [46:01<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_12 completes saving.\n",
      "CPU times: user 19min 49s, sys: 16min 34s, total: 36min 24s\n",
      "Wall time: 46min 12s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_carts = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1783737 sessions in total, we separate them into K batches\n",
    "K = 12\n",
    "batch_size = 1783737 // K  ##  -> 148644 dealing with around 150k sessions per batch\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (1024 * 145) * i for i in range(1, K+3)]   ## batch process every 1024 * 145 rows\n",
    "# 145 -> 148644 // 1024  -> batch_size // PARALLEL\n",
    "\n",
    "feature_batch_id = 0\n",
    "## load important tables\n",
    "valid_gt_sessions, gt_labels = load_gt_tables(\"carts\")\n",
    "print(\"finish loading the gt datas\")\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], item_total_likes, np.array([4.0, 2.0, 5.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels)\n",
    "        batch_result.to_parquet(f\"../../allData/features/cart_features_V5/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_carts\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_carts = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        \n",
    "    #break\n",
    "#     gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do same thing for carts, Draft from earlier, DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]<ipython-input-11-e41a977c9bca>:47: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64) to Tuple(bool, float64, float64, int64, int64, int64, float64). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid]))\n",
      "100%|██████████| 1742/1742 [09:11<00:00,  3.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 1min 35s, total: 5min 53s\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# result_iuf_orders = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_carts = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = []\n",
    "gc.collect()\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.64 s, sys: 26.8 s, total: 36.5 s\n",
      "Wall time: 2min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10031"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## save the result as df \n",
    "features_save = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n",
    "# features_save = features_save.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "del features_all_sessions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 1min 9s, total: 1min 27s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## A total of 569697 carts action that's predictable \n",
    "## get all valid sessions with carts actions\n",
    "## there are 150179 / 1783737 sessions have orders actions, 301057 / 1783737 have carts, 1737968 / 1783737 have clicks\n",
    "carts_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "carts_labels['aids'] = carts_labels[\"labels\"].apply(lambda x: x.get(\"carts\"))\n",
    "carts_labels = carts_labels[carts_labels.aids.notnull()]\n",
    "carts_labels = carts_labels.drop(\"labels\", axis = 1)\n",
    "## ========= special df to identify the unique session id to look at ================\n",
    "valid_cart_sessions = carts_labels.drop(\"aids\", axis = 1) \n",
    "## ========================================================================\n",
    "## keep go on for cart labels processing\n",
    "carts_labels = carts_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "carts_labels[\"gt\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join with features_save, now only cart_features in valid sessions are kept, and as expected 301057 sessions are the valid sessions to expand\n",
    "cart_features_valid_session = pd.merge(features_save, valid_cart_sessions, on=\"session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now explode the whole valid session aids, a total of 27168199 session - aid are served as the traini/val/test data for cart, \n",
    "## a total of 569697 correct guesses(not 100% included in the recall)\n",
    "cart_features_valid_session = cart_features_valid_session.set_index(['session']).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally \n",
    "carts_full_df = pd.merge(cart_features_valid_session, carts_labels, on=[\"session\", \"aids\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with ops_total\n"
     ]
    }
   ],
   "source": [
    "## open up the feature tuple \n",
    "carts_full_df[\"prev_int\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "carts_full_df[\"seq_w\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "carts_full_df[\"time_w\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "carts_full_df[\"ops_total\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "print(\"Done with ops_total\")\n",
    "carts_full_df[\"session_len\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "carts_full_df[\"session_unique_aid\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "carts_full_df[\"rank_score\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "carts_full_df = carts_full_df.drop(\"feature_tuple\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aids</th>\n",
       "      <th>gt</th>\n",
       "      <th>prev_int</th>\n",
       "      <th>seq_w</th>\n",
       "      <th>time_w</th>\n",
       "      <th>ops_total</th>\n",
       "      <th>session_len</th>\n",
       "      <th>session_unique_aid</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>11830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.737330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098528</td>\n",
       "      <td>1732105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.773733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098528</td>\n",
       "      <td>588923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.466177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098528</td>\n",
       "      <td>571762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098528</td>\n",
       "      <td>884502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aids  gt  prev_int     seq_w  time_w  ops_total  session_len  \\\n",
       "0  11098528    11830 NaN      True  0.231144     3.0          4            1   \n",
       "1  11098528  1732105 NaN     False  0.231144     3.0          4            1   \n",
       "2  11098528   588923 NaN     False  0.231144     3.0          4            1   \n",
       "3  11098528   571762 NaN     False  0.231144     3.0          4            1   \n",
       "4  11098528   884502 NaN     False  0.231144     3.0          4            1   \n",
       "\n",
       "   session_unique_aid  rank_score  \n",
       "0                   1   27.737330  \n",
       "1                   1    2.773733  \n",
       "2                   1    1.466177  \n",
       "3                   1    0.752040  \n",
       "4                   1    0.709451  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carts_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "carts_full_df.to_parquet(\"../../allData/features/carts_features_100_per_session_V3_opwFix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # final_df[\"prev_int\"] = final_df[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "    # final_df[\"seq_w_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "    # final_df[\"time_w_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "    # final_df[\"ops_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "    # final_df[\"session_len\"] = final_df[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "    # final_df[\"session_unique_aid\"] = final_df[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "    # final_df[\"cf_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "    # final_df[\"item_total_like\"] = final_df[\"feature_tuple\"].apply(lambda x: x[7])\n",
    "    # final_df[\"num_reference_time\"] = final_df[\"feature_tuple\"].apply(lambda x: x[8])\n",
    "    # final_df[\"max_sim_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[9])\n",
    "    # final_df[\"mean_sim_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[10])\n",
    "    # final_df[\"num_interact\"] = final_df[\"feature_tuple\"].apply(lambda x: x[11])\n",
    "    # final_df[\"time_span\"] = final_df[\"feature_tuple\"].apply(lambda x: x[12])\n",
    "    # final_df[\"action_recency\"] = final_df[\"feature_tuple\"].apply(lambda x: x[13])\n",
    "    # final_df[\"seq_w_max\"] = final_df[\"feature_tuple\"].apply(lambda x: x[14])\n",
    "    # final_df[\"seq_w_mean\"] = final_df[\"feature_tuple\"].apply(lambda x: x[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Given there are 1783737 sessions in total, we separate them into K batches\n",
    "K = 6\n",
    "batch_size = 1783737 // K  ##  -> 297289 ~ 1024 * 290 dealing with around 30k sessions per batch\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (1024 * 290)* i for i in range(1, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant change to feature storage, \n",
    "using Session Cache instead of dynamic save\n",
    "### Intended structure\n",
    "For each session, a structure of following will be created   \n",
    "session_ts_cache: {  \n",
    "                   aid1: [ts_1, ts_2, ...],   \n",
    "                   aid2: [ts, ......],  \n",
    "                   ..........  \n",
    "                   }\n",
    "\n",
    "session_ops_cache: {\n",
    "                  aid1: [ops_1, ops_2, ...] \n",
    "}\n",
    "\n",
    "session_simScore_cache: {\n",
    "                 aid1: []\n",
    "}\n",
    "\n",
    "session_CFIncre_cache: {\n",
    "                 aid1: []\n",
    "}\n",
    "\n",
    "session_seqW_cache: {\n",
    "    aid1\n",
    "}\n",
    "\n",
    "session_timeW_cache: {\n",
    "\n",
    "}\n",
    "\n",
    "session_rawSeqOrder_cache: {\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>1</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>1661119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529</td>\n",
       "      <td>1</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>1661119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530</td>\n",
       "      <td>6</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>1661120532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531</td>\n",
       "      <td>24</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>1661119746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532</td>\n",
       "      <td>2</td>\n",
       "      <td>1661119201</td>\n",
       "      <td>1661119996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783732</th>\n",
       "      <td>12899774</td>\n",
       "      <td>1</td>\n",
       "      <td>1661723968</td>\n",
       "      <td>1661723968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783733</th>\n",
       "      <td>12899775</td>\n",
       "      <td>1</td>\n",
       "      <td>1661723970</td>\n",
       "      <td>1661723970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783734</th>\n",
       "      <td>12899776</td>\n",
       "      <td>1</td>\n",
       "      <td>1661723972</td>\n",
       "      <td>1661723972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783735</th>\n",
       "      <td>12899777</td>\n",
       "      <td>1</td>\n",
       "      <td>1661723976</td>\n",
       "      <td>1661723976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783736</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1</td>\n",
       "      <td>1661723983</td>\n",
       "      <td>1661723983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1783737 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          session  total_action  session_start_time  session_end_time\n",
       "0        11098528             1          1661119200        1661119200\n",
       "1        11098529             1          1661119200        1661119200\n",
       "2        11098530             6          1661119200        1661120532\n",
       "3        11098531            24          1661119200        1661119746\n",
       "4        11098532             2          1661119201        1661119996\n",
       "...           ...           ...                 ...               ...\n",
       "1783732  12899774             1          1661723968        1661723968\n",
       "1783733  12899775             1          1661723970        1661723970\n",
       "1783734  12899776             1          1661723972        1661723972\n",
       "1783735  12899777             1          1661723976        1661723976\n",
       "1783736  12899778             1          1661723983        1661723983\n",
       "\n",
       "[1783737 rows x 4 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test interface\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10584520</th>\n",
       "      <td>11098531</td>\n",
       "      <td>24</td>\n",
       "      <td>1661119200</td>\n",
       "      <td>1661119746</td>\n",
       "      <td>163441177</td>\n",
       "      <td>1661119746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           session  total_action  session_start_time  session_end_time  \\\n",
       "10584520  11098531            24          1661119200        1661119746   \n",
       "\n",
       "          start_idx    end_time  \n",
       "10584520  163441177  1661119746  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.session == 11098531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session_by_caching(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, item_total_likes, test_ops_weights):\n",
    "\n",
    "    NOW_TIME = ts[-1] ## ts of latest avaiable action\n",
    "    PREV_INTERACT_BONUS = 20\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    time_span = end_time - start_time\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "\n",
    "    raw_sequence = np.arange(1, len(candidates) + 1)\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "\n",
    "\n",
    "    ## initiate the caches for the features\n",
    "    visit_flag = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.boolean) ## indicate if an aid be visited\n",
    "    ts_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    ops_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))    #value_type = nb.types.float64[:])\n",
    "    simScore_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    cfIncre_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    seqW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    timeW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    actionW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "    raw_seqOrder_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n",
    "\n",
    "    helper_idx = ending_idx - 1\n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, raw_seq_order, time_w in zip(candidates, candidates_ops, sequence_weight, raw_sequence, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "                ## init all cache obj\n",
    "                visit_flag[aid] = 1\n",
    "                ts_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                ops_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                simScore_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                cfIncre_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                seqW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                timeW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                actionW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                raw_seqOrder_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "            ## caculate scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op]\n",
    "            potential_to_recommend[aid] += cf_incre #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            ts_cache[aid] = np.append(ts_cache[aid], ts[helper_idx])\n",
    "            ops_cache[aid] = np.append(ops_cache[aid], op)\n",
    "            simScore_cache[aid] = np.append(simScore_cache[aid], 1)\n",
    "            cfIncre_cache[aid] = np.append(cfIncre_cache[aid], cf_incre)\n",
    "            seqW_cache[aid] = np.append(seqW_cache[aid], seq_w)\n",
    "            timeW_cache[aid] = np.append(timeW_cache[aid], time_w)\n",
    "            actionW_cache[aid] = np.append(actionW_cache[aid], test_ops_weights[op])\n",
    "            raw_seqOrder_cache[aid] = np.append(raw_seqOrder_cache[aid], raw_seq_order)\n",
    "            \n",
    "            \n",
    "            helper_idx -= 1\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, raw_seq_order, time_w in zip(candidates, candidates_ops, sequence_weight, raw_sequence, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "                ## init all cache obj\n",
    "                visit_flag[aid] = 1\n",
    "                ts_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                ops_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                simScore_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                cfIncre_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                seqW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                timeW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                actionW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "                raw_seqOrder_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n",
    "            ## get the scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            potential_to_recommend[aid] += cf_incre\n",
    "            ## append features\n",
    "            ts_cache[aid] = np.append(ts_cache[aid], ts[helper_idx])\n",
    "            ops_cache[aid] = np.append(ops_cache[aid], op)\n",
    "            simScore_cache[aid] = np.append(simScore_cache[aid], 1)\n",
    "            cfIncre_cache[aid] = np.append(cfIncre_cache[aid], cf_incre)\n",
    "            seqW_cache[aid] = np.append(seqW_cache[aid], seq_w)\n",
    "            timeW_cache[aid] = np.append(timeW_cache[aid], time_w)\n",
    "            actionW_cache[aid] = np.append(actionW_cache[aid], test_ops_weights[op])\n",
    "            raw_seqOrder_cache[aid] = np.append(raw_seqOrder_cache[aid], raw_seq_order)\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                    ## init all cache obj\n",
    "                    visit_flag[similar_item] = 0\n",
    "                    ts_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    ops_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    simScore_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    cfIncre_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    seqW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    timeW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    actionW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                    raw_seqOrder_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n",
    "                \n",
    "                cf_incre = seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]\n",
    "                potential_to_recommend[similar_item] += cf_incre  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                ts_cache[similar_item] = np.append(ts_cache[similar_item], ts[helper_idx])\n",
    "                ops_cache[similar_item] = np.append(ops_cache[similar_item], op)\n",
    "                simScore_cache[similar_item] = np.append(simScore_cache[similar_item], full_sim_matrix[aid][similar_item])\n",
    "                cfIncre_cache[similar_item] = np.append(cfIncre_cache[similar_item], cf_incre)\n",
    "                seqW_cache[similar_item] = np.append(seqW_cache[similar_item], seq_w)\n",
    "                timeW_cache[similar_item] = np.append(timeW_cache[similar_item], time_w)\n",
    "                actionW_cache[similar_item] = np.append(actionW_cache[similar_item], test_ops_weights[op])\n",
    "                raw_seqOrder_cache[similar_item] = np.append(raw_seqOrder_cache[similar_item], raw_seq_order)\n",
    "                \n",
    "            helper_idx -= 1\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 150))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "        # action_types_temp, counts = np.unique(ops_cache[aid], return_counts=True)\n",
    "        num_clicks, num_carts, num_orders = 0, 0, 0\n",
    "        for op in ops_cache[aid]:\n",
    "            if op == 0:\n",
    "                num_clicks += 1\n",
    "            elif op == 1:\n",
    "                num_carts += 1\n",
    "            elif op == 2:\n",
    "                num_orders += 1\n",
    "\n",
    "        if visit_flag[aid]:   ## write 6 features per row\n",
    "            feature_tuple_this_aid = (\n",
    "                visit_flag[aid], np.sum(seqW_cache[aid]), np.sum(timeW_cache[aid]), np.sum(actionW_cache[aid]), length, len(unique_aids),\n",
    "                potential_to_recommend[aid], item_total_likes[aid], 100, 1, 1, len(raw_seqOrder_cache[aid]),\n",
    "                time_span, end_time-ts_cache[aid][0], np.max(seqW_cache[aid]), np.mean(seqW_cache[aid]), np.min(seqW_cache[aid]), np.max(timeW_cache[aid]),\n",
    "                np.mean(timeW_cache[aid]), np.min(timeW_cache[aid]), np.max(actionW_cache[aid]), np.mean(actionW_cache[aid]), np.min(actionW_cache[aid]), num_clicks,\n",
    "                num_carts, num_orders, ops_cache[aid][0], NOW_TIME-ts_cache[aid][0], np.max(cfIncre_cache[aid]), np.mean(cfIncre_cache[aid]),\n",
    "                np.min(cfIncre_cache[aid]), np.std(seqW_cache[aid]), np.std(timeW_cache[aid]), np.std(actionW_cache[aid]), np.std(cfIncre_cache[aid]), \\\n",
    "                    np.max(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.mean(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.min(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]),\n",
    "                    np.max(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.mean(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.min(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]),\n",
    "                    np.max(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.mean(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.min(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]),\n",
    "                raw_seqOrder_cache[aid][0], np.max(raw_seqOrder_cache[aid]), np.mean(raw_seqOrder_cache[aid]), np.min(raw_seqOrder_cache[aid]),\n",
    "                ts_cache[aid][0]\n",
    "            )\n",
    "        else:\n",
    "            feature_tuple_this_aid = (\n",
    "                visit_flag[aid], np.sum(seqW_cache[aid]), np.sum(timeW_cache[aid]), np.sum(actionW_cache[aid]), length, len(unique_aids),\n",
    "                potential_to_recommend[aid], item_total_likes[aid], len(raw_seqOrder_cache[aid]), np.max(simScore_cache[aid]), np.mean(simScore_cache[aid]), 0,\n",
    "                time_span, end_time-ts_cache[aid][0], np.max(seqW_cache[aid]), np.mean(seqW_cache[aid]), np.min(seqW_cache[aid]), np.max(timeW_cache[aid]),\n",
    "                np.mean(timeW_cache[aid]), np.min(timeW_cache[aid]), np.max(actionW_cache[aid]), np.mean(actionW_cache[aid]), np.min(actionW_cache[aid]), num_clicks,\n",
    "                num_carts, num_orders, ops_cache[aid][0], NOW_TIME-ts_cache[aid][0], np.max(cfIncre_cache[aid]), np.mean(cfIncre_cache[aid]),\n",
    "                np.min(cfIncre_cache[aid]), np.std(seqW_cache[aid]), np.std(timeW_cache[aid]), np.std(actionW_cache[aid]), np.std(cfIncre_cache[aid]), \\\n",
    "                    np.max(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.mean(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.min(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), \n",
    "                    np.max(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.mean(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.min(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]),\n",
    "                    np.max(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.mean(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.min(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]),\n",
    "                raw_seqOrder_cache[aid][0], np.max(raw_seqOrder_cache[aid]), np.mean(raw_seqOrder_cache[aid]), np.min(raw_seqOrder_cache[aid]),\n",
    "                ts_cache[aid][0]\n",
    "            )\n",
    "            \n",
    "        feature_tuples_this_session.append(feature_tuple_this_aid)\n",
    "    \n",
    "    return raw_seqOrder_cache, timeW_cache, seqW_cache, cfIncre_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_result = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "raw_seqOrder_cache, timeW_cache, cfIncre_cachee, cfIncre_cache = save_feature_single_session_by_caching(11098531, 163441177, 24, 1661119200, aids, ops, ts, pseudo_result , simMatrices[\"iuf\"], item_total_likes, np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw seq order: [ 1.  4.  7. 12. 14. 16. 20.]\n",
      "timeW cache [3.         3.         2.62932138 2.44693858 2.4255841  2.39331921\n",
      " 2.31852881]\n",
      "cfIncre_cache [0.56767517 1.81535147 0.46075358 0.09041389 0.2901413  0.25172215\n",
      " 0.04965495]\n",
      "cfIncre_cache [3.         3.         2.62932138 2.44693858 2.4255841  2.39331921\n",
      " 2.31852881]\n"
     ]
    }
   ],
   "source": [
    "print(\"raw seq order:\", raw_seqOrder_cache[648093])\n",
    "print(\"timeW cache\", timeW_cache[648093])\n",
    "print(\"cfIncre_cache\", cfIncre_cache[648093])\n",
    "print(\"cfIncre_cache\", timeW_cache[648093])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  4.,  7., 12., 14., 16., 20.])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_1[648093]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.87734741, 0.76221665, 0.58580721, 0.52029135,\n",
       "       0.4574822 , 0.33954136])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_2[648093]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.        , 3.        , 2.62932138, 2.44693858, 2.4255841 ,\n",
       "       2.39331921, 2.31852881])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result_3[648093]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cache[653835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cache[653835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 11., 12., 13., 14.,\n",
       "       15., 16., 17., 18., 20., 21., 22., 24.])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1_cache[653835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.95825035, 0.91737222, 0.87734741, 0.83815811,\n",
       "       0.79978689, 0.76221665, 0.72543069, 0.68941263, 0.61961642,\n",
       "       0.58580721, 0.55270376, 0.52029135, 0.48855553, 0.4574822 ,\n",
       "       0.42705751, 0.39726794, 0.33954136, 0.31157867, 0.2841997 ,\n",
       "       0.23114441])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2_cache[653835]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "for aid in test_cache:\n",
    "    print(test_cache[aid])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 8, 7, 6, 5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(1, 10)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.88230446, 0.77153504, 0.66728415, 0.5691682 ,\n",
       "       0.47682615, 0.38991822, 0.30812463, 0.23114441])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(2, np.linspace(0.3, 1, 9))[::-1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[int64,array(int64, 1d, A)]<iv=None>({123: [ 234 4343]})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "test[123] = np.array([234, 4343])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops_cache = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int32, \n",
    "    value_type = nb.types.float32[:])\n",
    "ops_cache[123] = np.array([np.float32(0.0) for _ in range(0)], dtype=np.float32) # np.array(dtype=np.float32)\n",
    "ops_cache[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops_cache[123] = np.array(np.append(ops_cache[123], 0.0), dtype=np.float32)\n",
    "ops_cache[123] = np.array(np.append(ops_cache[123], 1.0), dtype=np.float32)\n",
    "# ops_cache[123] = np.array(np.append(ops_cache[123], 1.0), dtype=np.float32)\n",
    "# ops_cache[123] = np.array(np.append(ops_cache[123], 0.0), dtype=np.float32)\n",
    "# ops_cache[123] = np.array(np.append(ops_cache[123], 0.0), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ops_cache[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(ops_cache[123][-min(3, len(ops_cache[123])):] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
