{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T18:22:44.715327Z","iopub.status.busy":"2023-01-30T18:22:44.714412Z","iopub.status.idle":"2023-01-30T18:22:45.657367Z","shell.execute_reply":"2023-01-30T18:22:45.656277Z","shell.execute_reply.started":"2023-01-30T18:22:44.715204Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import heapq\n","import pickle\n","import numba as nb\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import math"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-01-30T18:22:45.661018Z","iopub.status.busy":"2023-01-30T18:22:45.660054Z","iopub.status.idle":"2023-01-30T18:24:05.543052Z","shell.execute_reply":"2023-01-30T18:24:05.541773Z","shell.execute_reply.started":"2023-01-30T18:22:45.660965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 8.47 s, sys: 3.97 s, total: 12.4 s\n","Wall time: 13 s\n"]}],"source":["%%time\n","df = pd.read_csv(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/train_meta_data.csv\")\n","df_test = pd.read_csv(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/test_meta_data.csv\")\n","df = pd.concat([df, df_test]).reset_index(drop = True)\n","npz = np.load(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/train_core_data.npz\")\n","npz_test = np.load(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/test_core_data.npz\")\n","aids = np.concatenate([npz['aids'], npz_test['aids']])\n","ts = np.concatenate([npz['ts'], npz_test['ts']])\n","ops = np.concatenate([npz['ops'], npz_test['ops']])\n","\n","df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n","df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T18:27:56.167166Z","iopub.status.busy":"2023-01-30T18:27:56.164976Z","iopub.status.idle":"2023-01-30T18:27:56.180096Z","shell.execute_reply":"2023-01-30T18:27:56.178476Z","shell.execute_reply.started":"2023-01-30T18:27:56.167047Z"},"trusted":true},"outputs":[],"source":["## Define constants\n","PARALLEL = 1024\n","LOOKBACK_WINDOW = 200   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n","#TOPN = 20\n","ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"]},{"cell_type":"markdown","metadata":{},"source":["## Phase I: sim matrix"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T18:27:58.540087Z","iopub.status.busy":"2023-01-30T18:27:58.539603Z","iopub.status.idle":"2023-01-30T18:27:58.928677Z","shell.execute_reply":"2023-01-30T18:27:58.927533Z","shell.execute_reply.started":"2023-01-30T18:27:58.540046Z"},"trusted":true},"outputs":[],"source":["# ==================================\n","# Methods for counting Item Total Likes\n","# ==================================\n","@nb.jit(nopython=True)\n","def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n","    \"\"\"\n","    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n","    \"\"\"\n","    for idx, item in enumerate(aids):\n","        if item not in item_total_likes: \n","            item_total_likes[item] = 0\n","        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n","\n","# ==================================\n","# Methods for rank and trim the sim score dict\n","# ==================================\n","@nb.jit(nopython = True)\n","def heap_topk(item_cnt_dict, cap):\n","    \"\"\"\n","    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n","    \"\"\"\n","    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n","    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n","    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n","        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n","        if len(q) > cap:\n","            heapq.heappop(q)\n","            \n","    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n","    for i in range(len(res)):\n","        dic[res[i][1]] = res[i][0]\n","    \n","    return dic\n","   \n","@nb.jit(nopython = True)\n","def trim_simMatrix_topk(fullSimMatrix, k = 50):\n","    \"\"\"\n","    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n","    \"\"\"\n","    for item, item_cnt_dict in fullSimMatrix.items():\n","        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n","\n","# ==================================\n","# Methods for score normalization\n","# ==================================\n","\n","# @nb.jit(nopython=True)\n","# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n","#     for aid_1, relations in fullSimMatrix.items():\n","#         for aid_2, sim_score in relations.items():\n","#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n","            \n","@nb.jit(nopython=True)\n","def maxNormSimMatrix(fullSimMatrix):\n","    for aid_1, relations in fullSimMatrix.items():\n","        max_num = -np.inf\n","        for _, sim_score in relations.items():\n","            if sim_score > max_num:\n","                max_num = sim_score\n","        ## DEGUG use, delete later\n","        if max_num == 0:\n","            print(aid_1)\n","            print(fullSimMatrix[aid_1])\n","        for aid_2, sim_score in relations.items():\n","#             if max_num == 0:\n","#                 max_num += 0.001\n","            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T18:28:55.087708Z","iopub.status.busy":"2023-01-30T18:28:55.087091Z","iopub.status.idle":"2023-01-30T18:28:55.118007Z","shell.execute_reply":"2023-01-30T18:28:55.116700Z","shell.execute_reply.started":"2023-01-30T18:28:55.087654Z"},"trusted":true},"outputs":[],"source":["@nb.jit(nopython=True)\n","def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n","    \"\"\"\n","    Get the sim scores of items within single session, can be ran in parallel within each batch. \n","    \"\"\"\n","    max_idx = start_idx + length\n","    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n","    for i in range(min_idx, max_idx):\n","        for j in range(i+1, max_idx):\n","            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n","            if aids[i] == aids[j]: continue\n","            \n","            if mode == \"cosine\":\n","                w_ij = action_weights[ops[j]] \n","                w_ji = action_weights[ops[i]] \n","            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n","                \n","                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n","                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n","                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n","                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n","            elif mode == \"time_decay\":\n","                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n","                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n","                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662528791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n","                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n","                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n","                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n","                \n","                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n","                \n","                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n","                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n","            elif mode == \"buy2buy\":\n","                if (ops[i] == 0) or (ops[j] == 0):\n","                    continue\n","                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n","                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n","                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n","                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n","                \n","            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n","            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n","\n","@nb.jit(nopython=True, parallel=True, cache=True)\n","def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n","    nrows = len(rows)\n","    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n","    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n","    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n","        _, start_idx, length, start_time = rows[row_i]\n","        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n","    ## merge pairs_this_batch into the fullSimMatrix\n","    for row_i in range(nrows):\n","        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n","            if aid1 not in fullSimMatrix: \n","                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n","            if aid2 not in fullSimMatrix[aid1]:\n","                fullSimMatrix[aid1][aid2] = 0.0\n","            fullSimMatrix[aid1][aid2] += score\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-01-30T18:28:58.135743Z","iopub.status.busy":"2023-01-30T18:28:58.134629Z","iopub.status.idle":"2023-01-30T18:29:39.668009Z","shell.execute_reply":"2023-01-30T18:29:39.666533Z","shell.execute_reply.started":"2023-01-30T18:28:58.135678Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 22.9 s, sys: 890 ms, total: 23.8 s\n","Wall time: 24 s\n"]}],"source":["%%time\n","## get the Total Like matrix\n","item_total_likes = nb.typed.Dict.empty(\n","    key_type = nb.types.int64,\n","    value_type = nb.types.float64)\n","\n","getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T20:38:46.583451Z","iopub.status.busy":"2023-01-29T20:38:46.583030Z","iopub.status.idle":"2023-01-29T20:41:17.832712Z","shell.execute_reply":"2023-01-29T20:41:17.831649Z","shell.execute_reply.started":"2023-01-29T20:38:46.583417Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  7%|▋         | 998/14231 [02:46<1:02:59,  3.50it/s]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  1000\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 1999/14231 [06:56<51:57:09, 15.29s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  2000\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 2999/14231 [10:23<54:41:33, 17.53s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  3000\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 3999/14231 [13:25<54:10:57, 19.06s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  4000\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 4999/14231 [16:43<50:31:33, 19.70s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  5000\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 5999/14231 [19:44<23:01:48, 10.07s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  6000\n"]},{"name":"stderr","output_type":"stream","text":[" 49%|████▉     | 6999/14231 [22:23<40:16:01, 20.04s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  7000\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▌    | 8000/14231 [25:04<26:27:37, 15.29s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  8000\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 8999/14231 [27:14<13:35:21,  9.35s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  9000\n"]},{"name":"stderr","output_type":"stream","text":[" 70%|███████   | 9999/14231 [29:21<22:15:39, 18.94s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  10000\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 10999/14231 [31:34<9:02:04, 10.06s/it] "]},{"name":"stdout","output_type":"stream","text":["batch_idx:  11000\n"]},{"name":"stderr","output_type":"stream","text":[" 84%|████████▍ | 11999/14231 [33:39<6:14:16, 10.06s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  12000\n"]},{"name":"stderr","output_type":"stream","text":[" 91%|█████████▏| 12999/14231 [35:45<1:43:47,  5.06s/it]"]},{"name":"stdout","output_type":"stream","text":["batch_idx:  13000\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 14001/14231 [37:10<09:36,  2.51s/it]  "]},{"name":"stdout","output_type":"stream","text":["batch_idx:  14000\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 14231/14231 [37:17<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1h 7min 34s, sys: 33min 22s, total: 1h 40min 56s\n","Wall time: 38min 33s\n"]}],"source":["%%time\n","simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n","TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n","MODES_TO_TRAIN = [\"iuf\"] #, \"time_decay\"]\n","\n","for mode in MODES_TO_TRAIN:\n","    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n","    fullSimMatrix = nb.typed.Dict.empty(\n","            key_type = nb.types.int64,\n","            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n","    max_idx = len(df)\n","    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n","    for idx in tqdm(range(0, max_idx, PARALLEL)):\n","        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n","        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n","        batch_idx += 1\n","        if batch_idx % TRIM_CYCLES == 0:\n","            print(\"batch_idx: \", batch_idx)\n","            trim_simMatrix_topk(fullSimMatrix, 150)  ## ALERT\n","            gc.collect()\n","#             break\n","\n","    \n","    ## trim top 50 when the training is complete\n","    trim_simMatrix_topk(fullSimMatrix, 150)   ## ALERT ## TODO: make this num small enough to reduce time for normalization, consider keeping 100, give more option for selection\n","    ## max norm of each score\n","    maxNormSimMatrix(fullSimMatrix)\n","    \n","    simMatrices[mode] = fullSimMatrix\n","    \n","    del fullSimMatrix\n","    gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["## Phase II: feature save"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T20:41:21.924962Z","iopub.status.busy":"2023-01-29T20:41:21.924557Z","iopub.status.idle":"2023-01-29T20:41:21.934105Z","shell.execute_reply":"2023-01-29T20:41:21.932978Z","shell.execute_reply.started":"2023-01-29T20:41:21.924929Z"},"trusted":true},"outputs":[],"source":["@nb.jit(nopython = True)\n","def heap_topk_return_list(item_cnt_dict, cap):\n","    \"\"\"\n","    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n","    \"\"\"\n","    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n","    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n","        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n","        if len(q) > cap:\n","            heapq.heappop(q)\n","            \n","    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n","    \n","    return res"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T20:41:27.035805Z","iopub.status.busy":"2023-01-29T20:41:27.035378Z","iopub.status.idle":"2023-01-29T20:41:27.042427Z","shell.execute_reply":"2023-01-29T20:41:27.041501Z","shell.execute_reply.started":"2023-01-29T20:41:27.035768Z"},"trusted":true},"outputs":[],"source":["# 3s version\n","FEATURE_NAMES = [\"prev_int\", \"seq_w_total\", \"time_w_total\", \"action_w_total\", \"session_len\", \"num_uniuqe_aids\", \"CF_score\", \"itemTotalLike\", \"ref_time\", \"max_sim_score\",\\\n","    \"mean_sim_score\", \"num_interact\", \"time_span\", \"action_recency\", \"seq_w_max\", \"seq_w_mean\", \"seq_w_min\", \"time_w_max\", \"time_w_mean\", \"time_w_min\", \\\n","        \"ops_w_max\", \"ops_w_mean\", \"ops_w_min\", \"num_clicks\", \"num_carts\", \"num_orders\", \"last_action_type\", \"time_to_now\", \"cf_incre_max\", \"cf_incre_mean\", \\\n","            \"cf_incre_min\", \"seqW_std\", \"timeW_std\", \"actionW_std\", \"cf_incre_std\", \"last3_cfIncre_max\", \"last3_cfIncre_mean\", \"last3_cfIncre_min\", \\\n","                \"last3_timeW_max\", \"last3_timeW_mean\", \"last3_timeW_min\", \"last3_seqW_max\", \"last3_seqW_mean\", \"last3_seqW_min\", \\\n","                    \"last1_seq_order_raw\", \"raw_seq_order_max\", \"raw_seq_order_mean\", \"raw_seq_order_min\", \"last_op_ts\"]\n","\n","## 5s version\n","# FEATURE_NAMES = [\"prev_int\", \"seq_w_total\", \"time_w_total\", \"action_w_total\", \"session_len\", \"num_uniuqe_aids\", \"CF_score\", \"itemTotalLike\", \"ref_time\", \"max_sim_score\",\\\n","#     \"mean_sim_score\", \"num_interact\", \"time_span\", \"action_recency\", \"seq_w_max\", \"seq_w_mean\", \"seq_w_min\", \"time_w_max\", \"time_w_mean\", \"time_w_min\", \\\n","#         \"ops_w_max\", \"ops_w_mean\", \"ops_w_min\", \"num_clicks\", \"num_carts\", \"num_orders\", \"last_action_type\", \"time_to_now\", \"cf_incre_max\", \"cf_incre_mean\", \\\n","#             \"cf_incre_min\", \"seqW_std\", \"timeW_std\", \"actionW_std\", \"cf_incre_std\", \"last5_cfIncre_max\", \"last5_cfIncre_mean\", \"last5_cfIncre_min\", \\\n","#                 \"last5_timeW_max\", \"last5_timeW_mean\", \"last5_timeW_min\", \"last5_seqW_max\", \"last5_seqW_mean\", \"last5_seqW_min\", \\\n","#                     \"last1_seq_order_raw\", \"raw_seq_order_max\", \"raw_seq_order_mean\", \"raw_seq_order_min\", \"last_op_ts\"]\n","                    \n","                    #\"day_of_week\", \"hour_of_day\", \"day_noon_night\"]"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T20:41:33.859470Z","iopub.status.busy":"2023-01-29T20:41:33.859037Z","iopub.status.idle":"2023-01-29T20:41:33.928756Z","shell.execute_reply":"2023-01-29T20:41:33.927752Z","shell.execute_reply.started":"2023-01-29T20:41:33.859430Z"},"trusted":true},"outputs":[],"source":["@nb.jit(nopython=True)\n","def save_feature_single_session_by_caching(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, item_total_likes, test_ops_weights):\n","\n","    NOW_TIME = ts[-1] ## ts of latest avaiable action\n","    PREV_INTERACT_BONUS = 20\n","    NEARBY_ACTION_BONUS = 1.5\n","    \n","    ending_idx = starting_idx + length \n","    end_time = ts[ending_idx - 1]\n","    time_span = end_time - start_time\n","    \n","    candidates = aids[starting_idx: ending_idx][::-1]\n","    candidates_ops = ops[starting_idx: ending_idx][::-1]\n","    \n","    ## record all potential aid that might be relevant\n","    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n","    \n","    ## get unique aid of each session \n","    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n","    for a in candidates:\n","        unique_aids[a] = 0\n","    \n","    ## Sequence weight to all the candidates, from near to far \n","    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n","\n","    raw_sequence = np.arange(1, len(candidates) + 1)\n","    \n","    ## Time weight of all candidates, from near to far\n","    time_weights = []\n","    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n","    for idx in range(starting_idx, ending_idx):\n","        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n","            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n","        else:\n","            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n","        time_weights.append(time_weight)\n","    time_weights = time_weights[::-1]\n","\n","\n","    ## initiate the caches for the features\n","    visit_flag = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.boolean) ## indicate if an aid be visited\n","    ts_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    ops_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))    #value_type = nb.types.float64[:])\n","    simScore_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    cfIncre_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    seqW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    timeW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    actionW_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","    raw_seqOrder_cache = nb.typed.Dict.empty(key_type = nb.types.int64, value_type=np.array([np.float64(0.0) for _ in range(0)]))\n","\n","    helper_idx = ending_idx - 1\n","    ## making inference\n","    if len(unique_aids) >= 20:  \n","        for aid, op, seq_w, raw_seq_order, time_w in zip(candidates, candidates_ops, sequence_weight, raw_sequence, time_weights):\n","            if aid not in potential_to_recommend:\n","                potential_to_recommend[aid] = 0\n","                ## init all cache obj\n","                visit_flag[aid] = 1\n","                ts_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                ops_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                simScore_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                cfIncre_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                seqW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                timeW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                actionW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                raw_seqOrder_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","            ## caculate scores\n","            cf_incre = seq_w * time_w * test_ops_weights[op]\n","            potential_to_recommend[aid] += cf_incre #* PREV_INTERACT_BONUS\n","            ## append features\n","            ts_cache[aid] = np.append(ts_cache[aid], ts[helper_idx])\n","            ops_cache[aid] = np.append(ops_cache[aid], op)\n","            simScore_cache[aid] = np.append(simScore_cache[aid], 1)\n","            cfIncre_cache[aid] = np.append(cfIncre_cache[aid], cf_incre)\n","            seqW_cache[aid] = np.append(seqW_cache[aid], seq_w)\n","            timeW_cache[aid] = np.append(timeW_cache[aid], time_w)\n","            actionW_cache[aid] = np.append(actionW_cache[aid], test_ops_weights[op])\n","            raw_seqOrder_cache[aid] = np.append(raw_seqOrder_cache[aid], raw_seq_order)\n","            \n","            \n","            helper_idx -= 1\n","    else:   ## otherwise, fill the rest with similar items.\n","        for aid, op, seq_w, raw_seq_order, time_w in zip(candidates, candidates_ops, sequence_weight, raw_sequence, time_weights):\n","            if aid not in potential_to_recommend:\n","                potential_to_recommend[aid] = 0\n","                ## init all cache obj\n","                visit_flag[aid] = 1\n","                ts_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                ops_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                simScore_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                cfIncre_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                seqW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                timeW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                actionW_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","                raw_seqOrder_cache[aid] = np.array([np.float64(0) for _ in range(0)])\n","            ## get the scores\n","            cf_incre = seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n","            potential_to_recommend[aid] += cf_incre\n","            ## append features\n","            ts_cache[aid] = np.append(ts_cache[aid], ts[helper_idx])\n","            ops_cache[aid] = np.append(ops_cache[aid], op)\n","            simScore_cache[aid] = np.append(simScore_cache[aid], 1)\n","            cfIncre_cache[aid] = np.append(cfIncre_cache[aid], cf_incre)\n","            seqW_cache[aid] = np.append(seqW_cache[aid], seq_w)\n","            timeW_cache[aid] = np.append(timeW_cache[aid], time_w)\n","            actionW_cache[aid] = np.append(actionW_cache[aid], test_ops_weights[op])\n","            raw_seqOrder_cache[aid] = np.append(raw_seqOrder_cache[aid], raw_seq_order)\n","            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n","            if aid not in full_sim_matrix:\n","                continue\n","            for similar_item in full_sim_matrix[aid]:\n","                ## if sim_item is in candidates, would be included above anyways, skip \n","                if similar_item in candidates:\n","                    continue\n","                if similar_item not in potential_to_recommend:\n","                    potential_to_recommend[similar_item] = 0\n","                    ## init all cache obj\n","                    visit_flag[similar_item] = 0\n","                    ts_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    ops_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    simScore_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    cfIncre_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    seqW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    timeW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    actionW_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                    raw_seqOrder_cache[similar_item] = np.array([np.float64(0) for _ in range(0)])\n","                \n","                cf_incre = seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]\n","                potential_to_recommend[similar_item] += cf_incre  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n","                ## append features\n","                ts_cache[similar_item] = np.append(ts_cache[similar_item], ts[helper_idx])\n","                ops_cache[similar_item] = np.append(ops_cache[similar_item], op)\n","                simScore_cache[similar_item] = np.append(simScore_cache[similar_item], full_sim_matrix[aid][similar_item])\n","                cfIncre_cache[similar_item] = np.append(cfIncre_cache[similar_item], cf_incre)\n","                seqW_cache[similar_item] = np.append(seqW_cache[similar_item], seq_w)\n","                timeW_cache[similar_item] = np.append(timeW_cache[similar_item], time_w)\n","                actionW_cache[similar_item] = np.append(actionW_cache[similar_item], test_ops_weights[op])\n","                raw_seqOrder_cache[similar_item] = np.append(raw_seqOrder_cache[similar_item], raw_seq_order)\n","                \n","            helper_idx -= 1\n","\n","    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 150)) ## ALERT ## Take top 100 for validation runs. \n","    \n","    feature_tuples_this_session = []\n","    for aid in result[session]:\n","        # action_types_temp, counts = np.unique(ops_cache[aid], return_counts=True)\n","        num_clicks, num_carts, num_orders = 0, 0, 0\n","        for op in ops_cache[aid]:\n","            if op == 0:\n","                num_clicks += 1\n","            elif op == 1:\n","                num_carts += 1\n","            elif op == 2:\n","                num_orders += 1\n","\n","        if visit_flag[aid]:   ## write 6 features per row\n","            feature_tuple_this_aid = (\n","                visit_flag[aid], np.sum(seqW_cache[aid]), np.sum(timeW_cache[aid]), np.sum(actionW_cache[aid]), length, len(unique_aids),\n","                potential_to_recommend[aid], item_total_likes[aid], 100, 1, 1, len(raw_seqOrder_cache[aid]),\n","                time_span, end_time-ts_cache[aid][0], np.max(seqW_cache[aid]), np.mean(seqW_cache[aid]), np.min(seqW_cache[aid]), np.max(timeW_cache[aid]),\n","                np.mean(timeW_cache[aid]), np.min(timeW_cache[aid]), np.max(actionW_cache[aid]), np.mean(actionW_cache[aid]), np.min(actionW_cache[aid]), num_clicks,\n","                num_carts, num_orders, ops_cache[aid][0], NOW_TIME-ts_cache[aid][0], np.max(cfIncre_cache[aid]), np.mean(cfIncre_cache[aid]),\n","                np.min(cfIncre_cache[aid]), np.std(seqW_cache[aid]), np.std(timeW_cache[aid]), np.std(actionW_cache[aid]), np.std(cfIncre_cache[aid]), \\\n","                    np.max(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.mean(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.min(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), \n","                    np.max(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.mean(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.min(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]),\n","                    np.max(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.mean(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.min(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]),\n","                raw_seqOrder_cache[aid][0], np.max(raw_seqOrder_cache[aid]), np.mean(raw_seqOrder_cache[aid]), np.min(raw_seqOrder_cache[aid]),\n","                ts_cache[aid][0]\n","            )\n","        else:\n","            feature_tuple_this_aid = (\n","                visit_flag[aid], np.sum(seqW_cache[aid]), np.sum(timeW_cache[aid]), np.sum(actionW_cache[aid]), length, len(unique_aids),\n","                potential_to_recommend[aid], item_total_likes[aid], len(raw_seqOrder_cache[aid]), np.max(simScore_cache[aid]), np.mean(simScore_cache[aid]), 0,\n","                time_span, end_time-ts_cache[aid][0], np.max(seqW_cache[aid]), np.mean(seqW_cache[aid]), np.min(seqW_cache[aid]), np.max(timeW_cache[aid]),\n","                np.mean(timeW_cache[aid]), np.min(timeW_cache[aid]), np.max(actionW_cache[aid]), np.mean(actionW_cache[aid]), np.min(actionW_cache[aid]), num_clicks,\n","                num_carts, num_orders, ops_cache[aid][0], NOW_TIME-ts_cache[aid][0], np.max(cfIncre_cache[aid]), np.mean(cfIncre_cache[aid]),\n","                np.min(cfIncre_cache[aid]), np.std(seqW_cache[aid]), np.std(timeW_cache[aid]), np.std(actionW_cache[aid]), np.std(cfIncre_cache[aid]), \\\n","                    np.max(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.mean(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), np.min(cfIncre_cache[aid][: min(3, len(cfIncre_cache[aid]))]), \n","                    np.max(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.mean(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]), np.min(timeW_cache[aid][: min(3, len(timeW_cache[aid]))]),\n","                    np.max(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.mean(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]), np.min(seqW_cache[aid][: min(3, len(seqW_cache[aid]))]),\n","                raw_seqOrder_cache[aid][0], np.max(raw_seqOrder_cache[aid]), np.mean(raw_seqOrder_cache[aid]), np.min(raw_seqOrder_cache[aid]),\n","                ts_cache[aid][0]\n","            )\n","            \n","        feature_tuples_this_session.append(feature_tuple_this_aid)\n","    \n","    return feature_tuples_this_session"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T20:41:36.802114Z","iopub.status.busy":"2023-01-29T20:41:36.801709Z","iopub.status.idle":"2023-01-29T20:41:36.810436Z","shell.execute_reply":"2023-01-29T20:41:36.809305Z","shell.execute_reply.started":"2023-01-29T20:41:36.802081Z"},"trusted":true},"outputs":[],"source":["# def load_gt_tables(type):\n","#     \"\"\" type -> carts / orders \"\"\"\n","#     gt_labels = pd.read_json(\"/kaggle/input/local-validation7days-test-labels/test_labels.jsonl\", lines=True)\n","#     gt_labels['aids'] = gt_labels[\"labels\"].apply(lambda x: x.get(type))\n","#     gt_labels = gt_labels[gt_labels.aids.notnull()]\n","#     gt_labels = gt_labels.drop(\"labels\", axis = 1)\n","#     ## ========= special df to identify the unique session id to look at ================\n","#     valid_gt_sessions = gt_labels.drop(\"aids\", axis = 1) \n","#     ## ========================================================================\n","#     ## keep go on for gt labels processing\n","#     gt_labels = gt_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n","#     gt_labels[\"gt\"] = 1\n","#     return valid_gt_sessions, gt_labels\n","\n","\"\"\"\n","This is different from validation phase, as we don't have any gt data, therefore, all data has to be exploded and save\n","\"\"\"\n","def process_batch_pipeline_subVersion(rawDf):\n","    \"\"\" rawDf -> Df with session, aids(100), feature_tuple \"\"\"\n","    ## Directly explode aids and feature_tuple\n","    final_df = rawDf.set_index(['session']).apply(pd.Series.explode).reset_index()\n","\n","    ## new method\n","    features = np.vstack(final_df[\"feature_tuple\"].values)\n","    temp_df = pd.DataFrame(features)\n","    del features\n","    temp_df.columns = [f'{feat_name}' for feat_name in FEATURE_NAMES]\n","    final_df[temp_df.columns] = temp_df\n","    del temp_df\n","    \n","    final_df = final_df.drop(\"feature_tuple\", axis = 1)\n","\n","    return final_df"]},{"cell_type":"markdown","metadata":{},"source":["## CARTs, comment out if run ORDERs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-01-29T21:07:08.269645Z","iopub.status.busy":"2023-01-29T21:07:08.269211Z","iopub.status.idle":"2023-01-29T21:07:08.277850Z","shell.execute_reply":"2023-01-29T21:07:08.276644Z","shell.execute_reply.started":"2023-01-29T21:07:08.269612Z"},"trusted":true},"outputs":[],"source":["# %%time\n","# result_iuf_carts = nb.typed.Dict.empty(\n","#     key_type = nb.types.int64,\n","#     value_type = nb.types.int64[:])\n","\n","# features_all_sessions = [] # session, aid, feature tuple\n","#                           ## session, aid, feature tuple\n","# gc.collect()\n","\n","# ## Given there are 1671803 sessions in total, we separate them into K batches\n","# K = 64\n","# # batch_size = 1671803 // K  ##  -> 139316 dealing with around 140k sessions per batch\n","# session_per_batch = len(df_test) // K \n","\n","# row_idx_cutoffs = [(len(df) - len(df_test)) + (PARALLEL * (session_per_batch//PARALLEL) ) * i for i in range(1, K+3)]   ## batch process every 1024 * 136 rows\n","# # 145 -> 148644 // 1024  -> batch_size // PARALLEL\n","\n","# feature_batch_id = 0\n","\n","# print(\"feature store starts:\")\n","\n","\n","# for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n","#     start_row = row_idx\n","#     end_row = min(row_idx + PARALLEL, len(df))\n","#     rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n","# #     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n","#     ## run things in parallel\n","#     for row_idx in nb.prange(len(rows)):\n","#         session, starting_idx, length, start_time = rows[row_idx]\n","#         features_tuples_this_session = save_feature_single_session_by_caching(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], item_total_likes, np.array([4.0, 2.0, 5.0]))\n","#         features_all_sessions.append(features_tuples_this_session)\n","    \n","#     if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n","#         ## save batch result\n","#         rawDf = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n","#         batch_result = process_batch_pipeline_subVersion(rawDf)\n","#         batch_result.to_parquet(f\"/kaggle/working/batch_result_{feature_batch_id}.parquet\")\n","#         ## clean the memory for next batch\n","#         del batch_result, rawDf, features_all_sessions, result_iuf_carts\n","#         gc.collect()\n","#         ## progress update\n","#         print(f\"feature_batch_{feature_batch_id} completes saving.\")\n","#         feature_batch_id += 1\n","#         ## initiate the struct for new batch again\n","#         result_iuf_carts = nb.typed.Dict.empty(\n","#             key_type = nb.types.int64,\n","#             value_type = nb.types.int64[:])\n","#         features_all_sessions = []"]},{"cell_type":"markdown","metadata":{},"source":["## ORDERs, comment out if run CARTS"]},{"cell_type":"code","execution_count":17,"metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2023-01-29T21:08:12.902434Z","iopub.status.busy":"2023-01-29T21:08:12.901964Z","iopub.status.idle":"2023-01-29T21:08:40.946302Z","shell.execute_reply":"2023-01-29T21:08:40.945114Z","shell.execute_reply.started":"2023-01-29T21:08:12.902399Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1633 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["feature store starts:\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-15-cebd34c36a18>:58: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1munsafe cast from int64 to bool. Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\n","  visit_flag[aid] = 1\n","  2%|▏         | 35/1633 [03:59<16:10:38, 36.44s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_0 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 69/1633 [07:44<16:05:04, 37.02s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_1 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▋         | 103/1633 [11:22<15:10:20, 35.70s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_2 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 137/1633 [15:05<15:26:39, 37.17s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_3 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 171/1633 [18:42<14:51:09, 36.57s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_4 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 205/1633 [22:19<14:45:04, 37.19s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_5 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 239/1633 [25:57<14:15:49, 36.84s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_6 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 273/1633 [29:30<13:44:31, 36.38s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_7 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 307/1633 [33:13<13:46:25, 37.39s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_8 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 341/1633 [36:54<13:28:38, 37.55s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_9 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 375/1633 [40:30<12:50:03, 36.73s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_10 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 409/1633 [44:16<13:13:21, 38.89s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_11 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 443/1633 [47:53<11:54:52, 36.04s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_12 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▉       | 477/1633 [51:29<11:50:00, 36.85s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_13 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███▏      | 511/1633 [55:02<11:18:35, 36.29s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_14 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 33%|███▎      | 545/1633 [58:44<11:59:30, 39.68s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_15 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 35%|███▌      | 579/1633 [1:02:37<11:30:15, 39.29s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_16 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 38%|███▊      | 613/1633 [1:06:33<11:23:56, 40.23s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_17 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 40%|███▉      | 647/1633 [1:10:26<10:54:45, 39.84s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_18 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 42%|████▏     | 681/1633 [1:14:07<9:50:21, 37.21s/it] "]},{"name":"stdout","output_type":"stream","text":["feature_batch_19 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 715/1633 [1:17:51<9:38:54, 37.84s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_20 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 46%|████▌     | 749/1633 [1:21:37<9:31:56, 38.82s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_21 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 48%|████▊     | 783/1633 [1:25:20<9:03:47, 38.39s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_22 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 817/1633 [1:29:10<8:53:26, 39.22s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_23 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 52%|█████▏    | 851/1633 [1:33:10<8:45:10, 40.29s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_24 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 54%|█████▍    | 885/1633 [1:36:59<8:01:38, 38.63s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_25 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 56%|█████▋    | 919/1633 [1:40:39<7:21:59, 37.14s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_26 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 58%|█████▊    | 953/1633 [1:44:23<7:10:15, 37.96s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_27 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 60%|██████    | 987/1633 [1:48:04<6:50:30, 38.13s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_28 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 63%|██████▎   | 1021/1633 [1:51:50<6:35:28, 38.77s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_29 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 65%|██████▍   | 1055/1633 [1:55:44<6:21:39, 39.62s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_30 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 67%|██████▋   | 1089/1633 [1:59:30<5:51:17, 38.75s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_31 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 69%|██████▉   | 1123/1633 [2:03:13<5:22:17, 37.92s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_32 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 71%|███████   | 1157/1633 [2:07:01<5:12:22, 39.37s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_33 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 73%|███████▎  | 1191/1633 [2:10:46<4:45:25, 38.75s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_34 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 75%|███████▌  | 1225/1633 [2:14:33<4:21:44, 38.49s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_35 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 77%|███████▋  | 1259/1633 [2:18:17<3:56:35, 37.96s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_36 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 79%|███████▉  | 1293/1633 [2:21:56<3:33:11, 37.62s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_37 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████▏ | 1327/1633 [2:25:35<3:13:39, 37.97s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_38 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 83%|████████▎ | 1361/1633 [2:29:19<2:55:21, 38.68s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_39 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 85%|████████▌ | 1395/1633 [2:32:57<2:27:53, 37.28s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_40 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 88%|████████▊ | 1429/1633 [2:36:49<2:16:04, 40.02s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_41 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 90%|████████▉ | 1463/1633 [2:40:43<1:53:25, 40.03s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_42 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 92%|█████████▏| 1497/1633 [2:44:34<1:30:53, 40.10s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_43 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 94%|█████████▍| 1531/1633 [2:48:20<1:07:28, 39.69s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_44 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 96%|█████████▌| 1565/1633 [2:52:04<44:20, 39.12s/it]  "]},{"name":"stdout","output_type":"stream","text":["feature_batch_45 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":[" 98%|█████████▊| 1599/1633 [2:55:52<22:48, 40.24s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_46 completes saving.\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1633/1633 [2:59:31<00:00,  6.60s/it]"]},{"name":"stdout","output_type":"stream","text":["feature_batch_47 completes saving.\n","CPU times: user 1h 46min 53s, sys: 48min 44s, total: 2h 35min 38s\n","Wall time: 2h 59min 31s\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["%%time\n","result_iuf_orders = nb.typed.Dict.empty(\n","    key_type = nb.types.int64,\n","    value_type = nb.types.int64[:])\n","\n","features_all_sessions = [] # session, aid, feature tuple\n","                          ## session, aid, feature tuple\n","gc.collect()\n","\n","## Given there are 1671803 sessions in total, we separate them into K batches\n","K = 48\n","# batch_size = 1671803 // K  ##  -> 139316 dealing with around 140k sessions per batch\n","session_per_batch = len(df_test) // K \n","\n","row_idx_cutoffs = [(len(df) - len(df_test)) + (PARALLEL * (session_per_batch//PARALLEL) ) * i for i in range(1, K+3)]   ## batch process every 1024 * 136 rows\n","# 145 -> 148644 // 1024  -> batch_size // PARALLEL\n","\n","feature_batch_id = 0\n","\n","print(\"feature store starts:\")\n","\n","\n","for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n","    start_row = row_idx\n","    end_row = min(row_idx + PARALLEL, len(df))\n","    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n","#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n","    ## run things in parallel\n","    for row_idx in nb.prange(len(rows)):\n","        session, starting_idx, length, start_time = rows[row_idx]\n","        features_tuples_this_session = save_feature_single_session_by_caching(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], item_total_likes, np.array([2.0, 6.0, 6.0]))\n","        features_all_sessions.append(features_tuples_this_session)\n","    \n","    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n","        ## save batch result\n","        rawDf = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n","        batch_result = process_batch_pipeline_subVersion(rawDf)\n","        batch_result.to_parquet(f\"../../allData/submission_phase_data/features_kaggle_eval_set/V2/order_features_last7day/batch_result_{feature_batch_id}.parquet\")\n","        ## clean the memory for next batch\n","        del batch_result, rawDf, features_all_sessions, result_iuf_orders\n","        gc.collect()\n","        ## progress update\n","        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n","        feature_batch_id += 1\n","        ## initiate the struct for new batch again\n","        result_iuf_orders = nb.typed.Dict.empty(\n","            key_type = nb.types.int64,\n","            value_type = nb.types.int64[:])\n","        features_all_sessions = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"},"vscode":{"interpreter":{"hash":"ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"}}},"nbformat":4,"nbformat_minor":4}
