{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar to item_CF_numba, but read different source, and should be code freeze once validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 4.22 s, total: 14.3 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/submission_phase_data/replicate_otto_fast_pipeline_source_data/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ts): 223644219\n",
      "ts[32564]: 1659305046\n",
      "len(aids): 223644219\n",
      "aids[9343654]: 838109\n",
      "len(ops): 223644219\n",
      "ops[797456]:  0\n",
      "df shape: (14571582, 6)\n"
     ]
    }
   ],
   "source": [
    "## sanity check \n",
    "print(\"len(ts):\", len(ts))\n",
    "print(\"ts[32564]:\", ts[32564])\n",
    "print(\"len(aids):\", len(aids))\n",
    "print(\"aids[9343654]:\", aids[9343654])\n",
    "print(\"len(ops):\", len(ops))\n",
    "print(\"ops[797456]: \", ops[797456])\n",
    "print(\"df shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 100   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 1.3 s, total: 28.8 s\n",
      "Wall time: 30.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 998/14231 [01:37<31:40,  6.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1999/14231 [04:22<36:11:24, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2998/14231 [06:28<32:01,  5.85it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3998/14231 [09:13<26:24,  6.46it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 4999/14231 [12:46<37:09:52, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5997/14231 [15:19<13:34, 10.11it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 6998/14231 [16:48<14:04,  8.57it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 7998/14231 [19:03<08:40, 11.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9000/14231 [21:57<15:05:25, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 9999/14231 [23:59<9:14:18,  7.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 11000/14231 [25:58<4:53:53,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 11998/14231 [27:51<02:25, 15.35it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 12999/14231 [29:33<1:46:50,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  13000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 13999/14231 [30:53<14:41,  3.80s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14231/14231 [30:59<00:00,  7.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55min 10s, sys: 28min 52s, total: 1h 24min 3s\n",
      "Wall time: 32min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\"] #, \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 100)\n",
    "            gc.collect()\n",
    "            # break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 80)   ## TODO: make this num small enough to reduce time for normalization, consider keeping 100, give more option for selection\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Saving Feature, Run E or F, depends on mode, this will not only generate result, but the featues associated.\n",
    "**Feature engineering are made here**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def update_feature_vec(aid, features_tuple_arr, features_idx_map, new_feat_tuple):\n",
    "    ## append features\n",
    "    if aid not in features_idx_map:\n",
    "        features_tuple_arr.append(new_feat_tuple)\n",
    "        new_pos = len(features_tuple_arr)-1\n",
    "        ## save the position in the tuple arr\n",
    "        features_idx_map[aid] = new_pos\n",
    "    else: # <is_prev_int, seq_w, time_w, total_ops_w, session_len, # uniuqe aids, CF_score, aid's itemTotalLike >\n",
    "        # ================== 8 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8]\n",
    "        else:\n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8]\n",
    "        # ================== 9 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot9_max_sim = 1\n",
    "        else:\n",
    "            slot9_max_sim = max(new_feat_tuple[9], features_tuple_arr[features_idx_map[aid]][9])\n",
    "        # ================== 10 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot10_mean_sim = 1\n",
    "        else:\n",
    "            slot10_mean_sim = ((features_tuple_arr[features_idx_map[aid]][10] * (features_tuple_arr[features_idx_map[aid]][8]-1) ) + new_feat_tuple[10] ) / features_tuple_arr[features_idx_map[aid]][8]\n",
    "        # ================== 14 seq_w ==\n",
    "        slot14_max_seq_w = max(new_feat_tuple[14], features_tuple_arr[features_idx_map[aid]][14])\n",
    "        # ================== 15 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================== 16 ==\n",
    "        slot16_min_seq_w = min(new_feat_tuple[16], features_tuple_arr[features_idx_map[aid]][16])\n",
    "        # ================== 17 time_w == \n",
    "        slot17_max_time_w = max(new_feat_tuple[17], features_tuple_arr[features_idx_map[aid]][17])\n",
    "        # ================== 18 == \n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 19 ==\n",
    "        slot19_min_time_w = min(new_feat_tuple[19], features_tuple_arr[features_idx_map[aid]][19])\n",
    "        # ================= 20 ops_w ==\n",
    "        slot20_max_ops_w = max(new_feat_tuple[20], features_tuple_arr[features_idx_map[aid]][20])\n",
    "        # ================= 21 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 22 ==\n",
    "        slot22_min_ops_w = min(new_feat_tuple[22], features_tuple_arr[features_idx_map[aid]][22]) #new_feat_tuple[22] if new_feat_tuple[22] < features_tuple_arr[features_idx_map[aid]][22] else features_tuple_arr[features_idx_map[aid]][22]\n",
    "        # ================= 28 cf_incre ==\n",
    "        slot28_max_cf_incre = max(new_feat_tuple[28], features_tuple_arr[features_idx_map[aid]][28])\n",
    "        # ================= 29 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 30 ==\n",
    "        slot30_min_cf_incre = min(new_feat_tuple[30], features_tuple_arr[features_idx_map[aid]][30])\n",
    "\n",
    "        \n",
    "        features_tuple_arr[features_idx_map[aid]] = (new_feat_tuple[0], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3],\n",
    "                                                     new_feat_tuple[4],\n",
    "                                                     new_feat_tuple[5],\n",
    "                                                     new_feat_tuple[6],\n",
    "                                                     new_feat_tuple[7],\n",
    "                                                     slot8_ref_time,\n",
    "                                                     slot9_max_sim,\n",
    "                                                     slot10_mean_sim,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11],\n",
    "                                                     new_feat_tuple[12],\n",
    "                                                     new_feat_tuple[13],\n",
    "                                                     slot14_max_seq_w,\n",
    "                                                     slot15_mean_seq_w,\n",
    "                                                     slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w,\n",
    "                                                     slot18_mean_time_w,\n",
    "                                                     slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w,\n",
    "                                                     slot21_mean_ops_w,\n",
    "                                                     slot22_min_ops_w,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][23] + new_feat_tuple[23], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][24] + new_feat_tuple[24],\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][25] + new_feat_tuple[25],\n",
    "                                                     new_feat_tuple[26],\n",
    "                                                     new_feat_tuple[27],\n",
    "                                                     slot28_max_cf_incre,\n",
    "                                                     slot29_mean_cf_incre,\n",
    "                                                     slot30_min_cf_incre,\n",
    "                                                     slot14_max_seq_w - slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w - slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w - slot22_min_ops_w,\n",
    "                                                     slot28_max_cf_incre - slot30_min_cf_incre\n",
    "                                                     )\n",
    "\n",
    "# <\n",
    "# slot_0: is_prev_interacted, \n",
    "# slot_1: seq_w_total, \n",
    "# slot_2: time_w_total, \n",
    "# slot_3: ops_w_total: for visited item, ops weight total in this session; for unvisited item, ops weight total of the item referencing this item. \n",
    "# slot_4: session_len, \n",
    "# slot_5: num uniuqe aids, \n",
    "# slot_6: CF_score, \n",
    "# slot_7: aid's itemTotalLike: total like score use for normalization.  \n",
    "# slot_8: reference time by similar matrix(if aid visited, default 100; if aid not visited, 1-19(when all aid only interact once, could grow to very large if a lot of actions on one aid), depending how many aid reference this item)\n",
    "# slot_9: max_sim_score:  (1 if it's a visited item)\n",
    "# slot_10: mean_sim_score: (1 if it's a visited item)\n",
    "# slot_11: num_interact, (0 for unvisited item; count of interaction for visited item)\n",
    "# slot_12: time_span of the session \n",
    "# slot_13: action_recency: time to last action(end time), for unvisited items -> the time to of reference_aid to the last action\n",
    "# slot_14: seq_w_max: \n",
    "# slot_15: seq_w_mean: for visited item -> seq_w_total / num_interact; for unvisited item -> seq_w_total / reference_time\n",
    "## ========================= round 2 ================\n",
    "# slot_16: seq_w_min: \n",
    "# slot_17: time_w_max:\n",
    "# slot_18: time_w_mean: similar to slot_15\n",
    "# slot_19: time_w_min\n",
    "# slot_20: ops_w_max:\n",
    "# slot_21: ops_w_mean:\n",
    "# slot_22: ops_w_min: \n",
    "# slot_23: num_clicks: visited item, direct num; unvisited item, take the reference item's num\n",
    "# slot_24: num_carts:\n",
    "# slot_25: num_orders:\n",
    "# slot_26: last_action_type: 0 -> clicks, 1-> carts, 2 -> orders\n",
    "# slot_27: time_to_now: latest interaction time to now \n",
    "# slot_28: cf_increment_max: \n",
    "# slot_29: cf_increment_mean:\n",
    "# slot_30: cf_increment_min:\n",
    "##  ======================== Derivable ============================\n",
    "## slot_31: seq_w max_min_gap: slot_14 - slot_16 \n",
    "## slot_32: time_w_max_min_gap: slot_17 - slot_19\n",
    "## slot_33: ops_w_max_min_gap: slot_20 - slot_22\n",
    "## slot_34: cf_incre_max_min_gap: slot_28 - slot_30\n",
    "# >\n",
    "FEATURE_TUPLE_TEMPLATE = (bool(0), np.float64(0.0), np.float64(0.0), np.int64(0), np.int64(0), np.int64(0), \\\n",
    "    np.float64(0.0), np.float64(0.0), np.int32(0), np.float32(0.0), np.float64(0.0), np.int32(0), np.float64(0.0), np.float64(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "        np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "            np.int32(0), np.int32(0), np.int32(0), np.int32(0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), \\\n",
    "                np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0))\n",
    "\n",
    "FEATURE_NAMES = [\"prev_int\", \"seq_w_total\", \"time_w_total\", \"ops_w_total\", \"session_len\", \"num_uniuqe_aids\", \"CF_score\", \"itemTotalLike\", \"ref_time\", \"max_sim_score\",\\\n",
    "    \"mean_sim_score\", \"num_interact\", \"time_span\", \"action_recency\", \"seq_w_max\", \"seq_w_mean\", \"seq_w_min\", \"time_w_max\", \"time_w_mean\", \"time_w_min\", \\\n",
    "        \"ops_w_max\", \"ops_w_mean\", \"ops_w_min\", \"num_clicks\", \"num_carts\", \"num_orders\", \"last_action_type\", \"time_to_now\", \"cf_incre_max\", \"cf_incre_mean\", \\\n",
    "            \"cf_incre_min\", \"seqW_max_min_gap\", \"timeW_max_min_gap\", \"opsW_max_min_gap\", \"cf_incre_max_min_gap\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main feature save logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, item_total_likes, test_ops_weights):\n",
    "    NOW_TIME = ts[-1] ## ts of latest avaiable action\n",
    "    PREV_INTERACT_BONUS = 30\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    time_span = end_time - start_time\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    ## feature vector template: [aid: <is_prev_int, seq_w, time_w, associated_action, session_len,.. >]\n",
    "    features_tuple_arr = nb.typed.List()\n",
    "    features_tuple_arr.append(FEATURE_TUPLE_TEMPLATE)\n",
    "    features_idx_map = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.int64)\n",
    "\n",
    "    helper_idx = starting_idx\n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## caculate scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op]\n",
    "            potential_to_recommend[aid] += cf_incre #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME-ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## get the scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            potential_to_recommend[aid] += cf_incre\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME - ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                \n",
    "                cf_incre = seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]\n",
    "                potential_to_recommend[similar_item] += cf_incre  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                update_feature_vec(similar_item, features_tuple_arr, features_idx_map, \\\n",
    "                    (0, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[similar_item], \\\n",
    "                        item_total_likes[similar_item], 1, full_sim_matrix[aid][similar_item], full_sim_matrix[aid][similar_item], 0, \\\n",
    "                            time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op,\\\n",
    "                                NOW_TIME-ts[helper_idx], cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 100))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "#         features_save[(session, aid)] = features_tuple_arr[features_idx_map[aid]]\n",
    "        feature_tuples_this_session.append(features_tuple_arr[features_idx_map[aid]])\n",
    "    \n",
    "    return feature_tuples_this_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_gt_tables(type):\n",
    "#     \"\"\" type -> carts / orders \"\"\"\n",
    "#     gt_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "#     gt_labels['aids'] = gt_labels[\"labels\"].apply(lambda x: x.get(type))\n",
    "#     gt_labels = gt_labels[gt_labels.aids.notnull()]\n",
    "#     gt_labels = gt_labels.drop(\"labels\", axis = 1)\n",
    "#     ## ========= special df to identify the unique session id to look at ================\n",
    "#     valid_gt_sessions = gt_labels.drop(\"aids\", axis = 1) \n",
    "#     ## ========================================================================\n",
    "#     ## keep go on for gt labels processing\n",
    "#     gt_labels = gt_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "#     gt_labels[\"gt\"] = 1\n",
    "#     return valid_gt_sessions, gt_labels\n",
    "\n",
    "\"\"\"\n",
    "This is different from validation phase, as we don't have any gt data, therefore, all data has to be exploded and save\n",
    "\"\"\"\n",
    "def process_batch_pipeline_subVersion(rawDf):\n",
    "    \"\"\" rawDf -> Df with session, aids(100), feature_tuple \"\"\"\n",
    "    ## Directly explode aids and feature_tuple\n",
    "    final_df = rawDf.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "\n",
    "    ## open up the feature tuple \n",
    "    # for slot_id, f_name in enumerate(FEATURE_NAMES):\n",
    "    #     final_df[f_name] = final_df[\"feature_tuple\"].apply(lambda x: x[slot_id])\n",
    "    \n",
    "    ## slow method\n",
    "    #final_df[FEATURE_NAMES] = pd.DataFrame([*final_df.feature_tuple], final_df.index)\n",
    "\n",
    "    ## new method\n",
    "    features = np.vstack(final_df[\"feature_tuple\"].values)\n",
    "    temp_df = pd.DataFrame(features)\n",
    "    del features\n",
    "    temp_df.columns = [f'{feat_name}' for feat_name in FEATURE_NAMES]\n",
    "    final_df[temp_df.columns] = temp_df\n",
    "    del temp_df\n",
    "    final_df = final_df.drop(\"feature_tuple\", axis = 1)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1671803"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature store starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-2fe8ac0fe4f8>:51: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, float64, float64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, int64, int64, int64, int32, int64, float64, float64, float64, float64, float64, float64, float64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
      "<ipython-input-16-2fe8ac0fe4f8>:51: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, int64, int64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, bool, bool, bool, int32, int64, float64, float64, float64, int64, int64, int64, int64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
      "<ipython-input-16-2fe8ac0fe4f8>:83: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64, float64, int64, float64, float64, int64, int64, int64, float64, float64, float64, float64, float64, float64, float64, float64, float64, bool, bool, bool, int32, int64, float64, float64, float64, int64, int64, int64, int64) to Tuple(bool, float64, float64, int64, int64, int64, float64, float64, int32, float32, float64, int32, float64, float64, float32, float32, float32, float32, float32, float32, float32, float32, float32, int32, int32, int32, int32, float32, float32, float32, float32, float32, float32, float32, float32). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(similar_item, features_tuple_arr, features_idx_map, \\\n",
      "  8%|▊         | 137/1633 [05:20<28:23:34, 68.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 273/1633 [10:31<25:06:30, 66.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 286/1633 [10:40<50:18,  2.24s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_orders = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1671803 sessions in total, we separate them into K batches\n",
    "K = 12\n",
    "# batch_size = 1671803 // K  ##  -> 139316 dealing with around 140k sessions per batch\n",
    "session_per_batch = len(df_test) // K \n",
    "\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (PARALLEL * (session_per_batch//PARALLEL) ) * i for i in range(1, K+3)]   ## batch process every 1024 * 136 rows\n",
    "# 145 -> 148644 // 1024  -> batch_size // PARALLEL\n",
    "\n",
    "feature_batch_id = 0\n",
    "\n",
    "print(\"feature store starts:\")\n",
    "\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], item_total_likes, np.array([2.0, 6.0, 6.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline_subVersion(rawDf)\n",
    "        batch_result.to_parquet(f\"../../allData/submission_phase_data/features_kaggle_eval_set/V_test/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_orders\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_orders = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save carts features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature store starts:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 137/1633 [12:05<66:05:38, 159.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 273/1633 [23:44<65:13:43, 172.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 409/1633 [34:27<53:46:48, 158.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_2 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 545/1633 [43:50<42:04:15, 139.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_3 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 681/1633 [53:49<38:50:58, 146.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_4 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 817/1633 [1:06:03<41:30:37, 183.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_5 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 953/1633 [1:17:53<33:55:21, 179.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_6 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1089/1633 [1:27:56<22:05:43, 146.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_7 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1225/1633 [1:37:25<15:44:15, 138.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_8 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1361/1633 [1:46:53<10:30:57, 139.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_9 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1497/1633 [1:57:40<6:04:34, 160.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_10 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1633/1633 [2:09:39<00:00,  4.76s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_11 completes saving.\n",
      "CPU times: user 33min 23s, sys: 49min 7s, total: 1h 22min 31s\n",
      "Wall time: 2h 9min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_carts = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1671803 sessions in total, we separate them into K batches\n",
    "K = 12\n",
    "session_per_batch = len(df_test) // K   # 1671803 // K  ##  -> 139316 dealing with around 140k sessions per batch\n",
    "\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (PARALLEL * (session_per_batch//PARALLEL) ) * i for i in range(1, K+3)]   ## batch process every 1024 * 136 rows\n",
    "\n",
    "feature_batch_id = 0\n",
    "\n",
    "print(\"feature store starts:\")\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], item_total_likes, np.array([4.0, 2.0, 5.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline_subVersion(rawDf)\n",
    "        batch_result.to_parquet(f\"../../allData/submission_phase_data/features_kaggle_eval_set/V1/carts_features/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_carts\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_carts = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        \n",
    "    #break\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
