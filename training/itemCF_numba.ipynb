{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item Collaborative Filtering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.9 s, sys: 3.82 s, total: 10.7 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/validationData/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/validationData/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/validationData/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/validationData/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661103727</td>\n",
       "      <td>0</td>\n",
       "      <td>1661103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660857067</td>\n",
       "      <td>147</td>\n",
       "      <td>1660857067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660577379</td>\n",
       "      <td>174</td>\n",
       "      <td>1660577379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661109666</td>\n",
       "      <td>187</td>\n",
       "      <td>1661109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1659304900</td>\n",
       "      <td>413</td>\n",
       "      <td>1659304900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  total_action  session_start_time  session_end_time  start_idx  \\\n",
       "0        0           147          1659304800        1661103727          0   \n",
       "1        1            27          1659304800        1660857067        147   \n",
       "2        2            13          1659304800        1660577379        174   \n",
       "3        3           226          1659304800        1661109666        187   \n",
       "4        4             3          1659304800        1659304900        413   \n",
       "\n",
       "     end_time  \n",
       "0  1661103727  \n",
       "1  1660857067  \n",
       "2  1660577379  \n",
       "3  1661109666  \n",
       "4  1659304900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 100   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                if ts[max_idx] - start_time > 24 * 60 * 60:\n",
    "                    special_factor = 0.8\n",
    "                else:\n",
    "                    special_factor = 1.0\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = special_factor * action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = special_factor * action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 856 ms, total: 21.6 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [02:05<36:13,  5.10it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [02:31<28:01,  6.59it/s]\n",
      "  8%|▊         | 998/12079 [02:44<30:29,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 30s, sys: 5min 43s, total: 18min 14s\n",
      "Wall time: 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\", \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 100)\n",
    "            gc.collect()\n",
    "            break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 80)   ## TODO: make this num small enough to reduce time for normalization\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[int64,float64]<iv=None>({243711: 1.0, 1801351: 0.6420301559334096, 1620477: 0.3205418336285459, 804782: 0.29906089873411057, 760663: 0.29806760860968573, 293222: 0.27718592726596897, 307484: 0.27045098602521467, 807502: 0.26387237977535155, 303302: 0.22713320313972551, 1693043: 0.22420742967890614, 355088: 0.17722081735789424, 1799312: 0.17286524364061515, 899438: 0.1725030172989855, 510211: 0.14385176274047243, 1334074: 0.13439749619481603, 779066: 0.13260267556352995, 527845: 0.11615256735060912, 448706: 0.11398814972170555, 1060697: 0.1109937190198574, 493315: 0.08608320039868826, 596425: 0.07842377422809194, 12725: 0.07664318150644019, 1635429: 0.07586586013132607, 231588: 0.0696293930997437, 1458399: 0.0682591022650933, 307896: 0.06333683757385032, 460942: 0.0574160295826731, 748986: 0.05603382771069549, 1511175: 0.05579452926801223, 102536: 0.048253553003078174, 15594: 0.045834551458824854, 1432934: 0.04521856508329519, 336884: 0.03469253902363178, 266228: 0.03135021377007742, 611085: 0.030041965683350474, 1135739: 0.02917299851297461, 262452: 0.027906841114417238, 1630327: 0.025410143882693043, 269602: 0.02442203519509887, 1697346: 0.02314803961020385, 1414771: 0.015165183225132252, 1692763: 0.014232695107706061, 64583: 0.014080488959693207, 893213: 0.011904000831566617, 16246: 0.011479195198732117, 630588: 0.01136773140612103, 1460029: 0.010882536978523573, 32249: 0.010535582575426503, 604331: 0.009727918467479786, 1730618: 0.008259356647616343, 1472544: 0.007300169789940763, 1462203: 0.006719249483574974, 783573: 0.006682963992500056, 1008954: 0.006134681681122241, 1188193: 0.004690790320152368, 376669: 0.003993137283606761, 1363333: 0.003902253192718231, 1665021: 0.0034458206536111527, 1554960: 0.002772790034351912, 198847: 0.0027281291183118114, 631008: 0.0025770505239755006, 387556: 0.0024929209877854264, 957481: 0.0020510011713242384, 1360972: 0.001795455855064018, 1441532: 0.0016814172262133709, 1363894: 0.0015995501337444699, 1228444: 0.0014926731265303124, 10446: 0.001389825257588493, 602088: 0.001358780943549754, 551874: 0.0009509593229263081, 641664: 0.0008443824254365834, 1198424: 0.0008068020719117819, 1722782: 0.000788563075771276, 1825743: 0.0004641587836717459, 1008932: 0.00043518645988113665, 408067: 0.0004051844879496568, 341650: 0.00038098471164021107, 933686: 0.00025224840650409547, 1313711: 0.00020870063022002816, 591029: 0.00013590029197128723})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A sanity check\n",
    "simMatrices[\"iuf\"][1517085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814440"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(simMatrices[\"iuf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference -- Make prediction using the matrices derived from above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res\n",
    "\n",
    "# ===================================\n",
    "\n",
    "# ===================================\n",
    "\n",
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getRealTimeActionWeight(ts_action, ts_start, ts_end, op, seq, length, ACTION_WEIGHTS, TRAIN_START_TS, TEST_END_TS):\n",
    "    \"\"\"\n",
    "    This function returns the real time importance weight of test set session actions\n",
    "    input: \n",
    "        ts_action: ts of the action takes place\n",
    "        ts_start: start_ts of this session\n",
    "        ts_end: end_ts of this session\n",
    "        op: type of the action\n",
    "        seq: seq order this action\n",
    "        length: total # of actions of this session. \n",
    "    \"\"\"\n",
    "#     overall_time_span = TEST_END_TS - TRAIN_START_TS\n",
    "#     #session_time_span = ts_end - ts_start\n",
    "#     time_to_end_of_session = ts_end - ts_action\n",
    "#     if time_to_end_of_session > 24 * 60 * 60:\n",
    "#         time_weight = 0.3\n",
    "#     else:\n",
    "#         time_weight = max(2**(1 - time_to_end_of_session/3600) - 1, 0.4)\n",
    "    ## TODO: add time decay \n",
    "    action_weight = ACTION_WEIGHTS[op]\n",
    "    sequence_weight = max(2 ** (seq/length) - 1, 0.5) ## floor at 0.1  #np.power(2, np.linspace(seq_weight_const, 1, length))[::-1] - 1\n",
    "    res = action_weight * sequence_weight # * time_weight\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section E: Main Logic in Making Inferences\n",
    "1. clicks_inferences: time_decay sim matrix + regular action weights <1, 6, 3>.\n",
    "2. carts_inferencs: iuf sim matrix + weights <4, 2, 5> (as clicks actions tend to lead to cart action next).\n",
    "3. orders_inferences: iuf sim matrix + regular action weights <1, 6, 3>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    ending_idx = starting_idx + length\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "\n",
    "    ## TODO: Add info_data list to carry all the features, <prev_int, seq_weight, time_weight, >\n",
    "    feat = []\n",
    "    \n",
    "\n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    if len(unique_aids) >= 20:   ## if the user has many actions, recommend based on the itemes it had been interacted with only.\n",
    "        PREV_INTERACT_BONUS = 10\n",
    "        sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "        for aid, op, w in zip(candidates, candidates_ops, sequence_weight):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += w * test_ops_weights[op] * PREV_INTERACT_BONUS \n",
    "        #result[session] = np.array(heap_topk_return_list(potential_to_recommend, 20)) \n",
    "    else:  \n",
    "        for idx in range(starting_idx, ending_idx):\n",
    "            candidate = aids[idx]\n",
    "            if candidate not in potential_to_recommend:\n",
    "                potential_to_recommend[candidate] = np.inf ## ensure large weights on items had interacted with. \n",
    "    \n",
    "    ## In case some items are duplicates, when potential_to_recommend not yet reach 20, impute with items from sim matrix\n",
    "    if len(potential_to_recommend) < 80: ## CAUTIOUS: validation purpose only \n",
    "        sequence_weight = np.power(2, np.linspace(0.1, 1, len(candidates))) - 1   ## CHANGE_MADE: 0.3 -> 0.1\n",
    "        for idx in range(starting_idx, ending_idx):\n",
    "            candidate = aids[idx] \n",
    "            time_weight = 1 + 0.1 ** ((1662328791-ts[idx])/(1662328791-1659304800))    ## TODO: consider \n",
    "            candidate_realtime_weight = test_ops_weights[ops[idx]] * sequence_weight[idx-starting_idx] * time_weight  \n",
    "            ## load the potential items to recommend,\n",
    "            if candidate not in full_sim_matrix: \n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[candidate]:\n",
    "#                 if similar_item in candidates:    ## skip the item if the it's already been interacted\n",
    "#                     continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += full_sim_matrix[candidate][similar_item] * candidate_realtime_weight \n",
    "    \n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 80))   ## CAUTIOUS: validation purpose only\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def run_inference_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [08:22<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 34s, sys: 53.8 s, total: 7min 28s\n",
      "Wall time: 8min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_2 = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_time_decay = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf_2, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))   ## considebly add the weights for click action in the real time.\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_time_decay, simMatrices[\"time_decay\"], np.array([3.0, 6.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11830, 1732105,  588923,  571762,  884502,  876129, 1157882,\n",
       "       1182614,  307904, 1790438,  231487, 1517680, 1633746,   77440,\n",
       "       1586171,  855613,  735729,  205357,  756588,  523174,  258814,\n",
       "        636101,  322370, 1689044,  532616, 1317291, 1718231, 1609734,\n",
       "        409620, 1125638,  603583,  215561, 1052212, 1394029,  822934,\n",
       "       1853703, 1383529, 1425172,  135833,  500334, 1519088,   19468,\n",
       "       1349230, 1390152,  542780, 1667019,  490677,  448755, 1169267,\n",
       "       1428162, 1695994,   42241,  804966, 1677053, 1311701,  672942,\n",
       "        487136, 1197172,   45494,  697336, 1572401,  432989, 1446918,\n",
       "        577040,  696526, 1241036, 1123537, 1307712, 1041771,  600258,\n",
       "       1307461,  293000,  460148, 1032776,   87442,  198496,  972466,\n",
       "       1421968, 1097576,  823991])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_time_decay[11098528]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37988"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Save the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length\n",
    "    end_time = ts[ending_idx]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "\n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "\n",
    "    ## item_features\n",
    "    item_features = []\n",
    "\n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        if a not in unique_aids.keys():\n",
    "            unique_aids[a] = 0\n",
    "        unique_aids[a] += 1\n",
    "\n",
    "    if len(unique_aids) >= 20:\n",
    "        \n",
    "        sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "        time_weights = []\n",
    "        for idx in range(starting_idx, ending_idx):\n",
    "            if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "                time_weight = (1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))) * NEARBY_ACTION_BONUS\n",
    "            else:\n",
    "                time_weight = 1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))\n",
    "            time_weights.append(time_weight)\n",
    "        time_weights = time_weights[::-1]\n",
    "\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            item_features.append(seq_w, time_w, 1)\n",
    "            \n",
    "\n",
    "   \n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def save_feature_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 33s, sys: 18.9 s, total: 5min 52s\n",
      "Wall time: 6min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 1732105 588923 571762 884502 876129 1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 1632356 1049489 612829 295362 333991 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 583026 963957 254154 752...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>1728212 1557766 1553691 1449555 1365569 130963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 1202618 1159379 77906 476681 17040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351206</th>\n",
       "      <td>12899774_orders</td>\n",
       "      <td>33035 1539309 819288 771913 31490 95488 218795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351207</th>\n",
       "      <td>12899775_orders</td>\n",
       "      <td>1743151 1760714 1163166 1255910 1498443 783827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351208</th>\n",
       "      <td>12899776_orders</td>\n",
       "      <td>548599 695829 1737908 773354 1762353 1440959 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351209</th>\n",
       "      <td>12899777_orders</td>\n",
       "      <td>384045 1308634 1688215 395762 703474 1486067 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351210</th>\n",
       "      <td>12899778_orders</td>\n",
       "      <td>561560 1167224 32070 971566 1175618 1314651 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5351211 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098528_clicks  11830 1732105 588923 571762 884502 876129 1157...\n",
       "1        11098529_clicks  1105029 1632356 1049489 612829 295362 333991 1...\n",
       "2        11098530_clicks  409236 264500 1603001 583026 963957 254154 752...\n",
       "3        11098531_clicks  1728212 1557766 1553691 1449555 1365569 130963...\n",
       "4        11098532_clicks  876469 7651 1202618 1159379 77906 476681 17040...\n",
       "...                  ...                                                ...\n",
       "5351206  12899774_orders  33035 1539309 819288 771913 31490 95488 218795...\n",
       "5351207  12899775_orders  1743151 1760714 1163166 1255910 1498443 783827...\n",
       "5351208  12899776_orders  548599 695829 1737908 773354 1762353 1440959 9...\n",
       "5351209  12899777_orders  384045 1308634 1688215 395762 703474 1486067 2...\n",
       "5351210  12899778_orders  561560 1167224 32070 971566 1175618 1314651 39...\n",
       "\n",
       "[5351211 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subs = []\n",
    "op_names = [\"clicks\", \"carts\", \"orders\"]\n",
    "\n",
    "for result, op in zip([result_time_decay, result_iuf_2, result_iuf], op_names):\n",
    "    sub = pd.DataFrame({\"session_type\": result.keys(), \"labels\": result.values()})\n",
    "    sub.session_type = sub.session_type.astype(str) + f\"_{op}\"\n",
    "    sub.labels = sub.labels.apply(lambda x: \" \".join(x.astype(str)))\n",
    "    subs.append(sub)\n",
    "    \n",
    "submission = pd.concat(subs).reset_index(drop=True)\n",
    "#sub.sort_values(by=[\"session_type\"])  ## optional\n",
    "#submission.to_csv('submission.csv', index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 s, sys: 3.32 s, total: 50.3 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submission.to_csv('../../allData/validationData/p_v_579_80_items.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submission.iloc[0][\"labels\"].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783737"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
