{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the baseline itemCF_numba, and add feature store function to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.75 s, sys: 2.96 s, total: 9.71 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/validationData/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/validationData/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/validationData/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/validationData/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661103727</td>\n",
       "      <td>0</td>\n",
       "      <td>1661103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660857067</td>\n",
       "      <td>147</td>\n",
       "      <td>1660857067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660577379</td>\n",
       "      <td>174</td>\n",
       "      <td>1660577379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661109666</td>\n",
       "      <td>187</td>\n",
       "      <td>1661109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1659304900</td>\n",
       "      <td>413</td>\n",
       "      <td>1659304900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  total_action  session_start_time  session_end_time  start_idx  \\\n",
       "0        0           147          1659304800        1661103727          0   \n",
       "1        1            27          1659304800        1660857067        147   \n",
       "2        2            13          1659304800        1660577379        174   \n",
       "3        3           226          1659304800        1661109666        187   \n",
       "4        4             3          1659304800        1659304900        413   \n",
       "\n",
       "     end_time  \n",
       "0  1661103727  \n",
       "1  1660857067  \n",
       "2  1660577379  \n",
       "3  1661109666  \n",
       "4  1659304900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 100   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                if ts[max_idx] - start_time > 24 * 60 * 60:\n",
    "                    special_factor = 0.8\n",
    "                else:\n",
    "                    special_factor = 1.0\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = special_factor * action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = special_factor * action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 763 ms, total: 19.6 s\n",
      "Wall time: 19.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [01:35<23:50,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1999/12079 [05:02<31:26:53, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2999/12079 [07:39<15:45:28,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3999/12079 [10:06<29:34:23, 13.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 4999/12079 [12:34<28:25:53, 14.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5997/12079 [14:00<09:24, 10.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7000/12079 [16:48<7:06:36,  5.04s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 7999/12079 [18:45<16:55:04, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 8998/12079 [19:50<03:31, 14.56it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 9999/12079 [22:26<4:18:01,  7.44s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 11000/12079 [23:51<1:29:35,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11999/12079 [25:03<04:37,  3.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12079/12079 [25:05<00:00,  8.03it/s]\n",
      "  8%|▊         | 999/12079 [02:55<27:11:34,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1998/12079 [05:30<24:55,  6.74it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2999/12079 [09:43<35:04:35, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3999/12079 [12:25<29:37:06, 13.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 5000/12079 [15:17<21:26:41, 10.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5999/12079 [17:43<26:03:26, 15.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6999/12079 [20:00<22:43:57, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 8000/12079 [22:09<12:54:32, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 8999/12079 [24:15<13:23:26, 15.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 9999/12079 [26:11<4:29:20,  7.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10999/12079 [27:41<1:06:55,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11999/12079 [28:57<09:25,  7.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12079/12079 [28:59<00:00,  6.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 36min 27s, sys: 53min 3s, total: 2h 29min 31s\n",
      "Wall time: 56min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\", \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 100)\n",
    "            gc.collect()\n",
    "            # break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 80)   ## TODO: make this num small enough to reduce time for normalization\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A sanity check\n",
    "#simMatrices[\"iuf\"][1517085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814440"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(simMatrices[\"iuf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference -- Make prediction using the matrices derived from above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section E: Main Logic in Making Inferences\n",
    "1. clicks_inferences: time_decay sim matrix + regular action weights <1, 6, 3>.\n",
    "2. carts_inferencs: iuf sim matrix + weights <4, 2, 5> (as clicks actions tend to lead to cart action next).\n",
    "3. orders_inferences: iuf sim matrix + regular action weights <1, 6, 3>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length\n",
    "    end_time = ts[ending_idx]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += np.inf #seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 20))\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def run_inference_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [08:22<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 34s, sys: 53.8 s, total: 7min 28s\n",
      "Wall time: 8min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_2 = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_time_decay = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf_2, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))   ## considebly add the weights for click action in the real time.\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_time_decay, simMatrices[\"time_decay\"], np.array([3.0, 6.0, 3.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions - Convert results to csv, get validation set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 33s, sys: 18.9 s, total: 5min 52s\n",
      "Wall time: 6min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 1732105 588923 571762 884502 876129 1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 1632356 1049489 612829 295362 333991 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 583026 963957 254154 752...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>1728212 1557766 1553691 1449555 1365569 130963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 1202618 1159379 77906 476681 17040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351206</th>\n",
       "      <td>12899774_orders</td>\n",
       "      <td>33035 1539309 819288 771913 31490 95488 218795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351207</th>\n",
       "      <td>12899775_orders</td>\n",
       "      <td>1743151 1760714 1163166 1255910 1498443 783827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351208</th>\n",
       "      <td>12899776_orders</td>\n",
       "      <td>548599 695829 1737908 773354 1762353 1440959 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351209</th>\n",
       "      <td>12899777_orders</td>\n",
       "      <td>384045 1308634 1688215 395762 703474 1486067 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351210</th>\n",
       "      <td>12899778_orders</td>\n",
       "      <td>561560 1167224 32070 971566 1175618 1314651 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5351211 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098528_clicks  11830 1732105 588923 571762 884502 876129 1157...\n",
       "1        11098529_clicks  1105029 1632356 1049489 612829 295362 333991 1...\n",
       "2        11098530_clicks  409236 264500 1603001 583026 963957 254154 752...\n",
       "3        11098531_clicks  1728212 1557766 1553691 1449555 1365569 130963...\n",
       "4        11098532_clicks  876469 7651 1202618 1159379 77906 476681 17040...\n",
       "...                  ...                                                ...\n",
       "5351206  12899774_orders  33035 1539309 819288 771913 31490 95488 218795...\n",
       "5351207  12899775_orders  1743151 1760714 1163166 1255910 1498443 783827...\n",
       "5351208  12899776_orders  548599 695829 1737908 773354 1762353 1440959 9...\n",
       "5351209  12899777_orders  384045 1308634 1688215 395762 703474 1486067 2...\n",
       "5351210  12899778_orders  561560 1167224 32070 971566 1175618 1314651 39...\n",
       "\n",
       "[5351211 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subs = []\n",
    "op_names = [\"clicks\", \"carts\", \"orders\"]\n",
    "\n",
    "for result, op in zip([result_time_decay, result_iuf_2, result_iuf], op_names):\n",
    "    sub = pd.DataFrame({\"session_type\": result.keys(), \"labels\": result.values()})\n",
    "    sub.session_type = sub.session_type.astype(str) + f\"_{op}\"\n",
    "    sub.labels = sub.labels.apply(lambda x: \" \".join(x.astype(str)))\n",
    "    subs.append(sub)\n",
    "    \n",
    "submission = pd.concat(subs).reset_index(drop=True)\n",
    "#sub.sort_values(by=[\"session_type\"])  ## optional\n",
    "#submission.to_csv('submission.csv', index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 s, sys: 3.32 s, total: 50.3 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submission.to_csv('../../allData/validationData/p_v_579_80_items.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Saving Feature, Run E or F, depends on mode, this will not only generate result, but the featues associated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def update_feature_vec(aid, features_tuple_arr, features_idx_map, new_feat_tuple):\n",
    "    ## append features\n",
    "    if aid not in features_idx_map:\n",
    "        features_tuple_arr.append(new_feat_tuple)\n",
    "        new_pos = len(features_tuple_arr)-1\n",
    "        ## save the position in the tuple arr\n",
    "        features_idx_map[aid] = new_pos\n",
    "    else: # <is_prev_int, seq_w, time_w, associated_action, session_len,.. >\n",
    "        features_tuple_arr[features_idx_map[aid]] = (new_feat_tuple[0], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3],\n",
    "                                                     new_feat_tuple[4],\n",
    "                                                     new_feat_tuple[5],\n",
    "                                                     new_feat_tuple[6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main feature save logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    ## feature vector template: [aid: <is_prev_int, seq_w, time_w, associated_action, session_len,.. >]\n",
    "    features_tuple_arr = nb.typed.List()\n",
    "    features_tuple_arr.append((bool(0), np.float64(0.0), np.float64(0.0), np.int64(0), np.int64(0), np.int64(0), np.float64(0.0)))\n",
    "    features_idx_map = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.int64)\n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## caculate scores\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, op, length, len(unique_aids), potential_to_recommend[aid]))\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## get the scores\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, op, length, len(unique_aids), potential_to_recommend[aid]))\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                update_feature_vec(similar_item, features_tuple_arr, features_idx_map, (0, seq_w, time_w, op, length, len(unique_aids), potential_to_recommend[similar_item]))\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 100))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "#         features_save[(session, aid)] = features_tuple_arr[features_idx_map[aid]]\n",
    "        feature_tuples_this_session.append(features_tuple_arr[features_idx_map[aid]])\n",
    "    \n",
    "    return feature_tuples_this_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [14:33<00:00,  1.99it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 21s, sys: 2min 25s, total: 6min 47s\n",
      "Wall time: 14min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_orders = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_iuf_carts = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = []\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-35e5c8a5ab93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.1 s, sys: 1min 49s, total: 2min 22s\n",
      "Wall time: 8min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8032"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "features_save = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n",
    "features_save = features_save.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "del features_all_sessions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_save[\"prev_int\"] = features_save[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "features_save[\"seq_w\"] = features_save[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "features_save[\"time_w\"] = features_save[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "features_save[\"ops_total\"] = features_save[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "features_save[\"session_len\"] = features_save[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "features_save[\"session_unique_aid\"] = features_save[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "features_save[\"rank_score\"] = features_save[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "features_save = features_save.drop(\"feature_tuple\", axis = 1)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aids</th>\n",
       "      <th>prev_int</th>\n",
       "      <th>seq_w</th>\n",
       "      <th>time_w</th>\n",
       "      <th>ops_total</th>\n",
       "      <th>session_len</th>\n",
       "      <th>session_unique_aid</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>11830</td>\n",
       "      <td>True</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.868665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098528</td>\n",
       "      <td>1732105</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.386866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098528</td>\n",
       "      <td>588923</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098528</td>\n",
       "      <td>571762</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098528</td>\n",
       "      <td>884502</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.354726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156190584</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1745743</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156190585</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1028447</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156190586</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1543085</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.052005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156190587</th>\n",
       "      <td>12899778</td>\n",
       "      <td>1653733</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156190588</th>\n",
       "      <td>12899778</td>\n",
       "      <td>312301</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156190589 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session     aids  prev_int     seq_w  time_w  ops_total  \\\n",
       "0          11098528    11830      True  0.231144     3.0          0   \n",
       "1          11098528  1732105     False  0.231144     3.0          0   \n",
       "2          11098528   588923     False  0.231144     3.0          0   \n",
       "3          11098528   571762     False  0.231144     3.0          0   \n",
       "4          11098528   884502     False  0.231144     3.0          0   \n",
       "...             ...      ...       ...       ...     ...        ...   \n",
       "156190584  12899778  1745743     False  0.231144     3.0          0   \n",
       "156190585  12899778  1028447     False  0.231144     3.0          0   \n",
       "156190586  12899778  1543085     False  0.231144     3.0          0   \n",
       "156190587  12899778  1653733     False  0.231144     3.0          0   \n",
       "156190588  12899778   312301     False  0.231144     3.0          0   \n",
       "\n",
       "           session_len  session_unique_aid  rank_score  \n",
       "0                    1                   1   13.868665  \n",
       "1                    1                   1    1.386866  \n",
       "2                    1                   1    0.733088  \n",
       "3                    1                   1    0.376020  \n",
       "4                    1                   1    0.354726  \n",
       "...                ...                 ...         ...  \n",
       "156190584            1                   1    0.052593  \n",
       "156190585            1                   1    0.052512  \n",
       "156190586            1                   1    0.052005  \n",
       "156190587            1                   1    0.051983  \n",
       "156190588            1                   1    0.051917  \n",
       "\n",
       "[156190589 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_save.to_parquet(\"../../allData/features/orders_features_100_per_session.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
