{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the baseline itemCF_numba, and add feature store function to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.76 s, sys: 2.82 s, total: 9.58 s\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/validationData/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/validationData/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/validationData/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/validationData/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661103727</td>\n",
       "      <td>0</td>\n",
       "      <td>1661103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660857067</td>\n",
       "      <td>147</td>\n",
       "      <td>1660857067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660577379</td>\n",
       "      <td>174</td>\n",
       "      <td>1660577379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661109666</td>\n",
       "      <td>187</td>\n",
       "      <td>1661109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1659304900</td>\n",
       "      <td>413</td>\n",
       "      <td>1659304900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  total_action  session_start_time  session_end_time  start_idx  \\\n",
       "0        0           147          1659304800        1661103727          0   \n",
       "1        1            27          1659304800        1660857067        147   \n",
       "2        2            13          1659304800        1660577379        174   \n",
       "3        3           226          1659304800        1661109666        187   \n",
       "4        4             3          1659304800        1659304900        413   \n",
       "\n",
       "     end_time  \n",
       "0  1661103727  \n",
       "1  1660857067  \n",
       "2  1660577379  \n",
       "3  1661109666  \n",
       "4  1659304900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 200   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                if ts[max_idx] - start_time > 24 * 60 * 60:\n",
    "                    special_factor = 0.8\n",
    "                else:\n",
    "                    special_factor = 1.0\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = special_factor * action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = special_factor * action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 725 ms, total: 18.6 s\n",
      "Wall time: 18.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [01:16<18:21, 10.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1999/12079 [03:30<25:30:23,  9.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2999/12079 [05:29<13:00:30,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3999/12079 [07:22<24:06:53, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 4999/12079 [09:30<11:46:00,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5999/12079 [21:20<86:33:36, 51.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6998/12079 [23:11<05:39, 14.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 7999/12079 [24:57<4:52:05,  4.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 8998/12079 [26:36<03:34, 14.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 9999/12079 [28:19<8:09:12, 14.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 11001/12079 [29:36<1:22:19,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11999/12079 [30:41<05:34,  4.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12079/12079 [30:43<00:00,  6.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39min 23s, sys: 17min 58s, total: 57min 21s\n",
      "Wall time: 31min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\"] #, \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 100)\n",
    "            gc.collect()\n",
    "            # break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 80)   ## TODO: make this num small enough to reduce time for normalization\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[int64,float64]<iv=None>({331941: 1.0, 243711: 0.7190634944882741, 1801351: 0.5040513231086448, 371417: 0.4167012125283079, 32249: 0.41373445935315617, 899438: 0.3626232356223332, 303302: 0.3081623040015135, 461689: 0.24140817919093385, 1799312: 0.20439432086051285, 576535: 0.20363825079722686, 516937: 0.17150816792050863, 1500897: 0.1474968554333243, 807502: 0.12350333543572184, 528847: 0.12185781543011626, 734463: 0.12045400414403375, 460942: 0.1139099425003342, 12725: 0.10691090671837272, 215107: 0.09943509171737062, 893213: 0.09210204073235082, 1364441: 0.0843909572280193, 1620477: 0.07891101754442596, 355088: 0.0760826198068775, 1630327: 0.07572153181228192, 804782: 0.07362283905259837, 760663: 0.07337831080008321, 1390627: 0.07327155250282527, 1775482: 0.07042982880108292, 293222: 0.06833266345318563, 307484: 0.06657964815873083, 1432934: 0.06421905471330858, 1190046: 0.06345478375755678, 1371202: 0.0631481939274279, 1350724: 0.06124414306732429, 800979: 0.05847478548902716, 1693043: 0.05519540528206179, 932220: 0.0548184226341905, 16246: 0.05382331288999998, 808248: 0.0468322918606895, 598127: 0.046760823146036384, 103072: 0.046309211766148904, 396565: 0.046030035777334305, 1110941: 0.04092365838339326, 1099100: 0.03619644771308173, 510211: 0.03541343993983789, 639347: 0.03419103352166187, 1334074: 0.033085987748001644, 779066: 0.032644138642937116, 551290: 0.03158274416788717, 1349093: 0.03030716586650167, 652554: 0.030212446221542864, 527845: 0.028594449517798545, 1436167: 0.02846870027639107, 448706: 0.028061612990489603, 1060697: 0.027324443769941212, 723528: 0.02612315374262451, 1068049: 0.025968607215378716, 102536: 0.024083005848090942, 943586: 0.02206653719173201, 493315: 0.02119197004660886, 269602: 0.02037388714337668, 596425: 0.01930637182036119, 1635429: 0.018677605376125284, 202224: 0.01754375408122563, 1542438: 0.017252435250822866, 231588: 0.01714137028013897, 1458399: 0.016804031958741056, 22721: 0.016564447257208324, 447175: 0.016118699527914587, 307896: 0.015592268392619143, 810071: 0.015403622390389955, 851412: 0.014540034870713103, 1794652: 0.014092601761241007, 748986: 0.013794412765118282, 1511175: 0.013735502252892239, 968570: 0.013560333072959863, 292471: 0.011918520614207184, 317435: 0.011816882064306762, 77442: 0.011655700039315542, 15594: 0.01128355401653921, 37942: 0.011175260405487176})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A sanity check\n",
    "simMatrices[\"iuf\"][1517085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(simMatrices[\"iuf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference -- Make prediction using the matrices derived from above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section E: Main Logic in Making Inferences\n",
    "1. clicks_inferences: time_decay sim matrix + regular action weights <1, 6, 3>.\n",
    "2. carts_inferencs: iuf sim matrix + weights <4, 2, 5> (as clicks actions tend to lead to cart action next).\n",
    "3. orders_inferences: iuf sim matrix + regular action weights <1, 6, 3>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length\n",
    "    end_time = ts[ending_idx]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += np.inf #seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 20))\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def run_inference_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [08:22<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 34s, sys: 53.8 s, total: 7min 28s\n",
      "Wall time: 8min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_2 = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "result_time_decay = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_iuf_2, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))   ## considebly add the weights for click action in the real time.\n",
    "    run_inference_parallel(rows, aids, ops, ts, result_time_decay, simMatrices[\"time_decay\"], np.array([3.0, 6.0, 3.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions - Convert results to csv, get validation set result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 33s, sys: 18.9 s, total: 5min 52s\n",
      "Wall time: 6min 12s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session_type</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528_clicks</td>\n",
       "      <td>11830 1732105 588923 571762 884502 876129 1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098529_clicks</td>\n",
       "      <td>1105029 1632356 1049489 612829 295362 333991 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098530_clicks</td>\n",
       "      <td>409236 264500 1603001 583026 963957 254154 752...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098531_clicks</td>\n",
       "      <td>1728212 1557766 1553691 1449555 1365569 130963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098532_clicks</td>\n",
       "      <td>876469 7651 1202618 1159379 77906 476681 17040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351206</th>\n",
       "      <td>12899774_orders</td>\n",
       "      <td>33035 1539309 819288 771913 31490 95488 218795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351207</th>\n",
       "      <td>12899775_orders</td>\n",
       "      <td>1743151 1760714 1163166 1255910 1498443 783827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351208</th>\n",
       "      <td>12899776_orders</td>\n",
       "      <td>548599 695829 1737908 773354 1762353 1440959 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351209</th>\n",
       "      <td>12899777_orders</td>\n",
       "      <td>384045 1308634 1688215 395762 703474 1486067 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351210</th>\n",
       "      <td>12899778_orders</td>\n",
       "      <td>561560 1167224 32070 971566 1175618 1314651 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5351211 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            session_type                                             labels\n",
       "0        11098528_clicks  11830 1732105 588923 571762 884502 876129 1157...\n",
       "1        11098529_clicks  1105029 1632356 1049489 612829 295362 333991 1...\n",
       "2        11098530_clicks  409236 264500 1603001 583026 963957 254154 752...\n",
       "3        11098531_clicks  1728212 1557766 1553691 1449555 1365569 130963...\n",
       "4        11098532_clicks  876469 7651 1202618 1159379 77906 476681 17040...\n",
       "...                  ...                                                ...\n",
       "5351206  12899774_orders  33035 1539309 819288 771913 31490 95488 218795...\n",
       "5351207  12899775_orders  1743151 1760714 1163166 1255910 1498443 783827...\n",
       "5351208  12899776_orders  548599 695829 1737908 773354 1762353 1440959 9...\n",
       "5351209  12899777_orders  384045 1308634 1688215 395762 703474 1486067 2...\n",
       "5351210  12899778_orders  561560 1167224 32070 971566 1175618 1314651 39...\n",
       "\n",
       "[5351211 rows x 2 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "subs = []\n",
    "op_names = [\"clicks\", \"carts\", \"orders\"]\n",
    "\n",
    "for result, op in zip([result_time_decay, result_iuf_2, result_iuf], op_names):\n",
    "    sub = pd.DataFrame({\"session_type\": result.keys(), \"labels\": result.values()})\n",
    "    sub.session_type = sub.session_type.astype(str) + f\"_{op}\"\n",
    "    sub.labels = sub.labels.apply(lambda x: \" \".join(x.astype(str)))\n",
    "    subs.append(sub)\n",
    "    \n",
    "submission = pd.concat(subs).reset_index(drop=True)\n",
    "#sub.sort_values(by=[\"session_type\"])  ## optional\n",
    "#submission.to_csv('submission.csv', index = False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47 s, sys: 3.32 s, total: 50.3 s\n",
      "Wall time: 50.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "submission.to_csv('../../allData/validationData/p_v_579_80_items.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Saving Feature, Run E or F, depends on mode, this will not only generate result, but the featues associated."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def update_feature_vec(aid, features_tuple_arr, features_idx_map, new_feat_tuple):\n",
    "    ## append features\n",
    "    if aid not in features_idx_map:\n",
    "        features_tuple_arr.append(new_feat_tuple)\n",
    "        new_pos = len(features_tuple_arr)-1\n",
    "        ## save the position in the tuple arr\n",
    "        features_idx_map[aid] = new_pos\n",
    "    else: # <is_prev_int, seq_w, time_w, total_ops_w, session_len, # uniuqe aids, CF_score >\n",
    "        features_tuple_arr[features_idx_map[aid]] = (new_feat_tuple[0], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3],\n",
    "                                                     new_feat_tuple[4],\n",
    "                                                     new_feat_tuple[5],\n",
    "                                                     new_feat_tuple[6])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main feature save logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    ## feature vector template: [aid: <is_prev_int, seq_w, time_w, associated_action, session_len,.. >]\n",
    "    features_tuple_arr = nb.typed.List()\n",
    "    features_tuple_arr.append((bool(0), np.float64(0.0), np.float64(0.0), np.int64(0), np.int64(0), np.int64(0), np.float64(0.0)))\n",
    "    features_idx_map = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.int64)\n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## caculate scores\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid]))\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## get the scores\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid]))\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                update_feature_vec(similar_item, features_tuple_arr, features_idx_map, (0, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[similar_item]))\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 100))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "#         features_save[(session, aid)] = features_tuple_arr[features_idx_map[aid]]\n",
    "        feature_tuples_this_session.append(features_tuple_arr[features_idx_map[aid]])\n",
    "    \n",
    "    return feature_tuples_this_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]<ipython-input-12-e41a977c9bca>:47: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64) to Tuple(bool, float64, float64, int64, int64, int64, float64). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid]))\n",
      "100%|██████████| 1742/1742 [10:23<00:00,  2.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 12s, sys: 1min 46s, total: 5min 59s\n",
      "Wall time: 10min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_orders = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_iuf_carts = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = []\n",
    "gc.collect()\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.3 s, sys: 32.1 s, total: 42.4 s\n",
      "Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10031"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "features_save = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n",
    "# features_save = features_save.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "del features_all_sessions\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO STH HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get all valid sessions with orders actions\n",
    "## there are 150179 / 1783737 sessions have orders actions, 301057 / 1783737 have carts, 1737968 / 1783737 have clicks\n",
    "valid_order_sessions = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "valid_order_sessions['aids'] = valid_order_sessions[\"labels\"].apply(lambda x: x.get(\"orders\"))\n",
    "valid_order_sessions = valid_order_sessions.loc[valid_order_sessions.aids.notnull()]\n",
    "valid_order_sessions = valid_order_sessions.drop(\"labels\", axis = 1)\n",
    "valid_order_sessions = valid_order_sessions.drop(\"aids\", axis = 1)\n",
    "\n",
    "\n",
    "%%time\n",
    "## A total of 311649 that's predictable \n",
    "orders_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "orders_labels['aids'] = orders_labels[\"labels\"].apply(lambda x: x.get(\"orders\"))\n",
    "orders_labels = orders_labels[orders_labels.aids.notnull()]\n",
    "orders_labels = orders_labels.drop(\"labels\", axis = 1)\n",
    "## ========= special df to identify the unique session id to look at ================\n",
    "valid_order_sessions = orders_labels.drop(\"aids\", axis = 1) \n",
    "## ========================================================================\n",
    "# orders_labels[\"gt_type\"] = 2\n",
    "## keep go on for order labels processing\n",
    "orders_labels = orders_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "orders_labels[\"gt\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session    150179\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sanity check\n",
    "valid_order_sessions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join with features_save, now only order_features in valid sessions are kept\n",
    "order_features_valid_session = pd.merge(features_save, valid_order_sessions, on=\"session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete valid_order_sessions, features_save to save memory\n",
    "del valid_order_sessions, features_save\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explode the orders_featuers_valid_session, and join with order_labels to get the final trainable df, and there are 13601844 rows for the train data. \n",
    "order_features_valid_session = order_features_valid_session.set_index(['session']).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13601844, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_features_valid_session.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "orders_full_df = pd.merge(order_features_valid_session, orders_labels, on=[\"session\", \"aids\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_full_df[\"prev_int\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "orders_full_df[\"seq_w\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "orders_full_df[\"time_w\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "orders_full_df[\"ops_total\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "print(\"Done with ops_total\")\n",
    "orders_full_df[\"session_len\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "orders_full_df[\"session_unique_aid\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "orders_full_df[\"rank_score\"] = orders_full_df[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "orders_full_df = orders_full_df.drop(\"feature_tuple\", axis = 1)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13601844, 10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aids</th>\n",
       "      <th>gt</th>\n",
       "      <th>prev_int</th>\n",
       "      <th>seq_w</th>\n",
       "      <th>time_w</th>\n",
       "      <th>ops_total</th>\n",
       "      <th>session_len</th>\n",
       "      <th>session_unique_aid</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>11830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13.868665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098528</td>\n",
       "      <td>1732105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.386866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aids   gt  prev_int     seq_w  time_w  ops_total  session_len  \\\n",
       "0  11098528    11830  1.0      True  0.231144     3.0          2            1   \n",
       "1  11098528  1732105  NaN     False  0.231144     3.0          2            1   \n",
       "\n",
       "   session_unique_aid  rank_score  \n",
       "0                   1   13.868665  \n",
       "1                   1    1.386866  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## final output format \n",
    "orders_full_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.79 s, sys: 860 ms, total: 4.65 s\n",
      "Wall time: 4.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "orders_full_df.to_parquet(\"../../allData/features/orders_features_100_per_session_V3_.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
