{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactor the baseline itemCF_numba, and add feature store function to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import heapq\n",
    "import pickle\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.73 s, sys: 3.35 s, total: 10.1 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_csv(\"../../allData/validationData/train_meta_data.csv\")\n",
    "df_test = pd.read_csv(\"../../allData/validationData/test_meta_data.csv\")\n",
    "df = pd.concat([df, df_test]).reset_index(drop = True)\n",
    "npz = np.load(\"../../allData/validationData/train_core_data.npz\")\n",
    "npz_test = np.load(\"../../allData/validationData/test_core_data.npz\")\n",
    "aids = np.concatenate([npz['aids'], npz_test['aids']])\n",
    "ts = np.concatenate([npz['ts'], npz_test['ts']])\n",
    "ops = np.concatenate([npz['ops'], npz_test['ops']])\n",
    "\n",
    "df[\"start_idx\"] = df['total_action'].cumsum().shift(1).fillna(0).astype(int)\n",
    "df[\"end_time\"] = ts[df[\"start_idx\"] + df[\"total_action\"] - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>total_action</th>\n",
       "      <th>session_start_time</th>\n",
       "      <th>session_end_time</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661103727</td>\n",
       "      <td>0</td>\n",
       "      <td>1661103727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660857067</td>\n",
       "      <td>147</td>\n",
       "      <td>1660857067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1660577379</td>\n",
       "      <td>174</td>\n",
       "      <td>1660577379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>226</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1661109666</td>\n",
       "      <td>187</td>\n",
       "      <td>1661109666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1659304800</td>\n",
       "      <td>1659304900</td>\n",
       "      <td>413</td>\n",
       "      <td>1659304900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   session  total_action  session_start_time  session_end_time  start_idx  \\\n",
       "0        0           147          1659304800        1661103727          0   \n",
       "1        1            27          1659304800        1660857067        147   \n",
       "2        2            13          1659304800        1660577379        174   \n",
       "3        3           226          1659304800        1661109666        187   \n",
       "4        4             3          1659304800        1659304900        413   \n",
       "\n",
       "     end_time  \n",
       "0  1661103727  \n",
       "1  1660857067  \n",
       "2  1660577379  \n",
       "3  1661109666  \n",
       "4  1659304900  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training -- Derive ItemCF similarity Matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define constants\n",
    "PARALLEL = 1024\n",
    "LOOKBACK_WINDOW = 200   ## only fit the latest LOOKBACK_WINDOW to train the sim matrix\n",
    "#TOPN = 20\n",
    "ACTION_WEIGHTS = np.array([1.0, 6.0, 3.0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section A: Utils Functions \n",
    "1. Count Item Total likes: The similary score will be normalized by \"Item Total Like Scores\". In theory, popular items should have less weight in simiarity score.\n",
    "2. Trimming function: Helpful managing memoery usage. \n",
    "3. Method for normalization: Mostly item total like normalization, and max norm(make all sim score between 0 and 1) of the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Methods for counting Item Total Likes\n",
    "# ==================================\n",
    "@nb.jit(nopython=True)\n",
    "def getItemTotalLikesNaive(aids, ops, item_total_likes, action_weights):\n",
    "    \"\"\"\n",
    "    Stores the total like score of itemXXX in item_total_likes, based on action_weights parameter. np.array([X, Y, Z])\n",
    "    \"\"\"\n",
    "    for idx, item in enumerate(aids):\n",
    "        if item not in item_total_likes: \n",
    "            item_total_likes[item] = 0\n",
    "        item_total_likes[item] += action_weights[ops[idx]]   ## TODO: For time decay, consider replace with 1, for iuf keep this. \n",
    "\n",
    "# ==================================\n",
    "# Methods for rank and trim the sim score dict\n",
    "# ==================================\n",
    "@nb.jit(nopython = True)\n",
    "def heap_topk(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure\n",
    "    \"\"\"\n",
    "    dic = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q) for _ in range(len(q))][::-1]\n",
    "    for i in range(len(res)):\n",
    "        dic[res[i][1]] = res[i][0]\n",
    "    \n",
    "    return dic\n",
    "   \n",
    "@nb.jit(nopython = True)\n",
    "def trim_simMatrix_topk(fullSimMatrix, k = 50):\n",
    "    \"\"\"\n",
    "    trim top k items of each \"itemX: {itemY: score1, ...}\" pair in fullSimMatrix based on sim scores. \n",
    "    \"\"\"\n",
    "    for item, item_cnt_dict in fullSimMatrix.items():\n",
    "        fullSimMatrix[item] = heap_topk(item_cnt_dict, k)\n",
    "\n",
    "# ==================================\n",
    "# Methods for score normalization\n",
    "# ==================================\n",
    "\n",
    "# @nb.jit(nopython=True)\n",
    "# def itemTotalLikeNorm(fullSimMatrix, item_total_likes):\n",
    "#     for aid_1, relations in fullSimMatrix.items():\n",
    "#         for aid_2, sim_score in relations.items():\n",
    "#             fullSimMatrix[aid_1][aid_2] = sim_score / (item_total_likes[aid_1] * item_total_likes[aid_2]) ** 0.1  ## TODO: consider 0.1 or other small number\n",
    "            \n",
    "@nb.jit(nopython=True)\n",
    "def maxNormSimMatrix(fullSimMatrix):\n",
    "    for aid_1, relations in fullSimMatrix.items():\n",
    "        max_num = -np.inf\n",
    "        for _, sim_score in relations.items():\n",
    "            if sim_score > max_num:\n",
    "                max_num = sim_score\n",
    "        ## DEGUG use, delete later\n",
    "        if max_num == 0:\n",
    "            print(aid_1)\n",
    "            print(fullSimMatrix[aid_1])\n",
    "        for aid_2, sim_score in relations.items():\n",
    "#             if max_num == 0:\n",
    "#                 max_num += 0.001\n",
    "            fullSimMatrix[aid_1][aid_2] = sim_score / max_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section B: Sim Score Computation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@nb.jit(nopython=True)\n",
    "def getSimScoresSingleRow(pairs_this_row, start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode):\n",
    "    \"\"\"\n",
    "    Get the sim scores of items within single session, can be ran in parallel within each batch. \n",
    "    \"\"\"\n",
    "    max_idx = start_idx + length\n",
    "    min_idx = max(max_idx - LOOKBACK_WINDOW, start_idx)  \n",
    "    for i in range(min_idx, max_idx):\n",
    "        for j in range(i+1, max_idx):\n",
    "            if ts[j] - ts[i] > 2 * 60 * 60: continue  #TODO: try 2h only\n",
    "            if aids[i] == aids[j]: continue\n",
    "            \n",
    "            if mode == \"cosine\":\n",
    "                w_ij = action_weights[ops[j]] \n",
    "                w_ji = action_weights[ops[i]] \n",
    "            elif mode == \"iuf\":  ## penalize users that had lots of actions TODO: consider location weight\n",
    "                \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                w_ij = action_weights[ops[j]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * time_gap_weight * loc_weight / math.log1p(length)\n",
    "            elif mode == \"time_decay\":\n",
    "                ## calculate some time weights of each item, more weights are given when ts is later. #TODO: try adding (i-j) location weight, exponential weight, 0.5 ** (abs(i-j + 1)), \n",
    "                loc_weight = 0.5**(abs(i-j))   #math.exp(-0.02 * abs(i-j)) \n",
    "                #time_i = 1 + 0.1 ** ((1662328791-ts[i])/(1662328791-1659304800)) #1 + 3 * (ts[i] + start_time - 1659304800) / (1662328791 - 1659304800) #  #(1 - 0.8 *(TEST_END_TS - ts[i]) / TIME_SPAN) ** 0.5 # 0.2~1 #   ## time decay weight for item i \n",
    "                #time_j = 1 + 0.1 ** ((1662328791-ts[j])/(1662328791-1659304800))  # 1 + 3 * (ts[j] + start_time - 1659304800) / (1662328791 - 1659304800) # #  #(1 - 0.8 *(TEST_END_TS - ts[j]) / TIME_SPAN) ** 0.5   # \n",
    "                time_i = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[i])/(1662328791-1659304800)) - 0.6  )))\n",
    "                time_j = 1 + 1/(1 + math.exp(10*( ((1662328791-ts[j])/(1662328791-1659304800)) - 0.6  )))\n",
    "                \n",
    "                time_gap_weight = 0.5 ** (abs(ts[i]-ts[j]) / (1.5*60*60))  \n",
    "                \n",
    "                w_ij = action_weights[ops[j]] * loc_weight * time_gap_weight * time_i / math.log1p(length)\n",
    "                w_ji = action_weights[ops[i]] * loc_weight * time_gap_weight * time_j / math.log1p(length)\n",
    "                \n",
    "            pairs_this_row[(aids[i], aids[j])] = w_ij / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "            pairs_this_row[(aids[j], aids[i])] = w_ji / (item_total_likes[aids[i]] * item_total_likes[aids[j]]) ** 0.1\n",
    "\n",
    "@nb.jit(nopython=True, parallel=True, cache=True)\n",
    "def getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, action_weights, item_total_likes, mode=\"cosine\"):\n",
    "    nrows = len(rows)\n",
    "    pairs_this_batch = [{(0, 0): 0.0 for _ in range(0)} for _ in range(nrows)]\n",
    "    ## get the sim scores of each batch in seperate sub dict in pairs_this_batch\n",
    "    for row_i in nb.prange(nrows):  ## run each row of the batch in parallel\n",
    "        _, start_idx, length, start_time = rows[row_i]\n",
    "        getSimScoresSingleRow(pairs_this_batch[row_i], start_time, start_idx, length, aids, ts, ops, item_total_likes, action_weights, mode)\n",
    "    ## merge pairs_this_batch into the fullSimMatrix\n",
    "    for row_i in range(nrows):\n",
    "        for (aid1, aid2), score in pairs_this_batch[row_i].items():\n",
    "            if aid1 not in fullSimMatrix: \n",
    "                fullSimMatrix[aid1] = {0: 0.0 for _ in range(0)}\n",
    "            if aid2 not in fullSimMatrix[aid1]:\n",
    "                fullSimMatrix[aid1][aid2] = 0.0\n",
    "            fullSimMatrix[aid1][aid2] += score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section C: Train the similarity matrices\n",
    "1. Derive the total like score first\n",
    "2. Train 2 similarity matrices, one using iuf(Inverse User Frequence), the other using time_decay method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19 s, sys: 798 ms, total: 19.8 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## get the Total Like matrix\n",
    "item_total_likes = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.float64)\n",
    "\n",
    "getItemTotalLikesNaive(aids, ops, item_total_likes, ACTION_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 998/12079 [03:02<47:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2000/12079 [06:44<24:38:09,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2999/12079 [09:47<34:35:10, 13.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3999/12079 [12:27<32:21:39, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 4999/12079 [15:03<29:14:39, 14.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5999/12079 [17:22<24:36:44, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 6999/12079 [19:36<22:32:15, 15.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 7997/12079 [20:45<05:06, 13.33it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 8998/12079 [23:26<03:32, 14.49it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10000/12079 [25:13<6:02:56, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10999/12079 [26:38<4:20:04, 14.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  11000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 11999/12079 [27:54<09:30,  7.13s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx:  12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12079/12079 [27:56<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51min 38s, sys: 26min 51s, total: 1h 18min 29s\n",
      "Wall time: 28min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "simMatrices = {}   ## store a few different similarity matrices using different scoring system, for different prediction type\n",
    "TRIM_CYCLES = 1000   ## trim full sim matrix every XX batches. \n",
    "MODES_TO_TRAIN = [\"iuf\"] #, \"time_decay\"]\n",
    "\n",
    "for mode in MODES_TO_TRAIN:\n",
    "    ## the nested dict to store full sim matrix, {itemX: {itemY: score, itemZ: score, ...}}\n",
    "    fullSimMatrix = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.typeof(nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)))\n",
    "    max_idx = len(df)\n",
    "    batch_idx = 1  ## compute sim matrix for PARALLEL # of rows per batch, have a total of max_idx/PARALLEL batches.\n",
    "    for idx in tqdm(range(0, max_idx, PARALLEL)):\n",
    "        rows = df.iloc[idx: min(idx + PARALLEL, max_idx)][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "        getSimScoreBatch(aids, ts, ops, rows, fullSimMatrix, ACTION_WEIGHTS, item_total_likes, mode=mode)\n",
    "        batch_idx += 1\n",
    "        if batch_idx % TRIM_CYCLES == 0:\n",
    "            print(\"batch_idx: \", batch_idx)\n",
    "            trim_simMatrix_topk(fullSimMatrix, 100)\n",
    "            gc.collect()\n",
    "            # break\n",
    "\n",
    "    \n",
    "    ## trim top 50 when the training is complete\n",
    "    trim_simMatrix_topk(fullSimMatrix, 80)   ## TODO: make this num small enough to reduce time for normalization\n",
    "    ## max norm of each score\n",
    "    maxNormSimMatrix(fullSimMatrix)\n",
    "    \n",
    "    simMatrices[mode] = fullSimMatrix\n",
    "    \n",
    "    del fullSimMatrix\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictType[int64,float64]<iv=None>({331941: 1.0, 243711: 0.7190634944882741, 1801351: 0.5040513231086448, 371417: 0.4167012125283079, 32249: 0.41373445935315617, 899438: 0.3626232356223332, 303302: 0.3081623040015135, 461689: 0.24140817919093385, 1799312: 0.20439432086051285, 576535: 0.20363825079722686, 516937: 0.17150816792050863, 1500897: 0.1474968554333243, 807502: 0.12350333543572184, 528847: 0.12185781543011626, 734463: 0.12045400414403375, 460942: 0.1139099425003342, 12725: 0.10691090671837272, 215107: 0.09943509171737062, 893213: 0.09210204073235082, 1364441: 0.0843909572280193, 1620477: 0.07891101754442596, 355088: 0.0760826198068775, 1630327: 0.07572153181228192, 804782: 0.07362283905259837, 760663: 0.07337831080008321, 1390627: 0.07327155250282527, 1775482: 0.07042982880108292, 293222: 0.06833266345318563, 307484: 0.06657964815873083, 1432934: 0.06421905471330858, 1190046: 0.06345478375755678, 1371202: 0.0631481939274279, 1350724: 0.06124414306732429, 800979: 0.05847478548902716, 1693043: 0.05519540528206179, 932220: 0.0548184226341905, 16246: 0.05382331288999998, 808248: 0.0468322918606895, 598127: 0.046760823146036384, 103072: 0.046309211766148904, 396565: 0.046030035777334305, 1110941: 0.04092365838339326, 1099100: 0.03619644771308173, 510211: 0.03541343993983789, 639347: 0.03419103352166187, 1334074: 0.033085987748001644, 779066: 0.032644138642937116, 551290: 0.03158274416788717, 1349093: 0.03030716586650167, 652554: 0.030212446221542864, 527845: 0.028594449517798545, 1436167: 0.02846870027639107, 448706: 0.028061612990489603, 1060697: 0.027324443769941212, 723528: 0.02612315374262451, 1068049: 0.025968607215378716, 102536: 0.024083005848090942, 943586: 0.02206653719173201, 493315: 0.02119197004660886, 269602: 0.02037388714337668, 596425: 0.01930637182036119, 1635429: 0.018677605376125284, 202224: 0.01754375408122563, 1542438: 0.017252435250822866, 231588: 0.01714137028013897, 1458399: 0.016804031958741056, 22721: 0.016564447257208324, 447175: 0.016118699527914587, 307896: 0.015592268392619143, 810071: 0.015403622390389955, 851412: 0.014540034870713103, 1794652: 0.014092601761241007, 748986: 0.013794412765118282, 1511175: 0.013735502252892239, 968570: 0.013560333072959863, 292471: 0.011918520614207184, 317435: 0.011816882064306762, 77442: 0.011655700039315542, 15594: 0.01128355401653921, 37942: 0.011175260405487176})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## A sanity check\n",
    "simMatrices[\"iuf\"][1517085]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1814440"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(simMatrices[\"iuf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11016"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference -- Make prediction using the matrices derived from above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section D: Utils for inference:\n",
    "1. Select top items to recommend in re-ranking\n",
    "2. Compute Real time importance of each action (Not in use currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython = True)\n",
    "def heap_topk_return_list(item_cnt_dict, cap):\n",
    "    \"\"\"\n",
    "    get the top cap(k) elements of the cnt dict based on value, using a min-heap structure, return a list with top \"cap\" elements with highest score\n",
    "    \"\"\"\n",
    "    q = [(np.float64(0), np.int64(0)) for _ in range(0)]  ## generate empty queue to implement a heap, \n",
    "    for item_ref, sim_score in item_cnt_dict.items():   ## read in the dict in heap structure\n",
    "        heapq.heappush(q, (sim_score, item_ref))   ## push the <sim_score, item_ref_id> pair into min-heap, using sim_score for order\n",
    "        if len(q) > cap:\n",
    "            heapq.heappop(q)\n",
    "            \n",
    "    res = [heapq.heappop(q)[1] for _ in range(len(q))][::-1]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section E: Main Logic in Making Inferences (DO NOT RUN in this notebook)\n",
    "1. clicks_inferences: time_decay sim matrix + regular action weights <1, 6, 3>.\n",
    "2. carts_inferencs: iuf sim matrix + weights <4, 2, 5> (as clicks actions tend to lead to cart action next).\n",
    "3. orders_inferences: iuf sim matrix + regular action weights <1, 6, 3>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    PREV_INTERACT_BONUS = 10\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length\n",
    "    end_time = ts[ending_idx]\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/(end_time - start_time))\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    \n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += seq_w * time_w * test_ops_weights[op] #* PREV_INTERACT_BONUS\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            potential_to_recommend[aid] += np.inf #seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                potential_to_recommend[similar_item] += seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 20))\n",
    "    \n",
    "@nb.jit(nopython=True)\n",
    "def run_inference_parallel(rows, aids, ops, ts, result, full_sim_matrix, test_ops_weights):\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        inference_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, test_ops_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# result_iuf = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_iuf_2 = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "#     start_row = row_idx\n",
    "#     end_row = min(row_idx + PARALLEL, len(df))\n",
    "#     rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_iuf, simMatrices[\"iuf\"], np.array([2.0, 6.0, 6.0]))\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_iuf_2, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))   ## considebly add the weights for click action in the real time.\n",
    "#     run_inference_parallel(rows, aids, ops, ts, result_time_decay, simMatrices[\"time_decay\"], np.array([3.0, 6.0, 3.0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions - Convert results to csv, get validation set result - DO NOT RUN in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# subs = []\n",
    "# op_names = [\"clicks\", \"carts\", \"orders\"]\n",
    "\n",
    "# for result, op in zip([result_time_decay, result_iuf_2, result_iuf], op_names):\n",
    "#     sub = pd.DataFrame({\"session_type\": result.keys(), \"labels\": result.values()})\n",
    "#     sub.session_type = sub.session_type.astype(str) + f\"_{op}\"\n",
    "#     sub.labels = sub.labels.apply(lambda x: \" \".join(x.astype(str)))\n",
    "#     subs.append(sub)\n",
    "    \n",
    "# submission = pd.concat(subs).reset_index(drop=True)\n",
    "# #sub.sort_values(by=[\"session_type\"])  ## optional\n",
    "# #submission.to_csv('submission.csv', index = False)\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# submission.to_csv('../../allData/validationData/p_v_579_80_items.csv', index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section F: Saving Feature, Run E or F, depends on mode, this will not only generate result, but the featues associated.\n",
    "**Feature engineering are made here**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utils for features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def update_feature_vec(aid, features_tuple_arr, features_idx_map, new_feat_tuple):\n",
    "    ## append features\n",
    "    if aid not in features_idx_map:\n",
    "        features_tuple_arr.append(new_feat_tuple)\n",
    "        new_pos = len(features_tuple_arr)-1\n",
    "        ## save the position in the tuple arr\n",
    "        features_idx_map[aid] = new_pos\n",
    "    else: # <is_prev_int, seq_w, time_w, total_ops_w, session_len, # uniuqe aids, CF_score, aid's itemTotalLike >\n",
    "        # ================== 8 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8]\n",
    "        else:\n",
    "            slot8_ref_time = features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8]\n",
    "        # ================== 9 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot9_max_sim = 1\n",
    "        else:\n",
    "            slot9_max_sim = max(new_feat_tuple[9], features_tuple_arr[features_idx_map[aid]][9])\n",
    "        # ================== 10 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot10_mean_sim = 1\n",
    "        else:\n",
    "            slot10_mean_sim = ((features_tuple_arr[features_idx_map[aid]][10] * (features_tuple_arr[features_idx_map[aid]][8]-1) ) + new_feat_tuple[10] ) / features_tuple_arr[features_idx_map[aid]][8]\n",
    "        # ================== 14 seq_w ==\n",
    "        slot14_max_seq_w = max(new_feat_tuple[14], features_tuple_arr[features_idx_map[aid]][14])\n",
    "        # ================== 15 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot15_mean_seq_w = (features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================== 16 ==\n",
    "        slot16_min_seq_w = min(new_feat_tuple[16], features_tuple_arr[features_idx_map[aid]][16])\n",
    "        # ================== 17 time_w == \n",
    "        slot17_max_time_w = max(new_feat_tuple[17], features_tuple_arr[features_idx_map[aid]][17])\n",
    "        # ================== 18 == \n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot18_mean_time_w = (features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 19 ==\n",
    "        slot19_min_time_w = min(new_feat_tuple[19], features_tuple_arr[features_idx_map[aid]][19])\n",
    "        # ================= 20 ops_w ==\n",
    "        slot20_max_ops_w = max(new_feat_tuple[20], features_tuple_arr[features_idx_map[aid]][20])\n",
    "        # ================= 21 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot21_mean_ops_w = (features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3]) / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 22 ==\n",
    "        slot22_min_ops_w = min(new_feat_tuple[22], features_tuple_arr[features_idx_map[aid]][22]) #new_feat_tuple[22] if new_feat_tuple[22] < features_tuple_arr[features_idx_map[aid]][22] else features_tuple_arr[features_idx_map[aid]][22]\n",
    "        # ================= 28 cf_incre ==\n",
    "        slot28_max_cf_incre = max(new_feat_tuple[28], features_tuple_arr[features_idx_map[aid]][28])\n",
    "        # ================= 29 ==\n",
    "        if features_tuple_arr[features_idx_map[aid]][0]: \n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11])\n",
    "        else:\n",
    "            slot29_mean_cf_incre = new_feat_tuple[6] / (features_tuple_arr[features_idx_map[aid]][8] + new_feat_tuple[8])\n",
    "        # ================= 30 ==\n",
    "        slot30_min_cf_incre = min(new_feat_tuple[30], features_tuple_arr[features_idx_map[aid]][30])\n",
    "\n",
    "        \n",
    "        features_tuple_arr[features_idx_map[aid]] = (new_feat_tuple[0], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][1] + new_feat_tuple[1], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][2] + new_feat_tuple[2], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][3] + new_feat_tuple[3],\n",
    "                                                     new_feat_tuple[4],\n",
    "                                                     new_feat_tuple[5],\n",
    "                                                     new_feat_tuple[6],\n",
    "                                                     new_feat_tuple[7],\n",
    "                                                     slot8_ref_time,\n",
    "                                                     slot9_max_sim,\n",
    "                                                     slot10_mean_sim,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][11] + new_feat_tuple[11],\n",
    "                                                     new_feat_tuple[12],\n",
    "                                                     new_feat_tuple[13],\n",
    "                                                     slot14_max_seq_w,\n",
    "                                                     slot15_mean_seq_w,\n",
    "                                                     slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w,\n",
    "                                                     slot18_mean_time_w,\n",
    "                                                     slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w,\n",
    "                                                     slot21_mean_ops_w,\n",
    "                                                     slot22_min_ops_w,\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][23] + new_feat_tuple[23], \n",
    "                                                     features_tuple_arr[features_idx_map[aid]][24] + new_feat_tuple[24],\n",
    "                                                     features_tuple_arr[features_idx_map[aid]][25] + new_feat_tuple[25],\n",
    "                                                     new_feat_tuple[26],\n",
    "                                                     new_feat_tuple[27],\n",
    "                                                     slot28_max_cf_incre,\n",
    "                                                     slot29_mean_cf_incre,\n",
    "                                                     slot30_min_cf_incre,\n",
    "                                                     slot14_max_seq_w - slot16_min_seq_w,\n",
    "                                                     slot17_max_time_w - slot19_min_time_w,\n",
    "                                                     slot20_max_ops_w - slot22_min_ops_w,\n",
    "                                                     slot28_max_cf_incre - slot30_min_cf_incre\n",
    "                                                     )\n",
    "\n",
    "# <\n",
    "# slot_0: is_prev_interacted, \n",
    "# slot_1: seq_w_total, \n",
    "# slot_2: time_w_total, \n",
    "# slot_3: ops_w_total: for visited item, ops weight total in this session; for unvisited item, ops weight total of the item referencing this item. \n",
    "# slot_4: session_len, \n",
    "# slot_5: num uniuqe aids, \n",
    "# slot_6: CF_score, \n",
    "# slot_7: aid's itemTotalLike: total like score use for normalization.  \n",
    "# slot_8: reference time by similar matrix(if aid visited, default 100; if aid not visited, 1-19(when all aid only interact once, could grow to very large if a lot of actions on one aid), depending how many aid reference this item)\n",
    "# slot_9: max_sim_score:  (1 if it's a visited item)\n",
    "# slot_10: mean_sim_score: (1 if it's a visited item)\n",
    "# slot_11: num_interact, (0 for unvisited item; count of interaction for visited item)\n",
    "# slot_12: time_span of the session \n",
    "# slot_13: action_recency: time to last action(end time), for unvisited items -> the time to of reference_aid to the last action\n",
    "# slot_14: seq_w_max: \n",
    "# slot_15: seq_w_mean: for visited item -> seq_w_total / num_interact; for unvisited item -> seq_w_total / reference_time\n",
    "## ========================= round 2 ================\n",
    "# slot_16: seq_w_min: \n",
    "# slot_17: time_w_max:\n",
    "# slot_18: time_w_mean: similar to slot_15\n",
    "# slot_19: time_w_min\n",
    "# slot_20: ops_w_max:\n",
    "# slot_21: ops_w_mean:\n",
    "# slot_22: ops_w_min: \n",
    "# slot_23: num_clicks: visited item, direct num; unvisited item, take the reference item's num\n",
    "# slot_24: num_carts:\n",
    "# slot_25: num_orders:\n",
    "# slot_26: last_action_type: 0 -> clicks, 1-> carts, 2 -> orders\n",
    "# slot_27: time_to_now: latest interaction time to now \n",
    "# slot_28: cf_increment_max: \n",
    "# slot_29: cf_increment_mean:\n",
    "# slot_30: cf_increment_min:\n",
    "##  ======================== Derivable ============================\n",
    "## slot_31: seq_w max_min_gap: slot_14 - slot_16 \n",
    "## slot_32: time_w_max_min_gap: slot_17 - slot_19\n",
    "## slot_33: ops_w_max_min_gap: slot_20 - slot_22\n",
    "## slot_34: cf_incre_max_min_gap: slot_28 - slot_30\n",
    "# >\n",
    "FEATURE_TUPLE_TEMPLATE = (bool(0), np.float64(0.0), np.float64(0.0), np.int64(0), np.int64(0), np.int64(0), \\\n",
    "    np.float64(0.0), np.float64(0.0), np.int32(0), np.float32(0.0), np.float64(0.0), np.int32(0), np.float64(0.0), np.float64(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "        np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0),\\\n",
    "            np.int32(0), np.int32(0), np.int32(0), np.int32(0), np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0), \\\n",
    "                np.float32(0.0), np.float32(0.0), np.float32(0.0), np.float32(0.0))\n",
    "\n",
    "FEATURE_NAMES = [\"prev_int\", \"seq_w_total\", \"time_w_total\", \"ops_w_total\", \"session_len\", \"num_uniuqe_aids\", \"CF_score\", \"itemTotalLike\", \"ref_time\", \"max_sim_score\",\\\n",
    "    \"mean_sim_score\", \"num_interact\", \"time_span\", \"action_recency\", \"seq_w_max\", \"seq_w_mean\", \"seq_w_min\", \"time_w_max\", \"time_w_mean\", \"time_w_min\", \\\n",
    "        \"ops_w_max\", \"ops_w_mean\", \"ops_w_min\", \"num_clicks\", \"num_carts\", \"num_orders\", \"last_action_type\", \"time_to_now\", \"cf_incre_max\", \"cf_incre_mean\", \\\n",
    "            \"cf_incre_min\", \"seqW_max_min_gap\", \"timeW_max_min_gap\", \"opsW_max_min_gap\", \"cf_incre_max_min_gap\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main feature save logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "@nb.jit(nopython=True)\n",
    "def save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result, full_sim_matrix, item_total_likes, test_ops_weights):\n",
    "    NOW_TIME = ts[-1] ## ts of latest avaiable action\n",
    "    PREV_INTERACT_BONUS = 20\n",
    "    NEARBY_ACTION_BONUS = 1.5\n",
    "    \n",
    "    ending_idx = starting_idx + length \n",
    "    end_time = ts[ending_idx - 1]\n",
    "    time_span = end_time - start_time\n",
    "    \n",
    "    candidates = aids[starting_idx: ending_idx][::-1]\n",
    "    candidates_ops = ops[starting_idx: ending_idx][::-1]\n",
    "    \n",
    "    ## record all potential aid that might be relevant\n",
    "    potential_to_recommend = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.float64)\n",
    "    \n",
    "    ## get unique aid of each session \n",
    "    unique_aids = nb.typed.Dict.empty(key_type = nb.types.int64, value_type = nb.types.float64)\n",
    "    for a in candidates:\n",
    "        unique_aids[a] = 0\n",
    "    \n",
    "    ## Sequence weight to all the candidates, from near to far \n",
    "    sequence_weight = np.power(2, np.linspace(0.3, 1, len(candidates)))[::-1] - 1\n",
    "    \n",
    "    ## Time weight of all candidates, from near to far\n",
    "    time_weights = []\n",
    "    time_lapse = end_time - start_time + 1  ## +1 to avoid zero\n",
    "    for idx in range(starting_idx, ending_idx):\n",
    "        if end_time - ts[idx] < 2 * 60 * 60:   ## apply nearby action bonus\n",
    "            time_weight = (1 + 0.5 ** ((end_time - ts[idx])/time_lapse)) * NEARBY_ACTION_BONUS\n",
    "        else:\n",
    "            time_weight = 1 + 0.5 ** ((end_time - ts[idx])/time_lapse)\n",
    "        time_weights.append(time_weight)\n",
    "    time_weights = time_weights[::-1]\n",
    "    \n",
    "    ## feature vector template: [aid: <is_prev_int, seq_w, time_w, associated_action, session_len,.. >]\n",
    "    features_tuple_arr = nb.typed.List()\n",
    "    features_tuple_arr.append(FEATURE_TUPLE_TEMPLATE)\n",
    "    features_idx_map = nb.typed.Dict.empty(key_type=nb.types.int64, value_type=nb.types.int64)\n",
    "\n",
    "    helper_idx = starting_idx\n",
    "    ## making inference\n",
    "    if len(unique_aids) >= 20:  \n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## caculate scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op]\n",
    "            potential_to_recommend[aid] += cf_incre #* PREV_INTERACT_BONUS\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME-ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "    else:   ## otherwise, fill the rest with similar items.\n",
    "        for aid, op, seq_w, time_w in zip(candidates, candidates_ops, sequence_weight, time_weights):\n",
    "            if aid not in potential_to_recommend:\n",
    "                potential_to_recommend[aid] = 0\n",
    "            ## get the scores\n",
    "            cf_incre = seq_w * time_w * test_ops_weights[op] * PREV_INTERACT_BONUS\n",
    "            potential_to_recommend[aid] += cf_incre\n",
    "            ## append features\n",
    "            update_feature_vec(aid, features_tuple_arr, features_idx_map, \\\n",
    "                (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid], \\\n",
    "                    item_total_likes[aid], 100, 1, 1, 1, time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, \\\n",
    "                        time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op, NOW_TIME - ts[helper_idx],\\\n",
    "                            cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            ## adding the similar items, if full_sim_matrix don't have such record, skip. \n",
    "            if aid not in full_sim_matrix:\n",
    "                continue\n",
    "            for similar_item in full_sim_matrix[aid]:\n",
    "                ## if sim_item is in candidates, would be included above anyways, skip \n",
    "                if similar_item in candidates:\n",
    "                    continue\n",
    "                if similar_item not in potential_to_recommend:\n",
    "                    potential_to_recommend[similar_item] = 0\n",
    "                \n",
    "                cf_incre = seq_w * time_w * test_ops_weights[op] * full_sim_matrix[aid][similar_item]\n",
    "                potential_to_recommend[similar_item] += cf_incre  ## no PREV_INTERACT_BONUS as expected, replaced with sim_matrix scores\n",
    "                ## append features\n",
    "                update_feature_vec(similar_item, features_tuple_arr, features_idx_map, \\\n",
    "                    (0, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[similar_item], \\\n",
    "                        item_total_likes[similar_item], 1, full_sim_matrix[aid][similar_item], full_sim_matrix[aid][similar_item], 0, \\\n",
    "                            time_span, end_time-ts[helper_idx], seq_w, seq_w, seq_w, time_w, time_w, time_w, test_ops_weights[op], test_ops_weights[op], test_ops_weights[op], op==0, op==1, op==2, op,\\\n",
    "                                NOW_TIME-ts[helper_idx], cf_incre, cf_incre, cf_incre, 0, 0, 0, 0))\n",
    "            helper_idx += 1\n",
    "\n",
    "    result[session] = np.array(heap_topk_return_list(potential_to_recommend, 100))  ## Take top 100 for validation runs. \n",
    "    \n",
    "    feature_tuples_this_session = []\n",
    "    for aid in result[session]:\n",
    "#         features_save[(session, aid)] = features_tuple_arr[features_idx_map[aid]]\n",
    "        feature_tuples_this_session.append(features_tuple_arr[features_idx_map[aid]])\n",
    "    \n",
    "    return feature_tuples_this_session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utils for batch processing the features save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt_tables(type):\n",
    "    \"\"\" type -> carts / orders \"\"\"\n",
    "    gt_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "    gt_labels['aids'] = gt_labels[\"labels\"].apply(lambda x: x.get(type))\n",
    "    gt_labels = gt_labels[gt_labels.aids.notnull()]\n",
    "    gt_labels = gt_labels.drop(\"labels\", axis = 1)\n",
    "    ## ========= special df to identify the unique session id to look at ================\n",
    "    valid_gt_sessions = gt_labels.drop(\"aids\", axis = 1) \n",
    "    ## ========================================================================\n",
    "    ## keep go on for gt labels processing\n",
    "    gt_labels = gt_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "    gt_labels[\"gt\"] = 1\n",
    "    return valid_gt_sessions, gt_labels\n",
    "\n",
    "def process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels):\n",
    "    \"\"\" rawDf -> Df with session, aids(100), feature_tuple \"\"\"\n",
    "    ## join valid_gt_session with rawDf, now only gt_features in valid sessions(have at least 1 aid to predict) are kept\n",
    "    gt_features_valid_session = pd.merge(rawDf, valid_gt_sessions, on=\"session\")\n",
    "\n",
    "    ## Now explode the whole valid_gt_session aids, these session - aid are served as the train/val/test data for the reranker model, \n",
    "    ## for orders, \n",
    "    ## for carts, a total of 569697 correct guesses(not 100% included in the recall)\n",
    "    gt_features_valid_session = gt_features_valid_session.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "\n",
    "    ## finally, attach the gt_lables 1/null to the df to return\n",
    "    final_df = pd.merge(gt_features_valid_session, gt_labels, on=[\"session\", \"aids\"], how='left')\n",
    "\n",
    "    ## open up the feature tuple \n",
    "    for slot_id, f_name in enumerate(FEATURE_NAMES):\n",
    "        final_df[f_name] = final_df[\"feature_tuple\"].apply(lambda x: x[slot_id])\n",
    "\n",
    "    final_df = final_df.drop(\"feature_tuple\", axis = 1)\n",
    "\n",
    "    return final_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save features as batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the gt datas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 146/1742 [01:59<4:51:28, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 291/1742 [03:52<4:01:50, 10.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 436/1742 [05:48<4:06:32, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_2 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 581/1742 [07:57<3:53:22, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_3 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 726/1742 [10:04<3:14:35, 11.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_4 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 871/1742 [12:06<2:39:53, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_5 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 1016/1742 [14:04<1:59:19,  9.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_6 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1161/1742 [16:07<1:43:59, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_7 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 1306/1742 [18:10<1:14:58, 10.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_8 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1451/1742 [20:15<52:31, 10.83s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_9 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 1596/1742 [22:27<27:41, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_10 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1741/1742 [24:30<00:10, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_11 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [24:31<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_12 completes saving.\n",
      "CPU times: user 15min 32s, sys: 4min 18s, total: 19min 51s\n",
      "Wall time: 24min 42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result_iuf_orders = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_iuf_carts = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1783737 sessions in total, we separate them into K batches\n",
    "K = 12\n",
    "batch_size = 1783737 // K  ##  -> 148644 dealing with around 150k sessions per batch\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (1024 * 145) * i for i in range(1, K+3)]   ## batch process every 1024 * 145 rows\n",
    "# 145 -> 148644 // 1024  -> batch_size // PARALLEL\n",
    "\n",
    "feature_batch_id = 0\n",
    "## load important tables\n",
    "valid_gt_sessions, gt_labels = load_gt_tables(\"orders\")\n",
    "print(\"finish loading the gt datas\")\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], item_total_likes, np.array([2.0, 6.0, 6.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_orders.keys(), \"aids\": result_iuf_orders.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels)\n",
    "        batch_result.to_parquet(f\"../../allData/features/order_features_V5/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_orders\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_orders = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        \n",
    "    #break\n",
    "#     gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the feature storage pipeline for carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading the gt datas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 291/1742 [06:40<22:43:24, 56.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_0 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 581/1742 [10:47<10:37:42, 32.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_1 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 871/1742 [15:05<8:56:04, 36.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_2 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1161/1742 [19:06<5:00:30, 31.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_3 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1451/1742 [22:49<1:58:41, 24.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_4 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1741/1742 [26:56<00:32, 32.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_5 completes saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1742/1742 [26:58<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_batch_6 completes saving.\n",
      "CPU times: user 11min 44s, sys: 8min 27s, total: 20min 12s\n",
      "Wall time: 28min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# result_iuf_orders = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_carts = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = [] # session, aid, feature tuple\n",
    "                          ## session, aid, feature tuple\n",
    "gc.collect()\n",
    "\n",
    "## Given there are 1783737 sessions in total, we separate them into K batches\n",
    "K = 6\n",
    "batch_size = 1783737 // K  ##  -> 297289 ~ 1024 * 290 dealing with around 30k sessions per batch\n",
    "row_idx_cutoffs = [(len(df) - len(df_test)) + (1024 * 290)* i for i in range(1, 8)]\n",
    "\n",
    "feature_batch_id = 0\n",
    "## load important tables\n",
    "valid_gt_sessions, gt_labels = load_gt_tables(\"carts\")\n",
    "print(\"finish loading the gt datas\")\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], item_total_likes, np.array([4.0, 2.0, 5.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "    \n",
    "    if (start_row in row_idx_cutoffs) or (end_row == len(df)):\n",
    "        ## save batch result\n",
    "        rawDf = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n",
    "        batch_result = process_batch_pipeline(rawDf, valid_gt_sessions, gt_labels)\n",
    "        batch_result.to_parquet(f\"../../allData/features/cart_features_V4/batch_result_{feature_batch_id}.parquet\")\n",
    "        ## clean the memory for next batch\n",
    "        del batch_result, rawDf, features_all_sessions, result_iuf_carts\n",
    "        gc.collect()\n",
    "        ## progress update\n",
    "        print(f\"feature_batch_{feature_batch_id} completes saving.\")\n",
    "        feature_batch_id += 1\n",
    "        ## initiate the struct for new batch again\n",
    "        result_iuf_carts = nb.typed.Dict.empty(\n",
    "            key_type = nb.types.int64,\n",
    "            value_type = nb.types.int64[:])\n",
    "        features_all_sessions = []\n",
    "        \n",
    "    #break\n",
    "#     gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do same thing for carts, Draft from earlier, DEPRECATED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1742 [00:00<?, ?it/s]<ipython-input-11-e41a977c9bca>:47: NumbaTypeSafetyWarning: \u001b[1m\u001b[1m\u001b[1m\u001b[1munsafe cast from Tuple(int64, float64, float64, float64, int64, int64, float64) to Tuple(bool, float64, float64, int64, int64, int64, float64). Precision may be lost.\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
      "  update_feature_vec(aid, features_tuple_arr, features_idx_map, (1, seq_w, time_w, test_ops_weights[op], length, len(unique_aids), potential_to_recommend[aid]))\n",
      "100%|██████████| 1742/1742 [09:11<00:00,  3.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 18s, sys: 1min 35s, total: 5min 53s\n",
      "Wall time: 9min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# result_iuf_orders = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "result_iuf_carts = nb.typed.Dict.empty(\n",
    "    key_type = nb.types.int64,\n",
    "    value_type = nb.types.int64[:])\n",
    "\n",
    "# result_time_decay_clicks = nb.typed.Dict.empty(\n",
    "#     key_type = nb.types.int64,\n",
    "#     value_type = nb.types.int64[:])\n",
    "\n",
    "features_all_sessions = []\n",
    "gc.collect()\n",
    "\n",
    "for row_idx in tqdm(range(len(df) - len(df_test), len(df), PARALLEL)):\n",
    "    start_row = row_idx\n",
    "    end_row = min(row_idx + PARALLEL, len(df))\n",
    "    rows = df.iloc[start_row: end_row][['session', 'start_idx', 'total_action', 'session_start_time']].values\n",
    "#     save_features_parallel(rows, aids, ops, ts, result_iuf_orders, simMatrices[\"iuf\"], np.array([2.0, 8.0, 6.0]), orders_features_save)  \n",
    "    ## run things in parallel\n",
    "    for row_idx in nb.prange(len(rows)):\n",
    "        session, starting_idx, length, start_time = rows[row_idx]\n",
    "        features_tuples_this_session = save_feature_single_session(session, starting_idx, length, start_time, aids, ops, ts, result_iuf_carts, simMatrices[\"iuf\"], np.array([4.0, 2.0, 5.0]))\n",
    "        features_all_sessions.append(features_tuples_this_session)\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.64 s, sys: 26.8 s, total: 36.5 s\n",
      "Wall time: 2min 21s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10031"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## save the result as df \n",
    "features_save = pd.DataFrame({\"session\": result_iuf_carts.keys(), \"aids\": result_iuf_carts.values(), \"feature_tuple\": features_all_sessions})\n",
    "# features_save = features_save.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "del features_all_sessions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 1min 9s, total: 1min 27s\n",
      "Wall time: 5min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## A total of 569697 carts action that's predictable \n",
    "## get all valid sessions with carts actions\n",
    "## there are 150179 / 1783737 sessions have orders actions, 301057 / 1783737 have carts, 1737968 / 1783737 have clicks\n",
    "carts_labels = pd.read_json(\"../../allData/validationData/out_7day_test/test_labels.jsonl\", lines=True)\n",
    "carts_labels['aids'] = carts_labels[\"labels\"].apply(lambda x: x.get(\"carts\"))\n",
    "carts_labels = carts_labels[carts_labels.aids.notnull()]\n",
    "carts_labels = carts_labels.drop(\"labels\", axis = 1)\n",
    "## ========= special df to identify the unique session id to look at ================\n",
    "valid_cart_sessions = carts_labels.drop(\"aids\", axis = 1) \n",
    "## ========================================================================\n",
    "## keep go on for cart labels processing\n",
    "carts_labels = carts_labels.set_index(['session']).apply(pd.Series.explode).reset_index()\n",
    "carts_labels[\"gt\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join with features_save, now only cart_features in valid sessions are kept, and as expected 301057 sessions are the valid sessions to expand\n",
    "cart_features_valid_session = pd.merge(features_save, valid_cart_sessions, on=\"session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now explode the whole valid session aids, a total of 27168199 session - aid are served as the traini/val/test data for cart, \n",
    "## a total of 569697 correct guesses(not 100% included in the recall)\n",
    "cart_features_valid_session = cart_features_valid_session.set_index(['session']).apply(pd.Series.explode).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## finally \n",
    "carts_full_df = pd.merge(cart_features_valid_session, carts_labels, on=[\"session\", \"aids\"], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with ops_total\n"
     ]
    }
   ],
   "source": [
    "## open up the feature tuple \n",
    "carts_full_df[\"prev_int\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "carts_full_df[\"seq_w\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "carts_full_df[\"time_w\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "carts_full_df[\"ops_total\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "print(\"Done with ops_total\")\n",
    "carts_full_df[\"session_len\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "carts_full_df[\"session_unique_aid\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "carts_full_df[\"rank_score\"] = carts_full_df[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "carts_full_df = carts_full_df.drop(\"feature_tuple\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>aids</th>\n",
       "      <th>gt</th>\n",
       "      <th>prev_int</th>\n",
       "      <th>seq_w</th>\n",
       "      <th>time_w</th>\n",
       "      <th>ops_total</th>\n",
       "      <th>session_len</th>\n",
       "      <th>session_unique_aid</th>\n",
       "      <th>rank_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11098528</td>\n",
       "      <td>11830</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.737330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11098528</td>\n",
       "      <td>1732105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.773733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11098528</td>\n",
       "      <td>588923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.466177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11098528</td>\n",
       "      <td>571762</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.752040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11098528</td>\n",
       "      <td>884502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.231144</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.709451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    session     aids  gt  prev_int     seq_w  time_w  ops_total  session_len  \\\n",
       "0  11098528    11830 NaN      True  0.231144     3.0          4            1   \n",
       "1  11098528  1732105 NaN     False  0.231144     3.0          4            1   \n",
       "2  11098528   588923 NaN     False  0.231144     3.0          4            1   \n",
       "3  11098528   571762 NaN     False  0.231144     3.0          4            1   \n",
       "4  11098528   884502 NaN     False  0.231144     3.0          4            1   \n",
       "\n",
       "   session_unique_aid  rank_score  \n",
       "0                   1   27.737330  \n",
       "1                   1    2.773733  \n",
       "2                   1    1.466177  \n",
       "3                   1    0.752040  \n",
       "4                   1    0.709451  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carts_full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "carts_full_df.to_parquet(\"../../allData/features/carts_features_100_per_session_V3_opwFix.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # final_df[\"prev_int\"] = final_df[\"feature_tuple\"].apply(lambda x: x[0])\n",
    "    # final_df[\"seq_w_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[1])\n",
    "    # final_df[\"time_w_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[2])\n",
    "    # final_df[\"ops_total\"] = final_df[\"feature_tuple\"].apply(lambda x: x[3])\n",
    "    # final_df[\"session_len\"] = final_df[\"feature_tuple\"].apply(lambda x: x[4])\n",
    "    # final_df[\"session_unique_aid\"] = final_df[\"feature_tuple\"].apply(lambda x: x[5])\n",
    "    # final_df[\"cf_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[6])\n",
    "    # final_df[\"item_total_like\"] = final_df[\"feature_tuple\"].apply(lambda x: x[7])\n",
    "    # final_df[\"num_reference_time\"] = final_df[\"feature_tuple\"].apply(lambda x: x[8])\n",
    "    # final_df[\"max_sim_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[9])\n",
    "    # final_df[\"mean_sim_score\"] = final_df[\"feature_tuple\"].apply(lambda x: x[10])\n",
    "    # final_df[\"num_interact\"] = final_df[\"feature_tuple\"].apply(lambda x: x[11])\n",
    "    # final_df[\"time_span\"] = final_df[\"feature_tuple\"].apply(lambda x: x[12])\n",
    "    # final_df[\"action_recency\"] = final_df[\"feature_tuple\"].apply(lambda x: x[13])\n",
    "    # final_df[\"seq_w_max\"] = final_df[\"feature_tuple\"].apply(lambda x: x[14])\n",
    "    # final_df[\"seq_w_mean\"] = final_df[\"feature_tuple\"].apply(lambda x: x[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff58dbd0ec383761c792d0b6d4f0a9690d9c9b1bec648659fb440990235d5b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
